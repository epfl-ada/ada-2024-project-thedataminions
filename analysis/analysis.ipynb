{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_processing' from '/home/andreas/Nextcloud/Dokumente/Uni/Module/3sem-EPFL/ada/Project/ada-2024-project-thedataminions/preprocessing_tests/data_processing.py'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import data_processing as dp  # own functions and logic\n",
    "importlib.reload(dp)  # this makes it so that \n",
    "                      # the file with our functions is re-read every time, \n",
    "                      # in case we have made modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the folder where the YouNiverse dataset is stored here\n",
    "\n",
    "# when adding your own path, don't remove the existing path, just comment it\n",
    "# in this way, everyone can quickly uncomment their own path\n",
    "dataset_root_path = \"/media/andreas/Backup Plus/youniverse_dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel data (no chunks needed, as the file is not very large)\n",
    "df_channels = pd.read_csv(dataset_root_path + \"df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a small part of video and comment data, to try functions on them etc.\n",
    "# these datasets should not be used for calculations, as they don't contain all the data\n",
    "\n",
    "# load (first 100000 rows of) video data\n",
    "df_videos = pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", compression=\"infer\", lines=True, nrows=100000)\n",
    "\n",
    "# load (first 1000000 rows of) comment data\n",
    "df_comments = pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", compression=\"infer\", sep=\"\\t\", nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaming</td>\n",
       "      <td>2010-04-29</td>\n",
       "      <td>UC-lHJZR3Gqxm24_Vd_AJ5Yw</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>101000000</td>\n",
       "      <td>3956</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>2006-09-01</td>\n",
       "      <td>UCbCmjCuTUZos6Inko4u57UQ</td>\n",
       "      <td>Cocomelon - Nursery ...</td>\n",
       "      <td>60100000</td>\n",
       "      <td>458</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2006-09-20</td>\n",
       "      <td>UCpEhnqL0y41EpW2TvWAHD7Q</td>\n",
       "      <td>SET India</td>\n",
       "      <td>56018869</td>\n",
       "      <td>32661</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Howto &amp; Style</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>UC295-Dw_tDNtZXFeAPAW6Aw</td>\n",
       "      <td>5-Minute Crafts</td>\n",
       "      <td>60600000</td>\n",
       "      <td>3591</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sports</td>\n",
       "      <td>2007-05-11</td>\n",
       "      <td>UCJ5v_MCY6GNUBTO8-D3XoAg</td>\n",
       "      <td>WWE</td>\n",
       "      <td>48400000</td>\n",
       "      <td>43421</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category_cc   join_date                   channel  \\\n",
       "0         Gaming  2010-04-29  UC-lHJZR3Gqxm24_Vd_AJ5Yw   \n",
       "1      Education  2006-09-01  UCbCmjCuTUZos6Inko4u57UQ   \n",
       "2  Entertainment  2006-09-20  UCpEhnqL0y41EpW2TvWAHD7Q   \n",
       "3  Howto & Style  2016-11-15  UC295-Dw_tDNtZXFeAPAW6Aw   \n",
       "4         Sports  2007-05-11  UCJ5v_MCY6GNUBTO8-D3XoAg   \n",
       "\n",
       "                   name_cc  subscribers_cc  videos_cc  subscriber_rank_sb  \\\n",
       "0                PewDiePie       101000000       3956                 3.0   \n",
       "1  Cocomelon - Nursery ...        60100000        458                 7.0   \n",
       "2                SET India        56018869      32661                 8.0   \n",
       "3          5-Minute Crafts        60600000       3591                 9.0   \n",
       "4                      WWE        48400000      43421                11.0   \n",
       "\n",
       "   weights  \n",
       "0    2.087  \n",
       "1    2.087  \n",
       "2    2.087  \n",
       "3    2.087  \n",
       "4    2.087  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_channels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of \"readers\", i.e., objects that we can iterate through \n",
    "# and always get a chunk of the dataframe in each iteration\n",
    "\n",
    "def videos_in_chunks(chunksize: int = 100000) -> pd.io.json._json.JsonReader:\n",
    "    \"\"\"\n",
    "    Returns a Json reader which can be iterated through, to get chunks of the (unfiltered) video dataset.\n",
    "\n",
    "    Args:\n",
    "        chunksize: number of entries in each chunk\n",
    "\n",
    "    Returns:\n",
    "        the Json reader\n",
    "    \"\"\"\n",
    "    return pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", \n",
    "                        compression=\"infer\", lines=True, chunksize=chunksize, )\n",
    "                        # nrows=10000000, )   # uncomment this to only use the first million videos, for testing\n",
    "                                           # (remove the paranthesis above as well)\n",
    "\n",
    "def comments_in_chunks(chunksize: int = 1000000) -> pd.io.parsers.readers.TextFileReader:\n",
    "    \"\"\"\n",
    "    Returns a CSV reader which can be iterated through, to get chunks of the (unfiltered) comment dataset.\n",
    "\n",
    "    Args:\n",
    "        chunksize: number of entries in each chunk\n",
    "\n",
    "    Returns:\n",
    "        the CSV reader\n",
    "    \"\"\"\n",
    "    return pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", \n",
    "                       compression=\"infer\", sep=\"\\t\", chunksize=chunksize, )\n",
    "                       # nrows = 10000000)  # uncomment this to only use the first 10 million comments, for testing\n",
    "                                            # (remove the paranthesis above as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for NaNs in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* This is not complete, **(still a**\n",
    "# Todo\n",
    ")\n",
    "\n",
    "Right now, we are only checking the comments dataset for nans, not the other files\n",
    "\n",
    "also, we are only relying on pd.isna for finding NaNs.\n",
    "This function however only counts \"None\" or \"np.NaN\" and similar as Nans.\n",
    "If we for example have empty strings, or zeros in places they should not be, this would not be detected.\n",
    "\n",
    "So we should do some manual searching as well, eg. df_comments[df_comments.user_id == 0] and similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "1.239 secs per chunk on average. Meaning  177.597 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "1.133 secs per chunk on average. Meaning  162.369 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "1.151 secs per chunk on average. Meaning  164.961 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.193 secs per chunk on average. Meaning  170.846 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.185 secs per chunk on average. Meaning  169.757 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "1.189 secs per chunk on average. Meaning  170.327 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "1.214 secs per chunk on average. Meaning  173.905 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "1.211 secs per chunk on average. Meaning  173.418 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "1.212 secs per chunk on average. Meaning  173.554 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "1.220 secs per chunk on average. Meaning  174.592 minutes left.\n",
      "Going through chunk 10...\n",
      "The first 11000000 entries have been processed. 8589000000 left.\n",
      "1.254 secs per chunk on average. Meaning  179.460 minutes left.\n",
      "Going through chunk 11...\n",
      "The first 12000000 entries have been processed. 8588000000 left.\n",
      "1.244 secs per chunk on average. Meaning  178.003 minutes left.\n",
      "Going through chunk 12...\n",
      "The first 13000000 entries have been processed. 8587000000 left.\n",
      "1.247 secs per chunk on average. Meaning  178.480 minutes left.\n",
      "Going through chunk 13...\n",
      "The first 14000000 entries have been processed. 8586000000 left.\n",
      "1.244 secs per chunk on average. Meaning  178.085 minutes left.\n",
      "Going through chunk 14...\n",
      "The first 15000000 entries have been processed. 8585000000 left.\n",
      "1.238 secs per chunk on average. Meaning  177.148 minutes left.\n",
      "Going through chunk 15...\n",
      "The first 16000000 entries have been processed. 8584000000 left.\n",
      "1.237 secs per chunk on average. Meaning  177.032 minutes left.\n",
      "Going through chunk 16...\n",
      "The first 17000000 entries have been processed. 8583000000 left.\n",
      "1.235 secs per chunk on average. Meaning  176.637 minutes left.\n",
      "Going through chunk 17...\n",
      "The first 18000000 entries have been processed. 8582000000 left.\n",
      "1.234 secs per chunk on average. Meaning  176.545 minutes left.\n",
      "Going through chunk 18...\n",
      "The first 19000000 entries have been processed. 8581000000 left.\n",
      "1.234 secs per chunk on average. Meaning  176.469 minutes left.\n",
      "Going through chunk 19...\n",
      "The first 20000000 entries have been processed. 8580000000 left.\n",
      "1.236 secs per chunk on average. Meaning  176.772 minutes left.\n",
      "Going through chunk 20...\n",
      "The first 21000000 entries have been processed. 8579000000 left.\n",
      "1.235 secs per chunk on average. Meaning  176.582 minutes left.\n",
      "Going through chunk 21...\n",
      "The first 22000000 entries have been processed. 8578000000 left.\n",
      "1.235 secs per chunk on average. Meaning  176.517 minutes left.\n",
      "Going through chunk 22...\n",
      "The first 23000000 entries have been processed. 8577000000 left.\n",
      "1.233 secs per chunk on average. Meaning  176.239 minutes left.\n",
      "Going through chunk 23...\n",
      "The first 24000000 entries have been processed. 8576000000 left.\n",
      "1.216 secs per chunk on average. Meaning  173.866 minutes left.\n",
      "Going through chunk 24...\n",
      "The first 25000000 entries have been processed. 8575000000 left.\n",
      "1.217 secs per chunk on average. Meaning  173.914 minutes left.\n",
      "Going through chunk 25...\n",
      "The first 26000000 entries have been processed. 8574000000 left.\n",
      "1.219 secs per chunk on average. Meaning  174.214 minutes left.\n",
      "Going through chunk 26...\n",
      "The first 27000000 entries have been processed. 8573000000 left.\n",
      "1.204 secs per chunk on average. Meaning  172.078 minutes left.\n",
      "Going through chunk 27...\n",
      "The first 28000000 entries have been processed. 8572000000 left.\n",
      "1.188 secs per chunk on average. Meaning  169.749 minutes left.\n",
      "Going through chunk 28...\n",
      "The first 29000000 entries have been processed. 8571000000 left.\n",
      "1.172 secs per chunk on average. Meaning  167.491 minutes left.\n",
      "Going through chunk 29...\n",
      "The first 30000000 entries have been processed. 8570000000 left.\n",
      "1.158 secs per chunk on average. Meaning  165.421 minutes left.\n",
      "Going through chunk 30...\n",
      "The first 31000000 entries have been processed. 8569000000 left.\n",
      "1.144 secs per chunk on average. Meaning  163.327 minutes left.\n",
      "Going through chunk 31...\n",
      "The first 32000000 entries have been processed. 8568000000 left.\n",
      "1.129 secs per chunk on average. Meaning  161.282 minutes left.\n",
      "Going through chunk 32...\n",
      "The first 33000000 entries have been processed. 8567000000 left.\n",
      "1.118 secs per chunk on average. Meaning  159.568 minutes left.\n",
      "Going through chunk 33...\n",
      "The first 34000000 entries have been processed. 8566000000 left.\n",
      "1.106 secs per chunk on average. Meaning  157.894 minutes left.\n",
      "Going through chunk 34...\n",
      "The first 35000000 entries have been processed. 8565000000 left.\n",
      "1.096 secs per chunk on average. Meaning  156.384 minutes left.\n",
      "Going through chunk 35...\n",
      "The first 36000000 entries have been processed. 8564000000 left.\n",
      "1.088 secs per chunk on average. Meaning  155.250 minutes left.\n",
      "Going through chunk 36...\n",
      "The first 37000000 entries have been processed. 8563000000 left.\n",
      "1.079 secs per chunk on average. Meaning  153.989 minutes left.\n",
      "Going through chunk 37...\n",
      "The first 38000000 entries have been processed. 8562000000 left.\n",
      "1.072 secs per chunk on average. Meaning  152.984 minutes left.\n",
      "Going through chunk 38...\n",
      "The first 39000000 entries have been processed. 8561000000 left.\n",
      "1.063 secs per chunk on average. Meaning  151.661 minutes left.\n",
      "Going through chunk 39...\n",
      "The first 40000000 entries have been processed. 8560000000 left.\n",
      "1.056 secs per chunk on average. Meaning  150.701 minutes left.\n",
      "Going through chunk 40...\n",
      "The first 41000000 entries have been processed. 8559000000 left.\n",
      "1.049 secs per chunk on average. Meaning  149.647 minutes left.\n",
      "Going through chunk 41...\n",
      "The first 42000000 entries have been processed. 8558000000 left.\n",
      "1.040 secs per chunk on average. Meaning  148.403 minutes left.\n",
      "Going through chunk 42...\n",
      "The first 43000000 entries have been processed. 8557000000 left.\n",
      "1.036 secs per chunk on average. Meaning  147.724 minutes left.\n",
      "Going through chunk 43...\n",
      "The first 44000000 entries have been processed. 8556000000 left.\n",
      "1.029 secs per chunk on average. Meaning  146.698 minutes left.\n",
      "Going through chunk 44...\n",
      "The first 45000000 entries have been processed. 8555000000 left.\n",
      "1.022 secs per chunk on average. Meaning  145.676 minutes left.\n",
      "Going through chunk 45...\n",
      "The first 46000000 entries have been processed. 8554000000 left.\n",
      "1.015 secs per chunk on average. Meaning  144.750 minutes left.\n",
      "Going through chunk 46...\n",
      "The first 47000000 entries have been processed. 8553000000 left.\n",
      "1.010 secs per chunk on average. Meaning  143.945 minutes left.\n",
      "Going through chunk 47...\n",
      "The first 48000000 entries have been processed. 8552000000 left.\n",
      "1.004 secs per chunk on average. Meaning  143.123 minutes left.\n",
      "Going through chunk 48...\n",
      "The first 49000000 entries have been processed. 8551000000 left.\n",
      "0.999 secs per chunk on average. Meaning  142.411 minutes left.\n",
      "Going through chunk 49...\n",
      "The first 50000000 entries have been processed. 8550000000 left.\n",
      "0.995 secs per chunk on average. Meaning  141.760 minutes left.\n",
      "Going through chunk 50...\n",
      "The first 51000000 entries have been processed. 8549000000 left.\n",
      "0.998 secs per chunk on average. Meaning  142.196 minutes left.\n",
      "Going through chunk 51...\n",
      "The first 52000000 entries have been processed. 8548000000 left.\n",
      "1.005 secs per chunk on average. Meaning  143.133 minutes left.\n",
      "Going through chunk 52...\n",
      "The first 53000000 entries have been processed. 8547000000 left.\n",
      "1.010 secs per chunk on average. Meaning  143.923 minutes left.\n",
      "Going through chunk 53...\n",
      "The first 54000000 entries have been processed. 8546000000 left.\n",
      "1.015 secs per chunk on average. Meaning  144.567 minutes left.\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get the entries of the comment dataframe which have a na value in any column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nans \u001b[38;5;241m=\u001b[39m \u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_simple_function_on_chunks_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_na_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43many\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mprint_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8600000000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Nextcloud/Dokumente/Uni/Module/3sem-EPFL/ada/Project/ada-2024-project-thedataminions/preprocessing_tests/data_processing.py:39\u001b[0m, in \u001b[0;36mrun_simple_function_on_chunks_concat\u001b[0;34m(reader, fct, print_time)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     time_start_global \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGoing through chunk \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# get the entries of the comment dataframe which have a na value in any column\n",
    "nans = dp.run_simple_function_on_chunks_concat(comments_in_chunks(), \n",
    "                                        lambda x: dp.get_na_entries(x, \"any\", reverse=False),\n",
    "                                        print_time=(1000000, 8600000000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>video_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, video_id, likes, replies]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "0.730 secs per chunk on average. Meaning  104.669 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "0.726 secs per chunk on average. Meaning  104.055 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "0.745 secs per chunk on average. Meaning  106.686 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "0.736 secs per chunk on average. Meaning  105.499 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "0.756 secs per chunk on average. Meaning  108.361 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "0.757 secs per chunk on average. Meaning  108.372 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "0.758 secs per chunk on average. Meaning  108.604 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "0.769 secs per chunk on average. Meaning  110.185 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "0.767 secs per chunk on average. Meaning  109.795 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "0.774 secs per chunk on average. Meaning  110.804 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# count the entries of the comment dataframe which have a na value in any column\n",
    "counted_nans = dp.run_simple_function_on_chunks_concat(comments_in_chunks(), \n",
    "                                                lambda x: dp.count_na_entries(x, \"any\", reverse=False),\n",
    "                                                print_time=(1000000, 8600000000)).sum(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "na rows              0\n",
       "total rows    10000000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(counted_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the datasets to our needs\n",
    "\n",
    "### Filtering videos and channels by category News & Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 100000 entries have been processed. 72824794 left.\n",
      "3.634 secs per chunk on average. Meaning  44.105 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 200000 entries have been processed. 72724794 left.\n",
      "3.977 secs per chunk on average. Meaning  48.209 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 300000 entries have been processed. 72624794 left.\n",
      "4.049 secs per chunk on average. Meaning  49.011 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 400000 entries have been processed. 72524794 left.\n",
      "3.995 secs per chunk on average. Meaning  48.284 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 500000 entries have been processed. 72424794 left.\n",
      "3.776 secs per chunk on average. Meaning  45.584 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 600000 entries have been processed. 72324794 left.\n",
      "3.477 secs per chunk on average. Meaning  41.913 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 700000 entries have been processed. 72224794 left.\n",
      "3.283 secs per chunk on average. Meaning  39.523 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 800000 entries have been processed. 72124794 left.\n",
      "3.133 secs per chunk on average. Meaning  37.660 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 900000 entries have been processed. 72024794 left.\n",
      "3.113 secs per chunk on average. Meaning  37.370 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 1000000 entries have been processed. 71924794 left.\n",
      "3.195 secs per chunk on average. Meaning  38.296 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# filter the video dataframe to only include videos from news and politics category\n",
    "\n",
    "df_videos_news_pol = dp.run_simple_function_on_chunks_concat(videos_in_chunks(chunksize=100000),\n",
    "                                                             lambda x: x[x.categories == \"News & Politics\"], \n",
    "                                                             print_time=(100000, 72924794))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels_news_pol = df_channels[df_channels.category_cc == \"News & Politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWm1-4XF7AHxVUTkHCM1uw</td>\n",
       "      <td>2019-11-17 06:28:42.593675</td>\n",
       "      <td>retrogamer3.com</td>\n",
       "      <td>16.0</td>\n",
       "      <td>dfa8RRkKoa4</td>\n",
       "      <td>9251</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RetroGamer3,Live Stream,politics,Trump</td>\n",
       "      <td>Retrogamer3 Political Stream</td>\n",
       "      <td>2018-08-23 00:00:00</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWLsxDD373D4tY8kN-0LGQ</td>\n",
       "      <td>2019-11-05 00:42:33.012228</td>\n",
       "      <td>What are the forces at work that have created ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>_dIIEMvH86k</td>\n",
       "      <td>309</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NWO,Ebola,Ukraine,Mainstream,Media,Pyschology</td>\n",
       "      <td>Adam Curtis describes the Surkow Strategy of M...</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18005</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzVBu6oqlrAix0oq9T2rBFg</td>\n",
       "      <td>2019-11-19 20:40:22.403775</td>\n",
       "      <td>Social Media:\\n\\nFacebook.com/thebookoflaura\\n...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>eWXefhNB2po</td>\n",
       "      <td>707</td>\n",
       "      <td>625.0</td>\n",
       "      <td>michael jackson,lyrics,music video,court,child...</td>\n",
       "      <td>my thoughts on the michael jackson documentary.</td>\n",
       "      <td>2019-04-24 00:00:00</td>\n",
       "      <td>12780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24361</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:47:10.520209</td>\n",
       "      <td>ðŸ‘• Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>MBgzne7djFU</td>\n",
       "      <td>378</td>\n",
       "      <td>47027.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>Elizabeth Warren Gets a Big Surprise at the Ai...</td>\n",
       "      <td>2019-10-03 00:00:00</td>\n",
       "      <td>374711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24362</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:46:16.481889</td>\n",
       "      <td>ðŸ‘• Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>AbH3pJnFgY8</td>\n",
       "      <td>278</td>\n",
       "      <td>36384.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>No More Twitter? ðŸ˜‚</td>\n",
       "      <td>2019-10-02 00:00:00</td>\n",
       "      <td>245617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999870</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:26.687460</td>\n",
       "      <td>The Young Turks recently posted a video entitl...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Rmq0JmUbt8k</td>\n",
       "      <td>857</td>\n",
       "      <td>25.0</td>\n",
       "      <td>American Joe,American Joe Show,The Young Turks...</td>\n",
       "      <td>Young Turks Caught Lying and Race Baiting.... ...</td>\n",
       "      <td>2018-11-17 00:00:00</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999871</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:27.273595</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ts__Orp310M</td>\n",
       "      <td>49</td>\n",
       "      <td>34.0</td>\n",
       "      <td>American Joe,American Joe Show</td>\n",
       "      <td>President says he will send migrant Children B...</td>\n",
       "      <td>2018-11-15 00:00:00</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999872</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:27.847348</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bQ3_ZMVpiio</td>\n",
       "      <td>298</td>\n",
       "      <td>6.0</td>\n",
       "      <td>American Joe,American Joe Show,Michael Avenatt...</td>\n",
       "      <td>Creepy Porn Lawyer, and Woman Beater Michael A...</td>\n",
       "      <td>2018-11-14 00:00:00</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999873</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:28.400609</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>q92A939Nyj8</td>\n",
       "      <td>388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>American Joe,American Joe Show,Midterm Electio...</td>\n",
       "      <td>Midterm Fallout - How Bad is it For Trump?</td>\n",
       "      <td>2018-11-14 00:00:00</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999874</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:32.321224</td>\n",
       "      <td>Link to video by Conservative Youtuber ABL, An...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CPYdzsvg-Ns</td>\n",
       "      <td>573</td>\n",
       "      <td>18.0</td>\n",
       "      <td>American Joe,American Joe Show,Jim Acosta Dona...</td>\n",
       "      <td>Trump Vs. Jim Acosta - The Fake News is at it ...</td>\n",
       "      <td>2018-11-11 00:00:00</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145768 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             categories                channel_id                  crawl_date  \\\n",
       "1827    News & Politics  UCzWm1-4XF7AHxVUTkHCM1uw  2019-11-17 06:28:42.593675   \n",
       "7605    News & Politics  UCzWLsxDD373D4tY8kN-0LGQ  2019-11-05 00:42:33.012228   \n",
       "18005   News & Politics  UCzVBu6oqlrAix0oq9T2rBFg  2019-11-19 20:40:22.403775   \n",
       "24361   News & Politics  UCzUV5283-l5c0oKRtyenj6Q  2019-11-22 08:47:10.520209   \n",
       "24362   News & Politics  UCzUV5283-l5c0oKRtyenj6Q  2019-11-22 08:46:16.481889   \n",
       "...                 ...                       ...                         ...   \n",
       "999870  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:26.687460   \n",
       "999871  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:27.273595   \n",
       "999872  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:27.847348   \n",
       "999873  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:28.400609   \n",
       "999874  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:32.321224   \n",
       "\n",
       "                                              description  dislike_count  \\\n",
       "1827                                      retrogamer3.com           16.0   \n",
       "7605    What are the forces at work that have created ...            0.0   \n",
       "18005   Social Media:\\n\\nFacebook.com/thebookoflaura\\n...           89.0   \n",
       "24361   ðŸ‘• Order your shirts here: https://Teespring.co...          195.0   \n",
       "24362   ðŸ‘• Order your shirts here: https://Teespring.co...          114.0   \n",
       "...                                                   ...            ...   \n",
       "999870  The Young Turks recently posted a video entitl...            2.0   \n",
       "999871  Patriots I need your help growing the American...            0.0   \n",
       "999872  Patriots I need your help growing the American...            1.0   \n",
       "999873  Patriots I need your help growing the American...            2.0   \n",
       "999874  Link to video by Conservative Youtuber ABL, An...            2.0   \n",
       "\n",
       "         display_id  duration  like_count  \\\n",
       "1827    dfa8RRkKoa4      9251        25.0   \n",
       "7605    _dIIEMvH86k       309         9.0   \n",
       "18005   eWXefhNB2po       707       625.0   \n",
       "24361   MBgzne7djFU       378     47027.0   \n",
       "24362   AbH3pJnFgY8       278     36384.0   \n",
       "...             ...       ...         ...   \n",
       "999870  Rmq0JmUbt8k       857        25.0   \n",
       "999871  ts__Orp310M        49        34.0   \n",
       "999872  bQ3_ZMVpiio       298         6.0   \n",
       "999873  q92A939Nyj8       388         2.0   \n",
       "999874  CPYdzsvg-Ns       573        18.0   \n",
       "\n",
       "                                                     tags  \\\n",
       "1827               RetroGamer3,Live Stream,politics,Trump   \n",
       "7605        NWO,Ebola,Ukraine,Mainstream,Media,Pyschology   \n",
       "18005   michael jackson,lyrics,music video,court,child...   \n",
       "24361   Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "24362   Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "...                                                   ...   \n",
       "999870  American Joe,American Joe Show,The Young Turks...   \n",
       "999871                     American Joe,American Joe Show   \n",
       "999872  American Joe,American Joe Show,Michael Avenatt...   \n",
       "999873  American Joe,American Joe Show,Midterm Electio...   \n",
       "999874  American Joe,American Joe Show,Jim Acosta Dona...   \n",
       "\n",
       "                                                    title  \\\n",
       "1827                         Retrogamer3 Political Stream   \n",
       "7605    Adam Curtis describes the Surkow Strategy of M...   \n",
       "18005     my thoughts on the michael jackson documentary.   \n",
       "24361   Elizabeth Warren Gets a Big Surprise at the Ai...   \n",
       "24362                                  No More Twitter? ðŸ˜‚   \n",
       "...                                                   ...   \n",
       "999870  Young Turks Caught Lying and Race Baiting.... ...   \n",
       "999871  President says he will send migrant Children B...   \n",
       "999872  Creepy Porn Lawyer, and Woman Beater Michael A...   \n",
       "999873         Midterm Fallout - How Bad is it For Trump?   \n",
       "999874  Trump Vs. Jim Acosta - The Fake News is at it ...   \n",
       "\n",
       "                upload_date  view_count  \n",
       "1827    2018-08-23 00:00:00       478.0  \n",
       "7605    2015-01-04 00:00:00       865.0  \n",
       "18005   2019-04-24 00:00:00     12780.0  \n",
       "24361   2019-10-03 00:00:00    374711.0  \n",
       "24362   2019-10-02 00:00:00    245617.0  \n",
       "...                     ...         ...  \n",
       "999870  2018-11-17 00:00:00       273.0  \n",
       "999871  2018-11-15 00:00:00       353.0  \n",
       "999872  2018-11-14 00:00:00        76.0  \n",
       "999873  2018-11-14 00:00:00        38.0  \n",
       "999874  2018-11-11 00:00:00       146.0  \n",
       "\n",
       "[145768 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_videos_news_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2006-08-26</td>\n",
       "      <td>UCttspZesZIDEwwpVIgoZtWQ</td>\n",
       "      <td>IndiaTV</td>\n",
       "      <td>15177282</td>\n",
       "      <td>139814</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>UCRWFSbif-RFENbBrSiez1DA</td>\n",
       "      <td>ABP NEWS</td>\n",
       "      <td>16274836</td>\n",
       "      <td>129027</td>\n",
       "      <td>207.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>UCmphdqZNmqL72WJ2uyiNw5w</td>\n",
       "      <td>ABP NEWS HINDI</td>\n",
       "      <td>10800000</td>\n",
       "      <td>51298</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2015-03-23</td>\n",
       "      <td>UCx8Z14PpntdaxCt2hakbQLQ</td>\n",
       "      <td>The Lallantop</td>\n",
       "      <td>9120000</td>\n",
       "      <td>9423</td>\n",
       "      <td>438.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2007-06-19</td>\n",
       "      <td>UCIvaYmXn910QMdemBG3v1pQ</td>\n",
       "      <td>Zee News</td>\n",
       "      <td>9280000</td>\n",
       "      <td>102648</td>\n",
       "      <td>549.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135820</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2010-08-07</td>\n",
       "      <td>UC5rxiCGcNunIi5zI1hMYLMg</td>\n",
       "      <td>Salman Akhtar</td>\n",
       "      <td>10400</td>\n",
       "      <td>40</td>\n",
       "      <td>962468.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135825</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>UCLSEJQ8TWtlEkaytaa4Y7lw</td>\n",
       "      <td>WingsOfChrist</td>\n",
       "      <td>10420</td>\n",
       "      <td>61</td>\n",
       "      <td>962547.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135901</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>UCnkG_c5cyemVVsgCDoHiXew</td>\n",
       "      <td>The American Mirror</td>\n",
       "      <td>10500</td>\n",
       "      <td>329</td>\n",
       "      <td>963417.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136231</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>UC69lWS7UMbBQc-9yqp4nGjA</td>\n",
       "      <td>Patriotism Show</td>\n",
       "      <td>10320</td>\n",
       "      <td>46</td>\n",
       "      <td>975448.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136301</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>UCpbE1CJWNHpu8knuok8YBZQ</td>\n",
       "      <td>Jenny Constantine</td>\n",
       "      <td>10200</td>\n",
       "      <td>30</td>\n",
       "      <td>978433.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            category_cc   join_date                   channel  \\\n",
       "129     News & Politics  2006-08-26  UCttspZesZIDEwwpVIgoZtWQ   \n",
       "133     News & Politics  2012-06-01  UCRWFSbif-RFENbBrSiez1DA   \n",
       "212     News & Politics  2017-03-03  UCmphdqZNmqL72WJ2uyiNw5w   \n",
       "268     News & Politics  2015-03-23  UCx8Z14PpntdaxCt2hakbQLQ   \n",
       "337     News & Politics  2007-06-19  UCIvaYmXn910QMdemBG3v1pQ   \n",
       "...                 ...         ...                       ...   \n",
       "135820  News & Politics  2010-08-07  UC5rxiCGcNunIi5zI1hMYLMg   \n",
       "135825  News & Politics  2013-02-01  UCLSEJQ8TWtlEkaytaa4Y7lw   \n",
       "135901  News & Politics  2012-10-19  UCnkG_c5cyemVVsgCDoHiXew   \n",
       "136231  News & Politics  2017-11-25  UC69lWS7UMbBQc-9yqp4nGjA   \n",
       "136301  News & Politics  2017-03-26  UCpbE1CJWNHpu8knuok8YBZQ   \n",
       "\n",
       "                    name_cc  subscribers_cc  videos_cc  subscriber_rank_sb  \\\n",
       "129                 IndiaTV        15177282     139814               199.0   \n",
       "133                ABP NEWS        16274836     129027               207.0   \n",
       "212          ABP NEWS HINDI        10800000      51298               340.0   \n",
       "268           The Lallantop         9120000       9423               438.0   \n",
       "337                Zee News         9280000     102648               549.0   \n",
       "...                     ...             ...        ...                 ...   \n",
       "135820        Salman Akhtar           10400         40            962468.0   \n",
       "135825        WingsOfChrist           10420         61            962547.0   \n",
       "135901  The American Mirror           10500        329            963417.0   \n",
       "136231      Patriotism Show           10320         46            975448.0   \n",
       "136301    Jenny Constantine           10200         30            978433.0   \n",
       "\n",
       "        weights  \n",
       "129      2.0870  \n",
       "133      2.0870  \n",
       "212      2.0870  \n",
       "268      2.0870  \n",
       "337      2.0870  \n",
       "...         ...  \n",
       "135820  53.1435  \n",
       "135825  53.1435  \n",
       "135901  53.1435  \n",
       "136231  53.1435  \n",
       "136301  53.1435  \n",
       "\n",
       "[2263 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_channels_news_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWm1-4XF7AHxVUTkHCM1uw</td>\n",
       "      <td>2019-11-17 06:28:42.593675</td>\n",
       "      <td>retrogamer3.com</td>\n",
       "      <td>16.0</td>\n",
       "      <td>dfa8RRkKoa4</td>\n",
       "      <td>9251</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RetroGamer3,Live Stream,politics,Trump</td>\n",
       "      <td>Retrogamer3 Political Stream</td>\n",
       "      <td>2018-08-23 00:00:00</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWLsxDD373D4tY8kN-0LGQ</td>\n",
       "      <td>2019-11-05 00:42:33.012228</td>\n",
       "      <td>What are the forces at work that have created ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>_dIIEMvH86k</td>\n",
       "      <td>309</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NWO,Ebola,Ukraine,Mainstream,Media,Pyschology</td>\n",
       "      <td>Adam Curtis describes the Surkow Strategy of M...</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18005</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzVBu6oqlrAix0oq9T2rBFg</td>\n",
       "      <td>2019-11-19 20:40:22.403775</td>\n",
       "      <td>Social Media:\\n\\nFacebook.com/thebookoflaura\\n...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>eWXefhNB2po</td>\n",
       "      <td>707</td>\n",
       "      <td>625.0</td>\n",
       "      <td>michael jackson,lyrics,music video,court,child...</td>\n",
       "      <td>my thoughts on the michael jackson documentary.</td>\n",
       "      <td>2019-04-24 00:00:00</td>\n",
       "      <td>12780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28840</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzTmNzBxLEHbpZNOCpUTWbA</td>\n",
       "      <td>2019-11-03 04:38:01.617657</td>\n",
       "      <td>A young man is living a normal life with no ca...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>ck6Yl8TNoWs</td>\n",
       "      <td>1257</td>\n",
       "      <td>452.0</td>\n",
       "      <td>JoiRida,Cheatham,JoiRidaCheatham,Accepted,Detr...</td>\n",
       "      <td>Accepted - Award Winning Short Film</td>\n",
       "      <td>2013-10-13 00:00:00</td>\n",
       "      <td>27366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28860</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzTmNzBxLEHbpZNOCpUTWbA</td>\n",
       "      <td>2019-11-03 04:38:06.565138</td>\n",
       "      <td>Short Film</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tjUajxZAIZ8</td>\n",
       "      <td>422</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Joi.Rida,Cheatham,joiridacheatham,dread,loc,up...</td>\n",
       "      <td>JoiRida Twin Visit (Introducing Jive Viper)</td>\n",
       "      <td>2010-03-04 00:00:00</td>\n",
       "      <td>987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970869</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrXcatz6wlNHjuqgf-tglOA</td>\n",
       "      <td>2019-11-07 00:55:48.241832</td>\n",
       "      <td>As promised, our Wet Head Challenge using the ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>lIuK9DGtOx8</td>\n",
       "      <td>321</td>\n",
       "      <td>141.0</td>\n",
       "      <td>challenge,wet,head,gross,wet head challenge,we...</td>\n",
       "      <td>Gross Smoothie Wet Head Challenge ðŸ˜•</td>\n",
       "      <td>2016-08-23 00:00:00</td>\n",
       "      <td>8941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991815</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrVnMcE3GIyg2rM4gH34YWg</td>\n",
       "      <td>2019-11-10 10:02:03.075065</td>\n",
       "      <td>More Travel News...\\nhttp://www.petergreenberg...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aSWbywb7SBE</td>\n",
       "      <td>423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2008,Travel Inspiration,clinton,Presidential,P...</td>\n",
       "      <td>2008 Presidential Candidates Travel Scorecard</td>\n",
       "      <td>2008-01-25 00:00:00</td>\n",
       "      <td>588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998347</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:30.119011</td>\n",
       "      <td>Â© 2012 WMG  Webisode by Mutemath from The Blue...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H8-Al6B_J1g</td>\n",
       "      <td>106</td>\n",
       "      <td>17.0</td>\n",
       "      <td>mutemath,wbr,INDMUSIC,warner bros records</td>\n",
       "      <td>Mutemath - What Happens Before The Show [Webis...</td>\n",
       "      <td>2006-11-04 00:00:00</td>\n",
       "      <td>3136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998349</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:31.498209</td>\n",
       "      <td>Â© 2012 WMG  Webisode by Mutemath from Park Wes...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>wYI6dWaEHjk</td>\n",
       "      <td>56</td>\n",
       "      <td>8.0</td>\n",
       "      <td>INDMUSIC,wbr,mutemath,warner bros records</td>\n",
       "      <td>Mutemath - Built for Destruction [Webisode]</td>\n",
       "      <td>2006-11-04 00:00:00</td>\n",
       "      <td>1881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998365</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:41.975562</td>\n",
       "      <td>Â© 2012 WMG  Webisode by Mutemath from The Goth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gs5jw7rtrn8</td>\n",
       "      <td>216</td>\n",
       "      <td>27.0</td>\n",
       "      <td>wbr,warner bros records,INDMUSIC</td>\n",
       "      <td>Mutemath - The Gothic Theatre in Denver [Webis...</td>\n",
       "      <td>2005-12-31 00:00:00</td>\n",
       "      <td>7916.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8564 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             categories                channel_id                  crawl_date  \\\n",
       "1827    News & Politics  UCzWm1-4XF7AHxVUTkHCM1uw  2019-11-17 06:28:42.593675   \n",
       "7605    News & Politics  UCzWLsxDD373D4tY8kN-0LGQ  2019-11-05 00:42:33.012228   \n",
       "18005   News & Politics  UCzVBu6oqlrAix0oq9T2rBFg  2019-11-19 20:40:22.403775   \n",
       "28840   News & Politics  UCzTmNzBxLEHbpZNOCpUTWbA  2019-11-03 04:38:01.617657   \n",
       "28860   News & Politics  UCzTmNzBxLEHbpZNOCpUTWbA  2019-11-03 04:38:06.565138   \n",
       "...                 ...                       ...                         ...   \n",
       "970869  News & Politics  UCrXcatz6wlNHjuqgf-tglOA  2019-11-07 00:55:48.241832   \n",
       "991815  News & Politics  UCrVnMcE3GIyg2rM4gH34YWg  2019-11-10 10:02:03.075065   \n",
       "998347  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:30.119011   \n",
       "998349  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:31.498209   \n",
       "998365  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:41.975562   \n",
       "\n",
       "                                              description  dislike_count  \\\n",
       "1827                                      retrogamer3.com           16.0   \n",
       "7605    What are the forces at work that have created ...            0.0   \n",
       "18005   Social Media:\\n\\nFacebook.com/thebookoflaura\\n...           89.0   \n",
       "28840   A young man is living a normal life with no ca...           16.0   \n",
       "28860                                          Short Film            1.0   \n",
       "...                                                   ...            ...   \n",
       "970869  As promised, our Wet Head Challenge using the ...            3.0   \n",
       "991815  More Travel News...\\nhttp://www.petergreenberg...            0.0   \n",
       "998347  Â© 2012 WMG  Webisode by Mutemath from The Blue...            3.0   \n",
       "998349  Â© 2012 WMG  Webisode by Mutemath from Park Wes...            2.0   \n",
       "998365  Â© 2012 WMG  Webisode by Mutemath from The Goth...            0.0   \n",
       "\n",
       "         display_id  duration  like_count  \\\n",
       "1827    dfa8RRkKoa4      9251        25.0   \n",
       "7605    _dIIEMvH86k       309         9.0   \n",
       "18005   eWXefhNB2po       707       625.0   \n",
       "28840   ck6Yl8TNoWs      1257       452.0   \n",
       "28860   tjUajxZAIZ8       422        15.0   \n",
       "...             ...       ...         ...   \n",
       "970869  lIuK9DGtOx8       321       141.0   \n",
       "991815  aSWbywb7SBE       423         1.0   \n",
       "998347  H8-Al6B_J1g       106        17.0   \n",
       "998349  wYI6dWaEHjk        56         8.0   \n",
       "998365  Gs5jw7rtrn8       216        27.0   \n",
       "\n",
       "                                                     tags  \\\n",
       "1827               RetroGamer3,Live Stream,politics,Trump   \n",
       "7605        NWO,Ebola,Ukraine,Mainstream,Media,Pyschology   \n",
       "18005   michael jackson,lyrics,music video,court,child...   \n",
       "28840   JoiRida,Cheatham,JoiRidaCheatham,Accepted,Detr...   \n",
       "28860   Joi.Rida,Cheatham,joiridacheatham,dread,loc,up...   \n",
       "...                                                   ...   \n",
       "970869  challenge,wet,head,gross,wet head challenge,we...   \n",
       "991815  2008,Travel Inspiration,clinton,Presidential,P...   \n",
       "998347          mutemath,wbr,INDMUSIC,warner bros records   \n",
       "998349          INDMUSIC,wbr,mutemath,warner bros records   \n",
       "998365                   wbr,warner bros records,INDMUSIC   \n",
       "\n",
       "                                                    title  \\\n",
       "1827                         Retrogamer3 Political Stream   \n",
       "7605    Adam Curtis describes the Surkow Strategy of M...   \n",
       "18005     my thoughts on the michael jackson documentary.   \n",
       "28840                 Accepted - Award Winning Short Film   \n",
       "28860         JoiRida Twin Visit (Introducing Jive Viper)   \n",
       "...                                                   ...   \n",
       "970869                Gross Smoothie Wet Head Challenge ðŸ˜•   \n",
       "991815      2008 Presidential Candidates Travel Scorecard   \n",
       "998347  Mutemath - What Happens Before The Show [Webis...   \n",
       "998349        Mutemath - Built for Destruction [Webisode]   \n",
       "998365  Mutemath - The Gothic Theatre in Denver [Webis...   \n",
       "\n",
       "                upload_date  view_count  \n",
       "1827    2018-08-23 00:00:00       478.0  \n",
       "7605    2015-01-04 00:00:00       865.0  \n",
       "18005   2019-04-24 00:00:00     12780.0  \n",
       "28840   2013-10-13 00:00:00     27366.0  \n",
       "28860   2010-03-04 00:00:00       987.0  \n",
       "...                     ...         ...  \n",
       "970869  2016-08-23 00:00:00      8941.0  \n",
       "991815  2008-01-25 00:00:00       588.0  \n",
       "998347  2006-11-04 00:00:00      3136.0  \n",
       "998349  2006-11-04 00:00:00      1881.0  \n",
       "998365  2005-12-31 00:00:00      7916.0  \n",
       "\n",
       "[8564 rows x 12 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all videos we found in news&pol are also pulished by a channel in category news&pol\n",
    "\n",
    "df_videos_news_pol[np.logical_not(df_videos_news_pol.channel_id.isin(df_channels_news_pol.channel))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that indeed, not all videos in the news and politics category belong to a channel in this category!**\n",
    "A google search shows that apparently, you don't have to have the same category for all videos, but you set a \"default\" channel category which will be used for videos if you don't change it manually. Also, you can probably change the default category after a while if you want.\n",
    "\n",
    "This is the reason why most of the news&pol videos are uploaded by a news&pol channe, but not all.\n",
    "\n",
    "In the paper about the dataset, the authors say that the channel category is actually the \"most frequent category\", so I guess the video categories are the most relevant, as they are the true categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to verify this, if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the News&Pol videos by a list of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter news&pol videos to only include the channels we are interested in\n",
    "\n",
    "# Note: this selection of channels is still \"random\" (from my head), the final selection whould be from a source\n",
    "\n",
    "df_videos_news_pol_manually_selected = df_videos_news_pol[df_videos_news_pol.channel_id.isin([\"UCupvZG-5ko_eiXAupbDfxWw\",  # CNN\n",
    "                                                                                              \"UCXIJgqnII2ZOINSWNOGFThA\",  # Fox News\n",
    "                                                                                              \"UC16niRr50-MSBwiO3YDb3RA\",  # BBC News\n",
    "                                                                                              \"UCaXkIU1QidjPwiAYu6GcHjg\",  # MSNBC\n",
    "                                                                                            ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [categories, channel_id, crawl_date, description, dislike_count, display_id, duration, like_count, tags, title, upload_date, view_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos_news_pol_manually_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we probably don't need this\n",
    "\n",
    "# sort news&pol channels by subscriber count according to channel crawler\n",
    "df_channels_news_pol_sort_subscribers = df_channels_news_pol.sort_values(by=\"subscribers_cc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>UCRWFSbif-RFENbBrSiez1DA</td>\n",
       "      <td>ABP NEWS</td>\n",
       "      <td>16274836</td>\n",
       "      <td>129027</td>\n",
       "      <td>207.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2006-08-26</td>\n",
       "      <td>UCttspZesZIDEwwpVIgoZtWQ</td>\n",
       "      <td>IndiaTV</td>\n",
       "      <td>15177282</td>\n",
       "      <td>139814</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>UCmphdqZNmqL72WJ2uyiNw5w</td>\n",
       "      <td>ABP NEWS HINDI</td>\n",
       "      <td>10800000</td>\n",
       "      <td>51298</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2007-06-19</td>\n",
       "      <td>UCIvaYmXn910QMdemBG3v1pQ</td>\n",
       "      <td>Zee News</td>\n",
       "      <td>9280000</td>\n",
       "      <td>102648</td>\n",
       "      <td>549.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2015-03-23</td>\n",
       "      <td>UCx8Z14PpntdaxCt2hakbQLQ</td>\n",
       "      <td>The Lallantop</td>\n",
       "      <td>9120000</td>\n",
       "      <td>9423</td>\n",
       "      <td>438.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130581</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2008-03-26</td>\n",
       "      <td>UC65jmwvHLoCMBJXnzHrGwiA</td>\n",
       "      <td>The Virginian-Pilot</td>\n",
       "      <td>10200</td>\n",
       "      <td>3037</td>\n",
       "      <td>868834.0</td>\n",
       "      <td>14.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134955</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2009-05-17</td>\n",
       "      <td>UCuZTp4-0xPoGUxdKPLiQE6w</td>\n",
       "      <td>Harold Jackson</td>\n",
       "      <td>10133</td>\n",
       "      <td>1330</td>\n",
       "      <td>932456.0</td>\n",
       "      <td>23.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133413</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2016-03-25</td>\n",
       "      <td>UCWtAa1fyxYxR1gAH-IZtaXg</td>\n",
       "      <td>Trumpennials</td>\n",
       "      <td>10000</td>\n",
       "      <td>64</td>\n",
       "      <td>905989.0</td>\n",
       "      <td>14.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133426</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>UCaUCzPRX1bKUwz7a2aJgIDQ</td>\n",
       "      <td>Missing Persons &amp; My...</td>\n",
       "      <td>10000</td>\n",
       "      <td>31</td>\n",
       "      <td>906002.0</td>\n",
       "      <td>14.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133831</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2008-04-21</td>\n",
       "      <td>UCt0rmyaJboPs29J_YSQucqg</td>\n",
       "      <td>Christian World News...</td>\n",
       "      <td>10000</td>\n",
       "      <td>1184</td>\n",
       "      <td>915543.0</td>\n",
       "      <td>15.959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            category_cc   join_date                   channel  \\\n",
       "133     News & Politics  2012-06-01  UCRWFSbif-RFENbBrSiez1DA   \n",
       "129     News & Politics  2006-08-26  UCttspZesZIDEwwpVIgoZtWQ   \n",
       "212     News & Politics  2017-03-03  UCmphdqZNmqL72WJ2uyiNw5w   \n",
       "337     News & Politics  2007-06-19  UCIvaYmXn910QMdemBG3v1pQ   \n",
       "268     News & Politics  2015-03-23  UCx8Z14PpntdaxCt2hakbQLQ   \n",
       "...                 ...         ...                       ...   \n",
       "130581  News & Politics  2008-03-26  UC65jmwvHLoCMBJXnzHrGwiA   \n",
       "134955  News & Politics  2009-05-17  UCuZTp4-0xPoGUxdKPLiQE6w   \n",
       "133413  News & Politics  2016-03-25  UCWtAa1fyxYxR1gAH-IZtaXg   \n",
       "133426  News & Politics  2018-08-31  UCaUCzPRX1bKUwz7a2aJgIDQ   \n",
       "133831  News & Politics  2008-04-21  UCt0rmyaJboPs29J_YSQucqg   \n",
       "\n",
       "                        name_cc  subscribers_cc  videos_cc  \\\n",
       "133                    ABP NEWS        16274836     129027   \n",
       "129                     IndiaTV        15177282     139814   \n",
       "212              ABP NEWS HINDI        10800000      51298   \n",
       "337                    Zee News         9280000     102648   \n",
       "268               The Lallantop         9120000       9423   \n",
       "...                         ...             ...        ...   \n",
       "130581      The Virginian-Pilot           10200       3037   \n",
       "134955           Harold Jackson           10133       1330   \n",
       "133413             Trumpennials           10000         64   \n",
       "133426  Missing Persons & My...           10000         31   \n",
       "133831  Christian World News...           10000       1184   \n",
       "\n",
       "        subscriber_rank_sb  weights  \n",
       "133                  207.0    2.087  \n",
       "129                  199.0    2.087  \n",
       "212                  340.0    2.087  \n",
       "337                  549.0    2.087  \n",
       "268                  438.0    2.087  \n",
       "...                    ...      ...  \n",
       "130581            868834.0   14.148  \n",
       "134955            932456.0   23.492  \n",
       "133413            905989.0   14.780  \n",
       "133426            906002.0   14.699  \n",
       "133831            915543.0   15.959  \n",
       "\n",
       "[2263 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_channels_news_pol_sort_subscribers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the comments by channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The first function doesn't have to be done on the whole video dataset, as it is done here.\n",
    "The first function can be run on the dataset df_videos_news_pol_manually_selected, as all channels we could be interested in should already have all their videos included in that dataset; no need to consider any other videos. Therefore, we would also not need chunks here.\n",
    "\n",
    "The function could probably instead just be \n",
    "df_videos_news_pol_manually_selected.loc[df_videos_news_pol_manually_selected.channel_id == <channel id of the desired channel>]\n",
    "\n",
    "(Consider this a \n",
    "# Todo\n",
    ")\n",
    "\n",
    "However, the second function probably needs to look like this, as we haven't preprocessed the comment data yet, so it is still too big to be considered all at once, so we need the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 100000 entries have been processed. 72824794 left.\n",
      "3.304 secs per chunk on average. Meaning  40.097 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 200000 entries have been processed. 72724794 left.\n",
      "2.883 secs per chunk on average. Meaning  34.939 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 300000 entries have been processed. 72624794 left.\n",
      "2.706 secs per chunk on average. Meaning  32.759 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 400000 entries have been processed. 72524794 left.\n",
      "2.722 secs per chunk on average. Meaning  32.897 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 500000 entries have been processed. 72424794 left.\n",
      "2.856 secs per chunk on average. Meaning  34.473 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 600000 entries have been processed. 72324794 left.\n",
      "2.942 secs per chunk on average. Meaning  35.458 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 700000 entries have been processed. 72224794 left.\n",
      "3.024 secs per chunk on average. Meaning  36.407 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 800000 entries have been processed. 72124794 left.\n",
      "3.037 secs per chunk on average. Meaning  36.503 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 900000 entries have been processed. 72024794 left.\n",
      "3.144 secs per chunk on average. Meaning  37.741 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 1000000 entries have been processed. 71924794 left.\n",
      "3.214 secs per chunk on average. Meaning  38.525 minutes left.\n",
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "1.309 secs per chunk on average. Meaning  187.633 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "1.372 secs per chunk on average. Meaning  196.576 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "1.333 secs per chunk on average. Meaning  190.990 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.313 secs per chunk on average. Meaning  188.101 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.298 secs per chunk on average. Meaning  185.965 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "1.300 secs per chunk on average. Meaning  186.139 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "1.301 secs per chunk on average. Meaning  186.259 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "1.307 secs per chunk on average. Meaning  187.172 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "1.342 secs per chunk on average. Meaning  192.133 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "1.337 secs per chunk on average. Meaning  191.422 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# filter the video dataset to get only videos from a specific channel (here: just a random channel for testing)\n",
    "\n",
    "videos_from_channel_test = dp.run_simple_function_on_chunks_concat(\n",
    "    videos_in_chunks(chunksize=100000),\n",
    "    lambda x: x.loc[x.channel_id == \"UCzWrhkg9eK5I8Bm3HfV-unA\"],\n",
    "    print_time=(100000, 72924794))\n",
    "\n",
    "# use the filtered video dataset to filter the comment dataset, to get comments on videos from a specific channel\n",
    "\n",
    "comments_from_channel_test = dp.run_simple_function_on_chunks_concat(\n",
    "    comments_in_chunks(chunksize=1000000), \n",
    "    lambda df: df[df.video_id.isin(videos_from_channel_test.display_id)],\n",
    "    print_time=(1000000, 8600000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>video_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868268</th>\n",
       "      <td>453667</td>\n",
       "      <td>3vQK78eUg2A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094579</th>\n",
       "      <td>468696</td>\n",
       "      <td>SWZG-ba1qDk</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912192</th>\n",
       "      <td>594074</td>\n",
       "      <td>hn2zYwqSINY</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author     video_id  likes  replies\n",
       "6868268  453667  3vQK78eUg2A      2        1\n",
       "7094579  468696  SWZG-ba1qDk     15       18\n",
       "8912192  594074  hn2zYwqSINY      0        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(comments_from_channel_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: next step\n",
    "\n",
    "make a function which takes dataframes such as the above \"filtered_comments_test\" (df with comment data from videos only from one specific channel),\n",
    "and returns the number of comments by each user id\n",
    "( filtered_comments_test.groupby(\"user_id\").agg(\"sum\") )\n",
    "\n",
    "sort so that only users with x number of comments remain\n",
    "\n",
    "(flexible function where you give the threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_commenters(comment_channelX,x):\n",
    "    metadata_commenters = comment_channelX.groupby('author').agg(number_of_comments=('author', 'size')).reset_index()\n",
    "    metadata_commenters['number_of_videos']= comment_channelX.groupby('author')['video_id'].nunique().values\n",
    "    #only users with x numbers of comments remain\n",
    "    metadata_commenters=metadata_commenters[metadata_commenters['number_of_comments']>=x]\n",
    "    return metadata_commenters\n",
    "\n",
    "metadata_commenters= get_metadata_commenters(comments_from_channel_test,0)\n",
    "display(metadata_commenters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo after that\n",
    "\n",
    "create dataset of all comments which are under a video in the news and politics category\n",
    "\n",
    "use this dataset to get the list of videos under which each of the users we found (above) have made a comment\n",
    "\n",
    "for each pair of users, calculate \"number of videos in common (under which both have commented) / min number of videos both users have commented on\"\n",
    "(example: mila commented on 10 videos, andreas on 100 videos, they have 8 videos they both commented on, so the value we calculate is 8/10 = 0.8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
