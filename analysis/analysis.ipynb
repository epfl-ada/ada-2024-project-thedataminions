{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_processing' from 'c:\\\\Users\\\\vvval\\\\Documents\\\\EPFL\\\\ada-2024-project-thedataminions\\\\analysis\\\\data_processing.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import data_processing as dp  # own functions and logic\n",
    "importlib.reload(dp)  # this makes it so that \n",
    "                      # the file with our functions is re-read every time, \n",
    "                      # in case we have made modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the folder where the YouNiverse dataset is stored here\n",
    "\n",
    "# when adding your own path, don't remove the existing path, just comment it\n",
    "# in this way, everyone can quickly uncomment their own path\n",
    "#dataset_root_path = \"/media/andreas/Backup Plus/youniverse_dataset/\"   #andreas\n",
    "dataset_root_path = \"D:/youniverse/\"                                    #mila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro : topics distribution on YouTube\n",
    "With this analysis we want to show that News&Politics is a popular category of both videos and channels to justify our choice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel data (no chunks needed, as the file is not very large)\n",
    "df_channels = pd.read_csv(dataset_root_path + \"df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a small part of video and comment data, to try functions on them etc.\n",
    "# these datasets should not be used for calculations, as they don't contain all the data\n",
    "\n",
    "# load (first 100000 rows of) video data\n",
    "df_videos = pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", compression=\"infer\", lines=True, nrows=100000)\n",
    "\n",
    "# load (first 1000000 rows of) comment data\n",
    "df_comments = pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", compression=\"infer\", sep=\"\\t\", nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaming</td>\n",
       "      <td>2010-04-29</td>\n",
       "      <td>UC-lHJZR3Gqxm24_Vd_AJ5Yw</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>101000000</td>\n",
       "      <td>3956</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>2006-09-01</td>\n",
       "      <td>UCbCmjCuTUZos6Inko4u57UQ</td>\n",
       "      <td>Cocomelon - Nursery ...</td>\n",
       "      <td>60100000</td>\n",
       "      <td>458</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2006-09-20</td>\n",
       "      <td>UCpEhnqL0y41EpW2TvWAHD7Q</td>\n",
       "      <td>SET India</td>\n",
       "      <td>56018869</td>\n",
       "      <td>32661</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Howto &amp; Style</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>UC295-Dw_tDNtZXFeAPAW6Aw</td>\n",
       "      <td>5-Minute Crafts</td>\n",
       "      <td>60600000</td>\n",
       "      <td>3591</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sports</td>\n",
       "      <td>2007-05-11</td>\n",
       "      <td>UCJ5v_MCY6GNUBTO8-D3XoAg</td>\n",
       "      <td>WWE</td>\n",
       "      <td>48400000</td>\n",
       "      <td>43421</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category_cc   join_date                   channel  \\\n",
       "0         Gaming  2010-04-29  UC-lHJZR3Gqxm24_Vd_AJ5Yw   \n",
       "1      Education  2006-09-01  UCbCmjCuTUZos6Inko4u57UQ   \n",
       "2  Entertainment  2006-09-20  UCpEhnqL0y41EpW2TvWAHD7Q   \n",
       "3  Howto & Style  2016-11-15  UC295-Dw_tDNtZXFeAPAW6Aw   \n",
       "4         Sports  2007-05-11  UCJ5v_MCY6GNUBTO8-D3XoAg   \n",
       "\n",
       "                   name_cc  subscribers_cc  videos_cc  subscriber_rank_sb  \\\n",
       "0                PewDiePie       101000000       3956                 3.0   \n",
       "1  Cocomelon - Nursery ...        60100000        458                 7.0   \n",
       "2                SET India        56018869      32661                 8.0   \n",
       "3          5-Minute Crafts        60600000       3591                 9.0   \n",
       "4                      WWE        48400000      43421                11.0   \n",
       "\n",
       "   weights  \n",
       "0    2.087  \n",
       "1    2.087  \n",
       "2    2.087  \n",
       "3    2.087  \n",
       "4    2.087  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_channels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of \"readers\", i.e., objects that we can iterate through \n",
    "# and always get a chunk of the dataframe in each iteration\n",
    "\n",
    "def videos_in_chunks(chunksize: int = 100000) -> pd.io.json._json.JsonReader:\n",
    "    \"\"\"\n",
    "    Returns a Json reader which can be iterated through, to get chunks of the (unfiltered) video dataset.\n",
    "\n",
    "    Args:\n",
    "        chunksize: number of entries in each chunk\n",
    "\n",
    "    Returns:\n",
    "        the Json reader\n",
    "    \"\"\"\n",
    "    return pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", \n",
    "                        compression=\"infer\", lines=True, chunksize=chunksize, nrows=1000000, )   # uncomment this to only use the first million videos, for testing\n",
    "                                           # (remove the paranthesis above as well)\n",
    "\n",
    "def comments_in_chunks(chunksize: int = 1000000) -> pd.io.parsers.readers.TextFileReader:\n",
    "    \"\"\"\n",
    "    Returns a CSV reader which can be iterated through, to get chunks of the (unfiltered) comment dataset.\n",
    "\n",
    "    Args:\n",
    "        chunksize: number of entries in each chunk\n",
    "\n",
    "    Returns:\n",
    "        the CSV reader\n",
    "    \"\"\"\n",
    "    return pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", \n",
    "                       compression=\"infer\", sep=\"\\t\", chunksize=chunksize, nrows = 10000000)  # uncomment this to only use the first 10 million comments, for testing\n",
    "                                            # (remove the paranthesis above as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for NaNs and ' ' (here in youtube_comments.tsv.gz)\n",
    "\n",
    "Pending questions to TA (good practices): \n",
    "- is the analysis of useless data (NaN and '' rows) required for justification of their removal, or can we just go with the filtering only ?\n",
    "- are we allowed to filter once the dataset is already reduced to e.g one channel, or do we have to filter in the very beginning (painful) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "2.047 secs per chunk on average. Meaning  293.436 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "1.951 secs per chunk on average. Meaning  279.547 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "1.917 secs per chunk on average. Meaning  274.606 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.895 secs per chunk on average. Meaning  271.484 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.939 secs per chunk on average. Meaning  277.761 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "2.197 secs per chunk on average. Meaning  314.651 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "2.393 secs per chunk on average. Meaning  342.669 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "2.387 secs per chunk on average. Meaning  341.831 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "2.350 secs per chunk on average. Meaning  336.453 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "2.317 secs per chunk on average. Meaning  331.737 minutes left.\n"
     ]
    }
   ],
   "source": [
    "#replace empty character '' with NaN and remove rows containing NaN\n",
    "filtered_df = dp.run_simple_function_on_chunks_concat(comments_in_chunks(), \n",
    "                                        lambda x: x.replace('Gkb1QMHrGvA', np.nan).dropna(),\n",
    "                                        print_time=(1000000, 8600000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>video_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CNtp0xqoods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>249EEzQmVmQ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>_U443T2K_Bs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rJbjhm0weYc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>EpHERODJ7gM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>664459</td>\n",
       "      <td>GC3gqIbrK7c</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>664459</td>\n",
       "      <td>GC3gqIbrK7c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>664459</td>\n",
       "      <td>i9VRGaoFw8k</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>664459</td>\n",
       "      <td>-JLWZ1jz3FY</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>664459</td>\n",
       "      <td>-JLWZ1jz3FY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author     video_id  likes  replies\n",
       "1             1  CNtp0xqoods      0        0\n",
       "2             1  249EEzQmVmQ      1        0\n",
       "3             1  _U443T2K_Bs      0        0\n",
       "4             1  rJbjhm0weYc      0        0\n",
       "5             2  EpHERODJ7gM      0        0\n",
       "...         ...          ...    ...      ...\n",
       "9999995  664459  GC3gqIbrK7c      9        1\n",
       "9999996  664459  GC3gqIbrK7c      1        0\n",
       "9999997  664459  i9VRGaoFw8k      1        1\n",
       "9999998  664459  -JLWZ1jz3FY      0        2\n",
       "9999999  664459  -JLWZ1jz3FY      0        0\n",
       "\n",
       "[9999999 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filtered df\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have to analyze NaN rows before filtering, then use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "1.593 secs per chunk on average. Meaning  228.242 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "1.485 secs per chunk on average. Meaning  212.856 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "1.461 secs per chunk on average. Meaning  209.287 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.555 secs per chunk on average. Meaning  222.726 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.624 secs per chunk on average. Meaning  232.621 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "1.643 secs per chunk on average. Meaning  235.383 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "1.666 secs per chunk on average. Meaning  238.634 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "1.657 secs per chunk on average. Meaning  237.272 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "1.605 secs per chunk on average. Meaning  229.846 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "1.577 secs per chunk on average. Meaning  225.705 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# get the entries of the comment dataframe which have a na value in any column\n",
    "nans = dp.run_simple_function_on_chunks_concat(comments_in_chunks(), \n",
    "                                        lambda x: dp.get_na_entries(x, \"any\", reverse=False),\n",
    "                                        print_time=(1000000, 8600000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "1.338 secs per chunk on average. Meaning  191.752 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "1.330 secs per chunk on average. Meaning  190.553 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "1.283 secs per chunk on average. Meaning  183.879 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.271 secs per chunk on average. Meaning  182.041 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.266 secs per chunk on average. Meaning  181.289 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "1.265 secs per chunk on average. Meaning  181.245 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "1.272 secs per chunk on average. Meaning  182.151 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "1.270 secs per chunk on average. Meaning  181.910 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "1.274 secs per chunk on average. Meaning  182.483 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "1.283 secs per chunk on average. Meaning  183.747 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# count the entries of the comment dataframe which have a na value in any column\n",
    "\n",
    "counted_nans = dp.run_simple_function_on_chunks_concat(comments_in_chunks(), \n",
    "                                                lambda x: dp.count_na_entries(x, \"any\", reverse=False),\n",
    "                                                print_time=(1000000, 8600000000)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "na rows              0\n",
       "total rows    10000000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(counted_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the datasets to our needs\n",
    "\n",
    "### Filtering **videos** by category 'News & Politics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 100000 entries have been processed. 72824794 left.\n",
      "4.727 secs per chunk on average. Meaning  57.371 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 200000 entries have been processed. 72724794 left.\n",
      "4.874 secs per chunk on average. Meaning  59.080 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 300000 entries have been processed. 72624794 left.\n",
      "4.654 secs per chunk on average. Meaning  56.328 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 400000 entries have been processed. 72524794 left.\n",
      "4.546 secs per chunk on average. Meaning  54.955 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 500000 entries have been processed. 72424794 left.\n",
      "4.345 secs per chunk on average. Meaning  52.453 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 600000 entries have been processed. 72324794 left.\n",
      "4.196 secs per chunk on average. Meaning  50.581 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 700000 entries have been processed. 72224794 left.\n",
      "4.148 secs per chunk on average. Meaning  49.927 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 800000 entries have been processed. 72124794 left.\n",
      "4.060 secs per chunk on average. Meaning  48.801 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 900000 entries have been processed. 72024794 left.\n",
      "4.062 secs per chunk on average. Meaning  48.758 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 1000000 entries have been processed. 71924794 left.\n",
      "4.096 secs per chunk on average. Meaning  49.096 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# filter the video dataframe to only include videos from n&p category\n",
    "\n",
    "df_videos_news_pol = dp.run_simple_function_on_chunks_concat(videos_in_chunks(chunksize=100000),\n",
    "                                                             lambda x: x[x.categories == \"News & Politics\"], \n",
    "                                                             print_time=(100000, 72924794))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWm1-4XF7AHxVUTkHCM1uw</td>\n",
       "      <td>2019-11-17 06:28:42.593675</td>\n",
       "      <td>retrogamer3.com</td>\n",
       "      <td>16.0</td>\n",
       "      <td>dfa8RRkKoa4</td>\n",
       "      <td>9251</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RetroGamer3,Live Stream,politics,Trump</td>\n",
       "      <td>Retrogamer3 Political Stream</td>\n",
       "      <td>2018-08-23 00:00:00</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWLsxDD373D4tY8kN-0LGQ</td>\n",
       "      <td>2019-11-05 00:42:33.012228</td>\n",
       "      <td>What are the forces at work that have created ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>_dIIEMvH86k</td>\n",
       "      <td>309</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NWO,Ebola,Ukraine,Mainstream,Media,Pyschology</td>\n",
       "      <td>Adam Curtis describes the Surkow Strategy of M...</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18005</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzVBu6oqlrAix0oq9T2rBFg</td>\n",
       "      <td>2019-11-19 20:40:22.403775</td>\n",
       "      <td>Social Media:\\n\\nFacebook.com/thebookoflaura\\n...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>eWXefhNB2po</td>\n",
       "      <td>707</td>\n",
       "      <td>625.0</td>\n",
       "      <td>michael jackson,lyrics,music video,court,child...</td>\n",
       "      <td>my thoughts on the michael jackson documentary.</td>\n",
       "      <td>2019-04-24 00:00:00</td>\n",
       "      <td>12780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24361</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:47:10.520209</td>\n",
       "      <td>👕 Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>MBgzne7djFU</td>\n",
       "      <td>378</td>\n",
       "      <td>47027.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>Elizabeth Warren Gets a Big Surprise at the Ai...</td>\n",
       "      <td>2019-10-03 00:00:00</td>\n",
       "      <td>374711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24362</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:46:16.481889</td>\n",
       "      <td>👕 Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>AbH3pJnFgY8</td>\n",
       "      <td>278</td>\n",
       "      <td>36384.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>No More Twitter? 😂</td>\n",
       "      <td>2019-10-02 00:00:00</td>\n",
       "      <td>245617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999870</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:26.687460</td>\n",
       "      <td>The Young Turks recently posted a video entitl...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Rmq0JmUbt8k</td>\n",
       "      <td>857</td>\n",
       "      <td>25.0</td>\n",
       "      <td>American Joe,American Joe Show,The Young Turks...</td>\n",
       "      <td>Young Turks Caught Lying and Race Baiting.... ...</td>\n",
       "      <td>2018-11-17 00:00:00</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999871</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:27.273595</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ts__Orp310M</td>\n",
       "      <td>49</td>\n",
       "      <td>34.0</td>\n",
       "      <td>American Joe,American Joe Show</td>\n",
       "      <td>President says he will send migrant Children B...</td>\n",
       "      <td>2018-11-15 00:00:00</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999872</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:27.847348</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bQ3_ZMVpiio</td>\n",
       "      <td>298</td>\n",
       "      <td>6.0</td>\n",
       "      <td>American Joe,American Joe Show,Michael Avenatt...</td>\n",
       "      <td>Creepy Porn Lawyer, and Woman Beater Michael A...</td>\n",
       "      <td>2018-11-14 00:00:00</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999873</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:28.400609</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>q92A939Nyj8</td>\n",
       "      <td>388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>American Joe,American Joe Show,Midterm Electio...</td>\n",
       "      <td>Midterm Fallout - How Bad is it For Trump?</td>\n",
       "      <td>2018-11-14 00:00:00</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999874</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:32.321224</td>\n",
       "      <td>Link to video by Conservative Youtuber ABL, An...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CPYdzsvg-Ns</td>\n",
       "      <td>573</td>\n",
       "      <td>18.0</td>\n",
       "      <td>American Joe,American Joe Show,Jim Acosta Dona...</td>\n",
       "      <td>Trump Vs. Jim Acosta - The Fake News is at it ...</td>\n",
       "      <td>2018-11-11 00:00:00</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145768 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             categories                channel_id                  crawl_date  \\\n",
       "1827    News & Politics  UCzWm1-4XF7AHxVUTkHCM1uw  2019-11-17 06:28:42.593675   \n",
       "7605    News & Politics  UCzWLsxDD373D4tY8kN-0LGQ  2019-11-05 00:42:33.012228   \n",
       "18005   News & Politics  UCzVBu6oqlrAix0oq9T2rBFg  2019-11-19 20:40:22.403775   \n",
       "24361   News & Politics  UCzUV5283-l5c0oKRtyenj6Q  2019-11-22 08:47:10.520209   \n",
       "24362   News & Politics  UCzUV5283-l5c0oKRtyenj6Q  2019-11-22 08:46:16.481889   \n",
       "...                 ...                       ...                         ...   \n",
       "999870  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:26.687460   \n",
       "999871  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:27.273595   \n",
       "999872  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:27.847348   \n",
       "999873  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:28.400609   \n",
       "999874  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:32.321224   \n",
       "\n",
       "                                              description  dislike_count  \\\n",
       "1827                                      retrogamer3.com           16.0   \n",
       "7605    What are the forces at work that have created ...            0.0   \n",
       "18005   Social Media:\\n\\nFacebook.com/thebookoflaura\\n...           89.0   \n",
       "24361   👕 Order your shirts here: https://Teespring.co...          195.0   \n",
       "24362   👕 Order your shirts here: https://Teespring.co...          114.0   \n",
       "...                                                   ...            ...   \n",
       "999870  The Young Turks recently posted a video entitl...            2.0   \n",
       "999871  Patriots I need your help growing the American...            0.0   \n",
       "999872  Patriots I need your help growing the American...            1.0   \n",
       "999873  Patriots I need your help growing the American...            2.0   \n",
       "999874  Link to video by Conservative Youtuber ABL, An...            2.0   \n",
       "\n",
       "         display_id  duration  like_count  \\\n",
       "1827    dfa8RRkKoa4      9251        25.0   \n",
       "7605    _dIIEMvH86k       309         9.0   \n",
       "18005   eWXefhNB2po       707       625.0   \n",
       "24361   MBgzne7djFU       378     47027.0   \n",
       "24362   AbH3pJnFgY8       278     36384.0   \n",
       "...             ...       ...         ...   \n",
       "999870  Rmq0JmUbt8k       857        25.0   \n",
       "999871  ts__Orp310M        49        34.0   \n",
       "999872  bQ3_ZMVpiio       298         6.0   \n",
       "999873  q92A939Nyj8       388         2.0   \n",
       "999874  CPYdzsvg-Ns       573        18.0   \n",
       "\n",
       "                                                     tags  \\\n",
       "1827               RetroGamer3,Live Stream,politics,Trump   \n",
       "7605        NWO,Ebola,Ukraine,Mainstream,Media,Pyschology   \n",
       "18005   michael jackson,lyrics,music video,court,child...   \n",
       "24361   Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "24362   Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "...                                                   ...   \n",
       "999870  American Joe,American Joe Show,The Young Turks...   \n",
       "999871                     American Joe,American Joe Show   \n",
       "999872  American Joe,American Joe Show,Michael Avenatt...   \n",
       "999873  American Joe,American Joe Show,Midterm Electio...   \n",
       "999874  American Joe,American Joe Show,Jim Acosta Dona...   \n",
       "\n",
       "                                                    title  \\\n",
       "1827                         Retrogamer3 Political Stream   \n",
       "7605    Adam Curtis describes the Surkow Strategy of M...   \n",
       "18005     my thoughts on the michael jackson documentary.   \n",
       "24361   Elizabeth Warren Gets a Big Surprise at the Ai...   \n",
       "24362                                  No More Twitter? 😂   \n",
       "...                                                   ...   \n",
       "999870  Young Turks Caught Lying and Race Baiting.... ...   \n",
       "999871  President says he will send migrant Children B...   \n",
       "999872  Creepy Porn Lawyer, and Woman Beater Michael A...   \n",
       "999873         Midterm Fallout - How Bad is it For Trump?   \n",
       "999874  Trump Vs. Jim Acosta - The Fake News is at it ...   \n",
       "\n",
       "                upload_date  view_count  \n",
       "1827    2018-08-23 00:00:00       478.0  \n",
       "7605    2015-01-04 00:00:00       865.0  \n",
       "18005   2019-04-24 00:00:00     12780.0  \n",
       "24361   2019-10-03 00:00:00    374711.0  \n",
       "24362   2019-10-02 00:00:00    245617.0  \n",
       "...                     ...         ...  \n",
       "999870  2018-11-17 00:00:00       273.0  \n",
       "999871  2018-11-15 00:00:00       353.0  \n",
       "999872  2018-11-14 00:00:00        76.0  \n",
       "999873  2018-11-14 00:00:00        38.0  \n",
       "999874  2018-11-11 00:00:00       146.0  \n",
       "\n",
       "[145768 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_videos_news_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering **channels** by category News & Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter channels dataset to only include n&p\n",
    "df_channels_news_pol = df_channels[df_channels.category_cc == \"News & Politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2006-08-26</td>\n",
       "      <td>UCttspZesZIDEwwpVIgoZtWQ</td>\n",
       "      <td>IndiaTV</td>\n",
       "      <td>15177282</td>\n",
       "      <td>139814</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>UCRWFSbif-RFENbBrSiez1DA</td>\n",
       "      <td>ABP NEWS</td>\n",
       "      <td>16274836</td>\n",
       "      <td>129027</td>\n",
       "      <td>207.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>UCmphdqZNmqL72WJ2uyiNw5w</td>\n",
       "      <td>ABP NEWS HINDI</td>\n",
       "      <td>10800000</td>\n",
       "      <td>51298</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2015-03-23</td>\n",
       "      <td>UCx8Z14PpntdaxCt2hakbQLQ</td>\n",
       "      <td>The Lallantop</td>\n",
       "      <td>9120000</td>\n",
       "      <td>9423</td>\n",
       "      <td>438.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2007-06-19</td>\n",
       "      <td>UCIvaYmXn910QMdemBG3v1pQ</td>\n",
       "      <td>Zee News</td>\n",
       "      <td>9280000</td>\n",
       "      <td>102648</td>\n",
       "      <td>549.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135820</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2010-08-07</td>\n",
       "      <td>UC5rxiCGcNunIi5zI1hMYLMg</td>\n",
       "      <td>Salman Akhtar</td>\n",
       "      <td>10400</td>\n",
       "      <td>40</td>\n",
       "      <td>962468.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135825</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>UCLSEJQ8TWtlEkaytaa4Y7lw</td>\n",
       "      <td>WingsOfChrist</td>\n",
       "      <td>10420</td>\n",
       "      <td>61</td>\n",
       "      <td>962547.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135901</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>UCnkG_c5cyemVVsgCDoHiXew</td>\n",
       "      <td>The American Mirror</td>\n",
       "      <td>10500</td>\n",
       "      <td>329</td>\n",
       "      <td>963417.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136231</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>UC69lWS7UMbBQc-9yqp4nGjA</td>\n",
       "      <td>Patriotism Show</td>\n",
       "      <td>10320</td>\n",
       "      <td>46</td>\n",
       "      <td>975448.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136301</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>UCpbE1CJWNHpu8knuok8YBZQ</td>\n",
       "      <td>Jenny Constantine</td>\n",
       "      <td>10200</td>\n",
       "      <td>30</td>\n",
       "      <td>978433.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            category_cc   join_date                   channel  \\\n",
       "129     News & Politics  2006-08-26  UCttspZesZIDEwwpVIgoZtWQ   \n",
       "133     News & Politics  2012-06-01  UCRWFSbif-RFENbBrSiez1DA   \n",
       "212     News & Politics  2017-03-03  UCmphdqZNmqL72WJ2uyiNw5w   \n",
       "268     News & Politics  2015-03-23  UCx8Z14PpntdaxCt2hakbQLQ   \n",
       "337     News & Politics  2007-06-19  UCIvaYmXn910QMdemBG3v1pQ   \n",
       "...                 ...         ...                       ...   \n",
       "135820  News & Politics  2010-08-07  UC5rxiCGcNunIi5zI1hMYLMg   \n",
       "135825  News & Politics  2013-02-01  UCLSEJQ8TWtlEkaytaa4Y7lw   \n",
       "135901  News & Politics  2012-10-19  UCnkG_c5cyemVVsgCDoHiXew   \n",
       "136231  News & Politics  2017-11-25  UC69lWS7UMbBQc-9yqp4nGjA   \n",
       "136301  News & Politics  2017-03-26  UCpbE1CJWNHpu8knuok8YBZQ   \n",
       "\n",
       "                    name_cc  subscribers_cc  videos_cc  subscriber_rank_sb  \\\n",
       "129                 IndiaTV        15177282     139814               199.0   \n",
       "133                ABP NEWS        16274836     129027               207.0   \n",
       "212          ABP NEWS HINDI        10800000      51298               340.0   \n",
       "268           The Lallantop         9120000       9423               438.0   \n",
       "337                Zee News         9280000     102648               549.0   \n",
       "...                     ...             ...        ...                 ...   \n",
       "135820        Salman Akhtar           10400         40            962468.0   \n",
       "135825        WingsOfChrist           10420         61            962547.0   \n",
       "135901  The American Mirror           10500        329            963417.0   \n",
       "136231      Patriotism Show           10320         46            975448.0   \n",
       "136301    Jenny Constantine           10200         30            978433.0   \n",
       "\n",
       "        weights  \n",
       "129      2.0870  \n",
       "133      2.0870  \n",
       "212      2.0870  \n",
       "268      2.0870  \n",
       "337      2.0870  \n",
       "...         ...  \n",
       "135820  53.1435  \n",
       "135825  53.1435  \n",
       "135901  53.1435  \n",
       "136231  53.1435  \n",
       "136301  53.1435  \n",
       "\n",
       "[2263 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_channels_news_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWm1-4XF7AHxVUTkHCM1uw</td>\n",
       "      <td>2019-11-17 06:28:42.593675</td>\n",
       "      <td>retrogamer3.com</td>\n",
       "      <td>16.0</td>\n",
       "      <td>dfa8RRkKoa4</td>\n",
       "      <td>9251</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RetroGamer3,Live Stream,politics,Trump</td>\n",
       "      <td>Retrogamer3 Political Stream</td>\n",
       "      <td>2018-08-23 00:00:00</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWLsxDD373D4tY8kN-0LGQ</td>\n",
       "      <td>2019-11-05 00:42:33.012228</td>\n",
       "      <td>What are the forces at work that have created ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>_dIIEMvH86k</td>\n",
       "      <td>309</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NWO,Ebola,Ukraine,Mainstream,Media,Pyschology</td>\n",
       "      <td>Adam Curtis describes the Surkow Strategy of M...</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18005</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzVBu6oqlrAix0oq9T2rBFg</td>\n",
       "      <td>2019-11-19 20:40:22.403775</td>\n",
       "      <td>Social Media:\\n\\nFacebook.com/thebookoflaura\\n...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>eWXefhNB2po</td>\n",
       "      <td>707</td>\n",
       "      <td>625.0</td>\n",
       "      <td>michael jackson,lyrics,music video,court,child...</td>\n",
       "      <td>my thoughts on the michael jackson documentary.</td>\n",
       "      <td>2019-04-24 00:00:00</td>\n",
       "      <td>12780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28840</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzTmNzBxLEHbpZNOCpUTWbA</td>\n",
       "      <td>2019-11-03 04:38:01.617657</td>\n",
       "      <td>A young man is living a normal life with no ca...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>ck6Yl8TNoWs</td>\n",
       "      <td>1257</td>\n",
       "      <td>452.0</td>\n",
       "      <td>JoiRida,Cheatham,JoiRidaCheatham,Accepted,Detr...</td>\n",
       "      <td>Accepted - Award Winning Short Film</td>\n",
       "      <td>2013-10-13 00:00:00</td>\n",
       "      <td>27366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28860</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzTmNzBxLEHbpZNOCpUTWbA</td>\n",
       "      <td>2019-11-03 04:38:06.565138</td>\n",
       "      <td>Short Film</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tjUajxZAIZ8</td>\n",
       "      <td>422</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Joi.Rida,Cheatham,joiridacheatham,dread,loc,up...</td>\n",
       "      <td>JoiRida Twin Visit (Introducing Jive Viper)</td>\n",
       "      <td>2010-03-04 00:00:00</td>\n",
       "      <td>987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970869</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrXcatz6wlNHjuqgf-tglOA</td>\n",
       "      <td>2019-11-07 00:55:48.241832</td>\n",
       "      <td>As promised, our Wet Head Challenge using the ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>lIuK9DGtOx8</td>\n",
       "      <td>321</td>\n",
       "      <td>141.0</td>\n",
       "      <td>challenge,wet,head,gross,wet head challenge,we...</td>\n",
       "      <td>Gross Smoothie Wet Head Challenge 😕</td>\n",
       "      <td>2016-08-23 00:00:00</td>\n",
       "      <td>8941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991815</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrVnMcE3GIyg2rM4gH34YWg</td>\n",
       "      <td>2019-11-10 10:02:03.075065</td>\n",
       "      <td>More Travel News...\\nhttp://www.petergreenberg...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aSWbywb7SBE</td>\n",
       "      <td>423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2008,Travel Inspiration,clinton,Presidential,P...</td>\n",
       "      <td>2008 Presidential Candidates Travel Scorecard</td>\n",
       "      <td>2008-01-25 00:00:00</td>\n",
       "      <td>588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998347</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:30.119011</td>\n",
       "      <td>© 2012 WMG  Webisode by Mutemath from The Blue...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H8-Al6B_J1g</td>\n",
       "      <td>106</td>\n",
       "      <td>17.0</td>\n",
       "      <td>mutemath,wbr,INDMUSIC,warner bros records</td>\n",
       "      <td>Mutemath - What Happens Before The Show [Webis...</td>\n",
       "      <td>2006-11-04 00:00:00</td>\n",
       "      <td>3136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998349</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:31.498209</td>\n",
       "      <td>© 2012 WMG  Webisode by Mutemath from Park Wes...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>wYI6dWaEHjk</td>\n",
       "      <td>56</td>\n",
       "      <td>8.0</td>\n",
       "      <td>INDMUSIC,wbr,mutemath,warner bros records</td>\n",
       "      <td>Mutemath - Built for Destruction [Webisode]</td>\n",
       "      <td>2006-11-04 00:00:00</td>\n",
       "      <td>1881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998365</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:41.975562</td>\n",
       "      <td>© 2012 WMG  Webisode by Mutemath from The Goth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gs5jw7rtrn8</td>\n",
       "      <td>216</td>\n",
       "      <td>27.0</td>\n",
       "      <td>wbr,warner bros records,INDMUSIC</td>\n",
       "      <td>Mutemath - The Gothic Theatre in Denver [Webis...</td>\n",
       "      <td>2005-12-31 00:00:00</td>\n",
       "      <td>7916.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8564 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             categories                channel_id                  crawl_date  \\\n",
       "1827    News & Politics  UCzWm1-4XF7AHxVUTkHCM1uw  2019-11-17 06:28:42.593675   \n",
       "7605    News & Politics  UCzWLsxDD373D4tY8kN-0LGQ  2019-11-05 00:42:33.012228   \n",
       "18005   News & Politics  UCzVBu6oqlrAix0oq9T2rBFg  2019-11-19 20:40:22.403775   \n",
       "28840   News & Politics  UCzTmNzBxLEHbpZNOCpUTWbA  2019-11-03 04:38:01.617657   \n",
       "28860   News & Politics  UCzTmNzBxLEHbpZNOCpUTWbA  2019-11-03 04:38:06.565138   \n",
       "...                 ...                       ...                         ...   \n",
       "970869  News & Politics  UCrXcatz6wlNHjuqgf-tglOA  2019-11-07 00:55:48.241832   \n",
       "991815  News & Politics  UCrVnMcE3GIyg2rM4gH34YWg  2019-11-10 10:02:03.075065   \n",
       "998347  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:30.119011   \n",
       "998349  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:31.498209   \n",
       "998365  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:41.975562   \n",
       "\n",
       "                                              description  dislike_count  \\\n",
       "1827                                      retrogamer3.com           16.0   \n",
       "7605    What are the forces at work that have created ...            0.0   \n",
       "18005   Social Media:\\n\\nFacebook.com/thebookoflaura\\n...           89.0   \n",
       "28840   A young man is living a normal life with no ca...           16.0   \n",
       "28860                                          Short Film            1.0   \n",
       "...                                                   ...            ...   \n",
       "970869  As promised, our Wet Head Challenge using the ...            3.0   \n",
       "991815  More Travel News...\\nhttp://www.petergreenberg...            0.0   \n",
       "998347  © 2012 WMG  Webisode by Mutemath from The Blue...            3.0   \n",
       "998349  © 2012 WMG  Webisode by Mutemath from Park Wes...            2.0   \n",
       "998365  © 2012 WMG  Webisode by Mutemath from The Goth...            0.0   \n",
       "\n",
       "         display_id  duration  like_count  \\\n",
       "1827    dfa8RRkKoa4      9251        25.0   \n",
       "7605    _dIIEMvH86k       309         9.0   \n",
       "18005   eWXefhNB2po       707       625.0   \n",
       "28840   ck6Yl8TNoWs      1257       452.0   \n",
       "28860   tjUajxZAIZ8       422        15.0   \n",
       "...             ...       ...         ...   \n",
       "970869  lIuK9DGtOx8       321       141.0   \n",
       "991815  aSWbywb7SBE       423         1.0   \n",
       "998347  H8-Al6B_J1g       106        17.0   \n",
       "998349  wYI6dWaEHjk        56         8.0   \n",
       "998365  Gs5jw7rtrn8       216        27.0   \n",
       "\n",
       "                                                     tags  \\\n",
       "1827               RetroGamer3,Live Stream,politics,Trump   \n",
       "7605        NWO,Ebola,Ukraine,Mainstream,Media,Pyschology   \n",
       "18005   michael jackson,lyrics,music video,court,child...   \n",
       "28840   JoiRida,Cheatham,JoiRidaCheatham,Accepted,Detr...   \n",
       "28860   Joi.Rida,Cheatham,joiridacheatham,dread,loc,up...   \n",
       "...                                                   ...   \n",
       "970869  challenge,wet,head,gross,wet head challenge,we...   \n",
       "991815  2008,Travel Inspiration,clinton,Presidential,P...   \n",
       "998347          mutemath,wbr,INDMUSIC,warner bros records   \n",
       "998349          INDMUSIC,wbr,mutemath,warner bros records   \n",
       "998365                   wbr,warner bros records,INDMUSIC   \n",
       "\n",
       "                                                    title  \\\n",
       "1827                         Retrogamer3 Political Stream   \n",
       "7605    Adam Curtis describes the Surkow Strategy of M...   \n",
       "18005     my thoughts on the michael jackson documentary.   \n",
       "28840                 Accepted - Award Winning Short Film   \n",
       "28860         JoiRida Twin Visit (Introducing Jive Viper)   \n",
       "...                                                   ...   \n",
       "970869                Gross Smoothie Wet Head Challenge 😕   \n",
       "991815      2008 Presidential Candidates Travel Scorecard   \n",
       "998347  Mutemath - What Happens Before The Show [Webis...   \n",
       "998349        Mutemath - Built for Destruction [Webisode]   \n",
       "998365  Mutemath - The Gothic Theatre in Denver [Webis...   \n",
       "\n",
       "                upload_date  view_count  \n",
       "1827    2018-08-23 00:00:00       478.0  \n",
       "7605    2015-01-04 00:00:00       865.0  \n",
       "18005   2019-04-24 00:00:00     12780.0  \n",
       "28840   2013-10-13 00:00:00     27366.0  \n",
       "28860   2010-03-04 00:00:00       987.0  \n",
       "...                     ...         ...  \n",
       "970869  2016-08-23 00:00:00      8941.0  \n",
       "991815  2008-01-25 00:00:00       588.0  \n",
       "998347  2006-11-04 00:00:00      3136.0  \n",
       "998349  2006-11-04 00:00:00      1881.0  \n",
       "998365  2005-12-31 00:00:00      7916.0  \n",
       "\n",
       "[8564 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all videos we found in news&pol are also pulished by a channel in category n&p\n",
    "\n",
    "df_videos_news_pol[np.logical_not(df_videos_news_pol.channel_id.isin(df_channels_news_pol.channel))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that indeed, not all videos in the news and politics category belong to a channel in this category!**\n",
    "A google search shows that apparently, you don't have to have the same category for all videos, but you set a \"default\" channel category which will be used for videos if you don't change it manually. Also, you can probably change the default category after a while if you want.\n",
    "\n",
    "This is the reason why most of the news&pol videos are uploaded by a news&pol channe, but not all.\n",
    "\n",
    "In the paper about the dataset, the authors say that the channel category is actually the \"most frequent category\", so I guess the video categories are the most relevant, as they are the true categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to verify this, if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the N&P videos by a list of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the filtered N&P videos dataframe, keep those posted by channels manually selected (here CNN, Fox News, BBC News and MSNBC)\n",
    "\n",
    "df_videos_news_pol_manually_selected = df_videos_news_pol[df_videos_news_pol.channel_id.isin([\"UCupvZG-5ko_eiXAupbDfxWw\",  # CNN\n",
    "                                                                                              \"UCXIJgqnII2ZOINSWNOGFThA\",  # Fox News\n",
    "                                                                                              \"UC16niRr50-MSBwiO3YDb3RA\",  # BBC News\n",
    "                                                                                              \"UCaXkIU1QidjPwiAYu6GcHjg\",  # MSNBC\n",
    "                                                                                            ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [categories, channel_id, crawl_date, description, dislike_count, display_id, duration, like_count, tags, title, upload_date, view_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos_news_pol_manually_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing our own ranking of N&P channels (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the previously filtered n&p channels by subscriber count according to channel crawler\n",
    "#df_channels_news_pol_sort_subscribers = df_channels_news_pol.sort_values(by=\"subscribers_cc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(df_channels_news_pol_sort_subscribers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering **videos** in a specific channel\n",
    "Note: This function doesn't have to be done on the whole video dataset, as it is done here, since we have previously filtered N&P videos of specific channels.\n",
    "Therefore, we would also not need chunks here.\n",
    "\n",
    "Todo: simplify the function like this:\n",
    "\n",
    "df_videos_news_pol_manually_selected.loc[df_videos_news_pol_manually_selected.channel_id == <channel id of the desired channel>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 100000 entries have been processed. 72824794 left.\n",
      "4.383 secs per chunk on average. Meaning  53.197 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 200000 entries have been processed. 72724794 left.\n",
      "4.894 secs per chunk on average. Meaning  59.320 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 300000 entries have been processed. 72624794 left.\n",
      "5.154 secs per chunk on average. Meaning  62.384 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 400000 entries have been processed. 72524794 left.\n",
      "5.209 secs per chunk on average. Meaning  62.960 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 500000 entries have been processed. 72424794 left.\n",
      "4.883 secs per chunk on average. Meaning  58.936 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 600000 entries have been processed. 72324794 left.\n",
      "4.698 secs per chunk on average. Meaning  56.631 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 700000 entries have been processed. 72224794 left.\n",
      "4.577 secs per chunk on average. Meaning  55.092 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 800000 entries have been processed. 72124794 left.\n",
      "4.428 secs per chunk on average. Meaning  53.230 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 900000 entries have been processed. 72024794 left.\n",
      "4.384 secs per chunk on average. Meaning  52.625 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 1000000 entries have been processed. 71924794 left.\n",
      "4.424 secs per chunk on average. Meaning  53.032 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# filter the video dataset to get only videos from a specific channel (here: random channel)\n",
    "\n",
    "videos_from_channel_test = dp.run_simple_function_on_chunks_concat(\n",
    "    videos_in_chunks(chunksize=100000),\n",
    "    lambda x: x.loc[x.channel_id == \"UCzWrhkg9eK5I8Bm3HfV-unA\"],\n",
    "    print_time=(100000, 72924794))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering **comments** in a specific channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second function probably needs to look like this, as we haven't preprocessed the comment data yet, so it is still too big to be considered all at once, so we need the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "1.515 secs per chunk on average. Meaning  217.056 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "1.874 secs per chunk on average. Meaning  268.605 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "1.919 secs per chunk on average. Meaning  274.950 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.906 secs per chunk on average. Meaning  273.121 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.851 secs per chunk on average. Meaning  265.222 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "1.948 secs per chunk on average. Meaning  279.084 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "1.950 secs per chunk on average. Meaning  279.266 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "1.939 secs per chunk on average. Meaning  277.631 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "1.908 secs per chunk on average. Meaning  273.166 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "1.899 secs per chunk on average. Meaning  271.909 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# get comments on videos from a specific channel using the ids from filtered video dataset\n",
    "\n",
    "comments_from_channel_test = dp.run_simple_function_on_chunks_concat(\n",
    "    comments_in_chunks(chunksize=1000000), \n",
    "    lambda df: df[df.video_id.isin(videos_from_channel_test.display_id)],\n",
    "    print_time=(1000000, 8600000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>video_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868268</th>\n",
       "      <td>453667</td>\n",
       "      <td>3vQK78eUg2A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094579</th>\n",
       "      <td>468696</td>\n",
       "      <td>SWZG-ba1qDk</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912192</th>\n",
       "      <td>594074</td>\n",
       "      <td>hn2zYwqSINY</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author     video_id  likes  replies\n",
       "6868268  453667  3vQK78eUg2A      2        1\n",
       "7094579  468696  SWZG-ba1qDk     15       18\n",
       "8912192  594074  hn2zYwqSINY      0        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(comments_from_channel_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots \n",
    "1. Using the previous functions, plot the tot nb of comments and/or subscribers in the selected channels (all on same plot).\n",
    "\n",
    "2. For each channel, plot the distribution of comments for all the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the cluster of users in a specific channel \n",
    "\n",
    "(flexible function where you give the threshold of x comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>number_of_comments</th>\n",
       "      <th>number_of_videos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>453667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>468696</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>594074</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author  number_of_comments  number_of_videos\n",
       "0  453667                   1                 1\n",
       "1  468696                   1                 1\n",
       "2  594074                   1                 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#returns df with users_id and total number of comments made in a specific channel\n",
    "def get_metadata_commenters(comment_channelX,x):\n",
    "    metadata_commenters = comment_channelX.groupby('author').agg(number_of_comments=('author', 'size')).reset_index()\n",
    "    metadata_commenters['number_of_videos']= comment_channelX.groupby('author')['video_id'].nunique().values\n",
    "    \n",
    "    #keep users that wrote more than x comments \n",
    "    metadata_commenters=metadata_commenters[metadata_commenters['number_of_comments']>=x]\n",
    "    return metadata_commenters\n",
    "\n",
    "#test with threshold x=0\n",
    "metadata_commenters= get_metadata_commenters(comments_from_channel_test,0)\n",
    "display(metadata_commenters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "1. Venn diagram with clusters of the channels and underline overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper analysis and statistics\n",
    "Todo: \n",
    "create dataset of all comments which are under a video in the news and politics category\n",
    "\n",
    "use this dataset to get the list of videos under which each of the users we found (above) have made a comment\n",
    "\n",
    "for each pair of users, calculate \"number of videos in common (under which both have commented) / min number of videos both users have commented on\"\n",
    "(example: mila commented on 10 videos, andreas on 100 videos, they have 8 videos they both commented on, so the value we calculate is 8/10 = 0.8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
