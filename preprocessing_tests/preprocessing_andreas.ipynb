{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_processing' from '/home/andreas/Nextcloud/Dokumente/Uni/Module/3sem-EPFL/ada/Project/ada-2024-project-thedataminions/preprocessing_tests/data_processing.py'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import data_processing as dp  # own functions and logic\n",
    "importlib.reload(dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the folder where the YouNiverse dataset is stored here\n",
    "\n",
    "# when adding your own path, don't remove the existing path, just comment it\n",
    "# in this way, everyone can quickly uncomment their own path\n",
    "dataset_root_path = \"/media/andreas/Backup Plus/youniverse_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if we want to test something on the dataset without using chunks. Otherwise, keep commented\n",
    "\n",
    "\n",
    "# load channel data\n",
    "df_channels = pd.read_csv(dataset_root_path + \"df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n",
    "\n",
    "# load (first 100000 rows of) video data\n",
    "df_videos = pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", compression=\"infer\", lines=True, nrows=100000)\n",
    "\n",
    "# load (first 1000000 rows of) comment data\n",
    "df_comments = pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", compression=\"infer\", sep=\"\\t\", nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of \"readers\", i.e., objects that we can iterate through \n",
    "# and always get a chunk of the dataframe in each iteration\n",
    "\n",
    "def videos_in_chunks(chunksize: int = 100000):\n",
    "    return pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", \n",
    "                        compression=\"infer\", lines=True, chunksize=chunksize, )  \n",
    "                        # nrows=1000000, )   # uncomment this to only use the first million videos, for testing\n",
    "                                             # (remove the paranthesis above as well)\n",
    "def comments_in_chunks(chunksize: int = 1000000):\n",
    "    return pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", \n",
    "                       compression=\"infer\", sep=\"\\t\", chunksize=chunksize, )\n",
    "                       # nrows = 10000000)  # uncomment this to only use the first 10 million comments, for testing\n",
    "                                            # (remove the paranthesis above as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vid_by_channel(channel_id: str, video_chunksize: int = 100000) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Returns a dataframe which is the video metadata dataframe, filtered so that only videos from the \n",
    "    given channel remain.\n",
    "\n",
    "    The function does this by going through the dataset in chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        channel_id :  id of the channel which the videos will be sorted by\n",
    "        video_chunksize :  number of entries in each chunk. Default is 100 000.\n",
    "\n",
    "    Returns:\n",
    "        The filtered dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    videos_filtered = pd.DataFrame(dict())\n",
    "    \n",
    "    with videos_in_chunks(video_chunksize) as reader:\n",
    "        time_start_global = time.time()\n",
    "        i=0\n",
    "        for video_chunk in reader:\n",
    "            time_start_chunk = time.time()\n",
    "            print(f\"Going through video chunk {i}...\")\n",
    "            videos_filtered = pd.concat([videos_filtered, video_chunk.loc[video_chunk.channel_id == channel_id]])\n",
    "            print(f\"The first {(i+1) * video_chunksize} videos have been processed.\")\n",
    "            time_end = time.time()\n",
    "            print(f\"{(time_end-time_start_global)/(i+1):.3f} secs per chunk on average. Meaning {72924794 / video_chunksize * (time_end-time_start_global)/((i+1)*60):.3f} mins for entire dataset, i.e., {72924794 / video_chunksize * (time_end-time_start_global)/((i+1)*60) - (time_end-time_start_global)/60:.3f} mins left.\")\n",
    "            i=i+1\n",
    "    return videos_filtered\n",
    "\n",
    "def filter_comment_by_channel(channel_id: str,video_chunksize: int = 100000, comment_chunksize: int = 1000000) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Returns a dataframe which is the comment data, but filtered so that only comments\n",
    "    made on videos which were published by a certain channel are left.\n",
    "\n",
    "    The function does this by going through the dataset in chunks of a defined size.\n",
    "\n",
    "    Args:\n",
    "        channel_id: id of the channel to be filtered by\n",
    "        video_chunksize: number of entries per chunk when going through the videos (to find the videos uploaded by a certain channel)\n",
    "        comment_chunksize: number of entries per chunk when going through the comments\n",
    "\n",
    "    Returns:\n",
    "        The filtered dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    comments_filtered = pd.DataFrame(dict())\n",
    "    filtered_videos = filter_vid_by_channel(channel_id, video_chunksize=video_chunksize)\n",
    "    print(f\"Videos have been filtered by channel, {len(filtered_videos)} videos found. \\nNow going through comments....\")\n",
    "    with comments_in_chunks(comment_chunksize) as reader:\n",
    "        time_start_global = time.time()\n",
    "        for i, comment_chunk in enumerate(reader):\n",
    "            print(f\"Going through comment chunk {i}...\")\n",
    "            comments_filtered = pd.concat([comments_filtered, \n",
    "                                comment_chunk.loc[comment_chunk.video_id.isin(filtered_videos.display_id)]])\n",
    "            print(f\"The first {(i+1) * comment_chunksize} comments have been processed\")\n",
    "            time_end = time.time()\n",
    "            print(f\"{(time_end-time_start_global)/(i+1):.3f} secs per chunk on average.Meaning {8600000000 / comment_chunksize * (time_end-time_start_global)/((i+1)*60):.3f} mins for entire dataset, i.e., {8600000000 / comment_chunksize * (time_end-time_start_global)/((i+1)*60) - (time_end-time_start_global)/60:.3f} mins left.\")\n",
    "    return comments_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the above functions by searching for comments from a certain channel\n",
    "# filtered_comments_test = filter_comment_by_channel(\"UCzWrhkg9eK5I8Bm3HfV-unA\", video_chunksize=100000, comment_chunksize=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_comments_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "0.677 secs per chunk on average. Meaning  97.075 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "0.726 secs per chunk on average. Meaning  104.056 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "0.748 secs per chunk on average. Meaning  107.159 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.065 secs per chunk on average. Meaning  152.523 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.354 secs per chunk on average. Meaning  193.941 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "1.783 secs per chunk on average. Meaning  255.335 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "1.805 secs per chunk on average. Meaning  258.537 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "1.934 secs per chunk on average. Meaning  276.990 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "2.046 secs per chunk on average. Meaning  292.899 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "2.290 secs per chunk on average. Meaning  327.811 minutes left.\n"
     ]
    }
   ],
   "source": [
    "nans = dp.run_simple_function_on_chunks(comments_in_chunks(), \n",
    "                                        lambda x: dp.get_na_entries(x, \"any\", False),\n",
    "                                        print_time=(1000000, 8600000000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>video_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Gkb1QMHrGvA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CNtp0xqoods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>249EEzQmVmQ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>_U443T2K_Bs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rJbjhm0weYc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>664459</td>\n",
       "      <td>GC3gqIbrK7c</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>664459</td>\n",
       "      <td>GC3gqIbrK7c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>664459</td>\n",
       "      <td>i9VRGaoFw8k</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>664459</td>\n",
       "      <td>-JLWZ1jz3FY</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>664459</td>\n",
       "      <td>-JLWZ1jz3FY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author     video_id  likes  replies\n",
       "0             1  Gkb1QMHrGvA      2        0\n",
       "1             1  CNtp0xqoods      0        0\n",
       "2             1  249EEzQmVmQ      1        0\n",
       "3             1  _U443T2K_Bs      0        0\n",
       "4             1  rJbjhm0weYc      0        0\n",
       "...         ...          ...    ...      ...\n",
       "9999995  664459  GC3gqIbrK7c      9        1\n",
       "9999996  664459  GC3gqIbrK7c      1        0\n",
       "9999997  664459  i9VRGaoFw8k      1        1\n",
       "9999998  664459  -JLWZ1jz3FY      0        2\n",
       "9999999  664459  -JLWZ1jz3FY      0        0\n",
       "\n",
       "[10000000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "0.630 secs per chunk on average. Meaning  90.327 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "0.647 secs per chunk on average. Meaning  92.675 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "0.652 secs per chunk on average. Meaning  93.408 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "0.655 secs per chunk on average. Meaning  93.778 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "0.653 secs per chunk on average. Meaning  93.605 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "0.652 secs per chunk on average. Meaning  93.418 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "0.651 secs per chunk on average. Meaning  93.228 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "0.658 secs per chunk on average. Meaning  94.252 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "0.656 secs per chunk on average. Meaning  93.867 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "0.658 secs per chunk on average. Meaning  94.268 minutes left.\n"
     ]
    }
   ],
   "source": [
    "counted_nans = dp.run_simple_function_on_chunks(comments_in_chunks(), \n",
    "                                                lambda x: dp.count_na_entries(x, \"any\", False),\n",
    "                                                print_time=(1000000, 8600000000)).sum(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "na rows              0\n",
       "total rows    10000000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(counted_nans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
