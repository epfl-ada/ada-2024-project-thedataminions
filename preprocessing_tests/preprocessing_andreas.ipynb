{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_processing' from '/home/andreas/Nextcloud/Dokumente/Uni/Module/3sem-EPFL/ada/Project/ada-2024-project-thedataminions/preprocessing_tests/data_processing.py'>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import data_processing as dp  # own functions and logic\n",
    "importlib.reload(dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the folder where the YouNiverse dataset is stored here\n",
    "\n",
    "# when adding your own path, don't remove the existing path, just comment it\n",
    "# in this way, everyone can quickly uncomment their own path\n",
    "dataset_root_path = \"/media/andreas/Backup Plus/youniverse_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel data\n",
    "df_channels = pd.read_csv(dataset_root_path + \"df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if we want to test something on the dataset without using chunks. Otherwise, keep commented\n",
    "\n",
    "# load (first 100000 rows of) video data\n",
    "df_videos = pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", compression=\"infer\", lines=True, nrows=100000)\n",
    "\n",
    "# load (first 1000000 rows of) comment data\n",
    "df_comments = pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", compression=\"infer\", sep=\"\\t\", nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaming</td>\n",
       "      <td>2010-04-29</td>\n",
       "      <td>UC-lHJZR3Gqxm24_Vd_AJ5Yw</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>101000000</td>\n",
       "      <td>3956</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>2006-09-01</td>\n",
       "      <td>UCbCmjCuTUZos6Inko4u57UQ</td>\n",
       "      <td>Cocomelon - Nursery ...</td>\n",
       "      <td>60100000</td>\n",
       "      <td>458</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2006-09-20</td>\n",
       "      <td>UCpEhnqL0y41EpW2TvWAHD7Q</td>\n",
       "      <td>SET India</td>\n",
       "      <td>56018869</td>\n",
       "      <td>32661</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Howto &amp; Style</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>UC295-Dw_tDNtZXFeAPAW6Aw</td>\n",
       "      <td>5-Minute Crafts</td>\n",
       "      <td>60600000</td>\n",
       "      <td>3591</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sports</td>\n",
       "      <td>2007-05-11</td>\n",
       "      <td>UCJ5v_MCY6GNUBTO8-D3XoAg</td>\n",
       "      <td>WWE</td>\n",
       "      <td>48400000</td>\n",
       "      <td>43421</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136465</th>\n",
       "      <td>Music</td>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>UCuM-9AajUOwKw6ipOzu2DRQ</td>\n",
       "      <td>GONE.Fludd - Topic</td>\n",
       "      <td>10128</td>\n",
       "      <td>105</td>\n",
       "      <td>1008139.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136466</th>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>2013-10-17</td>\n",
       "      <td>UCtW9jp5TH0YrgYpwiRf9t-Q</td>\n",
       "      <td>saidthestory</td>\n",
       "      <td>10100</td>\n",
       "      <td>352</td>\n",
       "      <td>1008644.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136467</th>\n",
       "      <td>Gaming</td>\n",
       "      <td>2015-05-08</td>\n",
       "      <td>UCTsxFTIUs8vFDzGccDm6i7Q</td>\n",
       "      <td>Omni H</td>\n",
       "      <td>10000</td>\n",
       "      <td>475</td>\n",
       "      <td>1009505.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136468</th>\n",
       "      <td>Music</td>\n",
       "      <td>2011-04-13</td>\n",
       "      <td>UC1HOArgRCMGPjlcmkThERwA</td>\n",
       "      <td>TĀLĀ</td>\n",
       "      <td>10000</td>\n",
       "      <td>15</td>\n",
       "      <td>1025119.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136469</th>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>2006-11-11</td>\n",
       "      <td>UCITKvry4fW50iU4FSw9WERQ</td>\n",
       "      <td>Tangleblog</td>\n",
       "      <td>10000</td>\n",
       "      <td>159</td>\n",
       "      <td>1030844.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136470 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category_cc   join_date                   channel  \\\n",
       "0               Gaming  2010-04-29  UC-lHJZR3Gqxm24_Vd_AJ5Yw   \n",
       "1            Education  2006-09-01  UCbCmjCuTUZos6Inko4u57UQ   \n",
       "2        Entertainment  2006-09-20  UCpEhnqL0y41EpW2TvWAHD7Q   \n",
       "3        Howto & Style  2016-11-15  UC295-Dw_tDNtZXFeAPAW6Aw   \n",
       "4               Sports  2007-05-11  UCJ5v_MCY6GNUBTO8-D3XoAg   \n",
       "...                ...         ...                       ...   \n",
       "136465           Music  2016-10-06  UCuM-9AajUOwKw6ipOzu2DRQ   \n",
       "136466  People & Blogs  2013-10-17  UCtW9jp5TH0YrgYpwiRf9t-Q   \n",
       "136467          Gaming  2015-05-08  UCTsxFTIUs8vFDzGccDm6i7Q   \n",
       "136468           Music  2011-04-13  UC1HOArgRCMGPjlcmkThERwA   \n",
       "136469  People & Blogs  2006-11-11  UCITKvry4fW50iU4FSw9WERQ   \n",
       "\n",
       "                        name_cc  subscribers_cc  videos_cc  \\\n",
       "0                     PewDiePie       101000000       3956   \n",
       "1       Cocomelon - Nursery ...        60100000        458   \n",
       "2                     SET India        56018869      32661   \n",
       "3               5-Minute Crafts        60600000       3591   \n",
       "4                           WWE        48400000      43421   \n",
       "...                         ...             ...        ...   \n",
       "136465       GONE.Fludd - Topic           10128        105   \n",
       "136466             saidthestory           10100        352   \n",
       "136467                   Omni H           10000        475   \n",
       "136468                     TĀLĀ           10000         15   \n",
       "136469               Tangleblog           10000        159   \n",
       "\n",
       "        subscriber_rank_sb  weights  \n",
       "0                      3.0   2.0870  \n",
       "1                      7.0   2.0870  \n",
       "2                      8.0   2.0870  \n",
       "3                      9.0   2.0870  \n",
       "4                     11.0   2.0870  \n",
       "...                    ...      ...  \n",
       "136465           1008139.0  53.1435  \n",
       "136466           1008644.0  53.1435  \n",
       "136467           1009505.0  53.1435  \n",
       "136468           1025119.0  53.1435  \n",
       "136469           1030844.0  53.1435  \n",
       "\n",
       "[136470 rows x 8 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of \"readers\", i.e., objects that we can iterate through \n",
    "# and always get a chunk of the dataframe in each iteration\n",
    "\n",
    "def videos_in_chunks(chunksize: int = 100000):\n",
    "    return pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", \n",
    "                        compression=\"infer\", lines=True, chunksize=chunksize, \n",
    "                        nrows=1000000, )   # uncomment this to only use the first million videos, for testing\n",
    "                                             # (remove the paranthesis above as well)\n",
    "\n",
    "def comments_in_chunks(chunksize: int = 1000000):\n",
    "    return pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", \n",
    "                       compression=\"infer\", sep=\"\\t\", chunksize=chunksize, \n",
    "                       nrows = 10000000)  # uncomment this to only use the first 10 million comments, for testing\n",
    "                                            # (remove the paranthesis above as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vid_by_channel(channel_id: str, video_chunksize: int = 100000) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Returns a dataframe which is the video metadata dataframe, filtered so that only videos from the \n",
    "    given channel remain.\n",
    "\n",
    "    The function does this by going through the dataset in chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        channel_id :  id of the channel which the videos will be sorted by\n",
    "        video_chunksize :  number of entries in each chunk. Default is 100 000.\n",
    "\n",
    "    Returns:\n",
    "        The filtered dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    videos_filtered = pd.DataFrame(dict())\n",
    "    \n",
    "    with videos_in_chunks(video_chunksize) as reader:\n",
    "        time_start_global = time.time()\n",
    "        i=0\n",
    "        for video_chunk in reader:\n",
    "            time_start_chunk = time.time()\n",
    "            print(f\"Going through video chunk {i}...\")\n",
    "            videos_filtered = pd.concat([videos_filtered, video_chunk.loc[video_chunk.channel_id == channel_id]])\n",
    "            print(f\"The first {(i+1) * video_chunksize} videos have been processed.\")\n",
    "            time_end = time.time()\n",
    "            print(f\"{(time_end-time_start_global)/(i+1):.3f} secs per chunk on average. Meaning {72924794 / video_chunksize * (time_end-time_start_global)/((i+1)*60):.3f} mins for entire dataset, i.e., {72924794 / video_chunksize * (time_end-time_start_global)/((i+1)*60) - (time_end-time_start_global)/60:.3f} mins left.\")\n",
    "            i=i+1\n",
    "    return videos_filtered\n",
    "\n",
    "def filter_comment_by_channel(channel_id: str,video_chunksize: int = 100000, comment_chunksize: int = 1000000) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Returns a dataframe which is the comment data, but filtered so that only comments\n",
    "    made on videos which were published by a certain channel are left.\n",
    "\n",
    "    The function does this by going through the dataset in chunks of a defined size.\n",
    "\n",
    "    Args:\n",
    "        channel_id: id of the channel to be filtered by\n",
    "        video_chunksize: number of entries per chunk when going through the videos (to find the videos uploaded by a certain channel)\n",
    "        comment_chunksize: number of entries per chunk when going through the comments\n",
    "\n",
    "    Returns:\n",
    "        The filtered dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    comments_filtered = pd.DataFrame(dict())\n",
    "    filtered_videos = filter_vid_by_channel(channel_id, video_chunksize=video_chunksize)\n",
    "    print(f\"Videos have been filtered by channel, {len(filtered_videos)} videos found. \\nNow going through comments....\")\n",
    "    with comments_in_chunks(comment_chunksize) as reader:\n",
    "        time_start_global = time.time()\n",
    "        for i, comment_chunk in enumerate(reader):\n",
    "            print(f\"Going through comment chunk {i}...\")\n",
    "            comments_filtered = pd.concat([comments_filtered, \n",
    "                                comment_chunk.loc[comment_chunk.video_id.isin(filtered_videos.display_id)]])\n",
    "            print(f\"The first {(i+1) * comment_chunksize} comments have been processed\")\n",
    "            time_end = time.time()\n",
    "            print(f\"{(time_end-time_start_global)/(i+1):.3f} secs per chunk on average.Meaning {8600000000 / comment_chunksize * (time_end-time_start_global)/((i+1)*60):.3f} mins for entire dataset, i.e., {8600000000 / comment_chunksize * (time_end-time_start_global)/((i+1)*60) - (time_end-time_start_global)/60:.3f} mins left.\")\n",
    "    return comments_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through video chunk 0...\n",
      "The first 100000 videos have been processed.\n",
      "9.979 secs per chunk on average. Meaning 121.287 mins for entire dataset, i.e., 121.121 mins left.\n",
      "Going through video chunk 1...\n",
      "The first 200000 videos have been processed.\n",
      "8.103 secs per chunk on average. Meaning 98.479 mins for entire dataset, i.e., 98.209 mins left.\n",
      "Going through video chunk 2...\n",
      "The first 300000 videos have been processed.\n",
      "7.223 secs per chunk on average. Meaning 87.790 mins for entire dataset, i.e., 87.429 mins left.\n",
      "Going through video chunk 3...\n",
      "The first 400000 videos have been processed.\n",
      "6.482 secs per chunk on average. Meaning 78.782 mins for entire dataset, i.e., 78.350 mins left.\n",
      "Going through video chunk 4...\n",
      "The first 500000 videos have been processed.\n",
      "5.906 secs per chunk on average. Meaning 71.781 mins for entire dataset, i.e., 71.288 mins left.\n",
      "Going through video chunk 5...\n",
      "The first 600000 videos have been processed.\n",
      "5.530 secs per chunk on average. Meaning 67.217 mins for entire dataset, i.e., 66.664 mins left.\n",
      "Going through video chunk 6...\n",
      "The first 700000 videos have been processed.\n",
      "5.295 secs per chunk on average. Meaning 64.361 mins for entire dataset, i.e., 63.743 mins left.\n",
      "Going through video chunk 7...\n",
      "The first 800000 videos have been processed.\n",
      "5.072 secs per chunk on average. Meaning 61.651 mins for entire dataset, i.e., 60.975 mins left.\n",
      "Going through video chunk 8...\n",
      "The first 900000 videos have been processed.\n",
      "4.965 secs per chunk on average. Meaning 60.339 mins for entire dataset, i.e., 59.595 mins left.\n",
      "Going through video chunk 9...\n",
      "The first 1000000 videos have been processed.\n",
      "4.872 secs per chunk on average. Meaning 59.218 mins for entire dataset, i.e., 58.406 mins left.\n",
      "Videos have been filtered by channel, 1281 videos found. \n",
      "Now going through comments....\n",
      "Going through comment chunk 0...\n",
      "The first 1000000 comments have been processed\n",
      "1.285 secs per chunk on average.Meaning 184.240 mins for entire dataset, i.e., 184.219 mins left.\n",
      "Going through comment chunk 1...\n",
      "The first 2000000 comments have been processed\n",
      "1.331 secs per chunk on average.Meaning 190.832 mins for entire dataset, i.e., 190.788 mins left.\n",
      "Going through comment chunk 2...\n",
      "The first 3000000 comments have been processed\n",
      "1.441 secs per chunk on average.Meaning 206.541 mins for entire dataset, i.e., 206.469 mins left.\n",
      "Going through comment chunk 3...\n",
      "The first 4000000 comments have been processed\n",
      "1.412 secs per chunk on average.Meaning 202.357 mins for entire dataset, i.e., 202.263 mins left.\n",
      "Going through comment chunk 4...\n",
      "The first 5000000 comments have been processed\n",
      "1.393 secs per chunk on average.Meaning 199.673 mins for entire dataset, i.e., 199.557 mins left.\n",
      "Going through comment chunk 5...\n",
      "The first 6000000 comments have been processed\n",
      "1.372 secs per chunk on average.Meaning 196.587 mins for entire dataset, i.e., 196.450 mins left.\n",
      "Going through comment chunk 6...\n",
      "The first 7000000 comments have been processed\n",
      "1.384 secs per chunk on average.Meaning 198.351 mins for entire dataset, i.e., 198.189 mins left.\n",
      "Going through comment chunk 7...\n",
      "The first 8000000 comments have been processed\n",
      "1.374 secs per chunk on average.Meaning 196.998 mins for entire dataset, i.e., 196.815 mins left.\n",
      "Going through comment chunk 8...\n",
      "The first 9000000 comments have been processed\n",
      "1.364 secs per chunk on average.Meaning 195.442 mins for entire dataset, i.e., 195.238 mins left.\n",
      "Going through comment chunk 9...\n",
      "The first 10000000 comments have been processed\n",
      "1.356 secs per chunk on average.Meaning 194.301 mins for entire dataset, i.e., 194.075 mins left.\n",
      "Going through comment chunk 10...\n",
      "The first 11000000 comments have been processed\n",
      "1.352 secs per chunk on average.Meaning 193.797 mins for entire dataset, i.e., 193.549 mins left.\n",
      "Going through comment chunk 11...\n",
      "The first 12000000 comments have been processed\n",
      "1.339 secs per chunk on average.Meaning 191.883 mins for entire dataset, i.e., 191.615 mins left.\n",
      "Going through comment chunk 12...\n",
      "The first 13000000 comments have been processed\n",
      "1.338 secs per chunk on average.Meaning 191.722 mins for entire dataset, i.e., 191.432 mins left.\n",
      "Going through comment chunk 13...\n",
      "The first 14000000 comments have been processed\n",
      "1.330 secs per chunk on average.Meaning 190.701 mins for entire dataset, i.e., 190.390 mins left.\n",
      "Going through comment chunk 14...\n",
      "The first 15000000 comments have been processed\n",
      "1.325 secs per chunk on average.Meaning 189.939 mins for entire dataset, i.e., 189.608 mins left.\n",
      "Going through comment chunk 15...\n",
      "The first 16000000 comments have been processed\n",
      "1.319 secs per chunk on average.Meaning 189.068 mins for entire dataset, i.e., 188.717 mins left.\n",
      "Going through comment chunk 16...\n",
      "The first 17000000 comments have been processed\n",
      "1.319 secs per chunk on average.Meaning 189.107 mins for entire dataset, i.e., 188.733 mins left.\n",
      "Going through comment chunk 17...\n",
      "The first 18000000 comments have been processed\n",
      "1.327 secs per chunk on average.Meaning 190.144 mins for entire dataset, i.e., 189.746 mins left.\n",
      "Going through comment chunk 18...\n",
      "The first 19000000 comments have been processed\n",
      "1.329 secs per chunk on average.Meaning 190.438 mins for entire dataset, i.e., 190.017 mins left.\n",
      "Going through comment chunk 19...\n",
      "The first 20000000 comments have been processed\n",
      "1.338 secs per chunk on average.Meaning 191.771 mins for entire dataset, i.e., 191.325 mins left.\n",
      "Going through comment chunk 20...\n",
      "The first 21000000 comments have been processed\n",
      "1.340 secs per chunk on average.Meaning 192.125 mins for entire dataset, i.e., 191.656 mins left.\n",
      "Going through comment chunk 21...\n",
      "The first 22000000 comments have been processed\n",
      "1.343 secs per chunk on average.Meaning 192.454 mins for entire dataset, i.e., 191.962 mins left.\n",
      "Going through comment chunk 22...\n",
      "The first 23000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.966 mins for entire dataset, i.e., 192.450 mins left.\n",
      "Going through comment chunk 23...\n",
      "The first 24000000 comments have been processed\n",
      "1.349 secs per chunk on average.Meaning 193.387 mins for entire dataset, i.e., 192.847 mins left.\n",
      "Going through comment chunk 24...\n",
      "The first 25000000 comments have been processed\n",
      "1.350 secs per chunk on average.Meaning 193.504 mins for entire dataset, i.e., 192.941 mins left.\n",
      "Going through comment chunk 25...\n",
      "The first 26000000 comments have been processed\n",
      "1.349 secs per chunk on average.Meaning 193.294 mins for entire dataset, i.e., 192.709 mins left.\n",
      "Going through comment chunk 26...\n",
      "The first 27000000 comments have been processed\n",
      "1.348 secs per chunk on average.Meaning 193.221 mins for entire dataset, i.e., 192.614 mins left.\n",
      "Going through comment chunk 27...\n",
      "The first 28000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.954 mins for entire dataset, i.e., 192.326 mins left.\n",
      "Going through comment chunk 28...\n",
      "The first 29000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.138 mins for entire dataset, i.e., 192.486 mins left.\n",
      "Going through comment chunk 29...\n",
      "The first 30000000 comments have been processed\n",
      "1.353 secs per chunk on average.Meaning 193.946 mins for entire dataset, i.e., 193.270 mins left.\n",
      "Going through comment chunk 30...\n",
      "The first 31000000 comments have been processed\n",
      "1.358 secs per chunk on average.Meaning 194.603 mins for entire dataset, i.e., 193.901 mins left.\n",
      "Going through comment chunk 31...\n",
      "The first 32000000 comments have been processed\n",
      "1.356 secs per chunk on average.Meaning 194.391 mins for entire dataset, i.e., 193.668 mins left.\n",
      "Going through comment chunk 32...\n",
      "The first 33000000 comments have been processed\n",
      "1.358 secs per chunk on average.Meaning 194.632 mins for entire dataset, i.e., 193.885 mins left.\n",
      "Going through comment chunk 33...\n",
      "The first 34000000 comments have been processed\n",
      "1.357 secs per chunk on average.Meaning 194.523 mins for entire dataset, i.e., 193.754 mins left.\n",
      "Going through comment chunk 34...\n",
      "The first 35000000 comments have been processed\n",
      "1.357 secs per chunk on average.Meaning 194.474 mins for entire dataset, i.e., 193.683 mins left.\n",
      "Going through comment chunk 35...\n",
      "The first 36000000 comments have been processed\n",
      "1.357 secs per chunk on average.Meaning 194.566 mins for entire dataset, i.e., 193.751 mins left.\n",
      "Going through comment chunk 36...\n",
      "The first 37000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.780 mins for entire dataset, i.e., 193.942 mins left.\n",
      "Going through comment chunk 37...\n",
      "The first 38000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.794 mins for entire dataset, i.e., 193.933 mins left.\n",
      "Going through comment chunk 38...\n",
      "The first 39000000 comments have been processed\n",
      "1.361 secs per chunk on average.Meaning 195.097 mins for entire dataset, i.e., 194.212 mins left.\n",
      "Going through comment chunk 39...\n",
      "The first 40000000 comments have been processed\n",
      "1.360 secs per chunk on average.Meaning 194.880 mins for entire dataset, i.e., 193.973 mins left.\n",
      "Going through comment chunk 40...\n",
      "The first 41000000 comments have been processed\n",
      "1.360 secs per chunk on average.Meaning 194.879 mins for entire dataset, i.e., 193.950 mins left.\n",
      "Going through comment chunk 41...\n",
      "The first 42000000 comments have been processed\n",
      "1.360 secs per chunk on average.Meaning 194.926 mins for entire dataset, i.e., 193.974 mins left.\n",
      "Going through comment chunk 42...\n",
      "The first 43000000 comments have been processed\n",
      "1.357 secs per chunk on average.Meaning 194.547 mins for entire dataset, i.e., 193.574 mins left.\n",
      "Going through comment chunk 43...\n",
      "The first 44000000 comments have been processed\n",
      "1.355 secs per chunk on average.Meaning 194.168 mins for entire dataset, i.e., 193.175 mins left.\n",
      "Going through comment chunk 44...\n",
      "The first 45000000 comments have been processed\n",
      "1.354 secs per chunk on average.Meaning 194.111 mins for entire dataset, i.e., 193.096 mins left.\n",
      "Going through comment chunk 45...\n",
      "The first 46000000 comments have been processed\n",
      "1.353 secs per chunk on average.Meaning 193.938 mins for entire dataset, i.e., 192.901 mins left.\n",
      "Going through comment chunk 46...\n",
      "The first 47000000 comments have been processed\n",
      "1.353 secs per chunk on average.Meaning 193.996 mins for entire dataset, i.e., 192.936 mins left.\n",
      "Going through comment chunk 47...\n",
      "The first 48000000 comments have been processed\n",
      "1.354 secs per chunk on average.Meaning 194.126 mins for entire dataset, i.e., 193.043 mins left.\n",
      "Going through comment chunk 48...\n",
      "The first 49000000 comments have been processed\n",
      "1.356 secs per chunk on average.Meaning 194.366 mins for entire dataset, i.e., 193.258 mins left.\n",
      "Going through comment chunk 49...\n",
      "The first 50000000 comments have been processed\n",
      "1.356 secs per chunk on average.Meaning 194.340 mins for entire dataset, i.e., 193.211 mins left.\n",
      "Going through comment chunk 50...\n",
      "The first 51000000 comments have been processed\n",
      "1.358 secs per chunk on average.Meaning 194.593 mins for entire dataset, i.e., 193.439 mins left.\n",
      "Going through comment chunk 51...\n",
      "The first 52000000 comments have been processed\n",
      "1.357 secs per chunk on average.Meaning 194.463 mins for entire dataset, i.e., 193.287 mins left.\n",
      "Going through comment chunk 52...\n",
      "The first 53000000 comments have been processed\n",
      "1.357 secs per chunk on average.Meaning 194.494 mins for entire dataset, i.e., 193.295 mins left.\n",
      "Going through comment chunk 53...\n",
      "The first 54000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.791 mins for entire dataset, i.e., 193.568 mins left.\n",
      "Going through comment chunk 54...\n",
      "The first 55000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.791 mins for entire dataset, i.e., 193.545 mins left.\n",
      "Going through comment chunk 55...\n",
      "The first 56000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.833 mins for entire dataset, i.e., 193.564 mins left.\n",
      "Going through comment chunk 56...\n",
      "The first 57000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.829 mins for entire dataset, i.e., 193.538 mins left.\n",
      "Going through comment chunk 57...\n",
      "The first 58000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.735 mins for entire dataset, i.e., 193.421 mins left.\n",
      "Going through comment chunk 58...\n",
      "The first 59000000 comments have been processed\n",
      "1.357 secs per chunk on average.Meaning 194.575 mins for entire dataset, i.e., 193.240 mins left.\n",
      "Going through comment chunk 59...\n",
      "The first 60000000 comments have been processed\n",
      "1.356 secs per chunk on average.Meaning 194.418 mins for entire dataset, i.e., 193.062 mins left.\n",
      "Going through comment chunk 60...\n",
      "The first 61000000 comments have been processed\n",
      "1.359 secs per chunk on average.Meaning 194.828 mins for entire dataset, i.e., 193.446 mins left.\n",
      "Going through comment chunk 61...\n",
      "The first 62000000 comments have been processed\n",
      "1.358 secs per chunk on average.Meaning 194.675 mins for entire dataset, i.e., 193.272 mins left.\n",
      "Going through comment chunk 62...\n",
      "The first 63000000 comments have been processed\n",
      "1.360 secs per chunk on average.Meaning 194.881 mins for entire dataset, i.e., 193.454 mins left.\n",
      "Going through comment chunk 63...\n",
      "The first 64000000 comments have been processed\n",
      "1.358 secs per chunk on average.Meaning 194.626 mins for entire dataset, i.e., 193.178 mins left.\n",
      "Going through comment chunk 64...\n",
      "The first 65000000 comments have been processed\n",
      "1.356 secs per chunk on average.Meaning 194.425 mins for entire dataset, i.e., 192.956 mins left.\n",
      "Going through comment chunk 65...\n",
      "The first 66000000 comments have been processed\n",
      "1.355 secs per chunk on average.Meaning 194.255 mins for entire dataset, i.e., 192.765 mins left.\n",
      "Going through comment chunk 66...\n",
      "The first 67000000 comments have been processed\n",
      "1.355 secs per chunk on average.Meaning 194.189 mins for entire dataset, i.e., 192.677 mins left.\n",
      "Going through comment chunk 67...\n",
      "The first 68000000 comments have been processed\n",
      "1.354 secs per chunk on average.Meaning 194.013 mins for entire dataset, i.e., 192.479 mins left.\n",
      "Going through comment chunk 68...\n",
      "The first 69000000 comments have been processed\n",
      "1.354 secs per chunk on average.Meaning 194.050 mins for entire dataset, i.e., 192.493 mins left.\n",
      "Going through comment chunk 69...\n",
      "The first 70000000 comments have been processed\n",
      "1.353 secs per chunk on average.Meaning 193.933 mins for entire dataset, i.e., 192.354 mins left.\n",
      "Going through comment chunk 70...\n",
      "The first 71000000 comments have been processed\n",
      "1.353 secs per chunk on average.Meaning 193.867 mins for entire dataset, i.e., 192.267 mins left.\n",
      "Going through comment chunk 71...\n",
      "The first 72000000 comments have been processed\n",
      "1.351 secs per chunk on average.Meaning 193.619 mins for entire dataset, i.e., 191.998 mins left.\n",
      "Going through comment chunk 72...\n",
      "The first 73000000 comments have been processed\n",
      "1.349 secs per chunk on average.Meaning 193.333 mins for entire dataset, i.e., 191.692 mins left.\n",
      "Going through comment chunk 73...\n",
      "The first 74000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.046 mins for entire dataset, i.e., 191.385 mins left.\n",
      "Going through comment chunk 74...\n",
      "The first 75000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.889 mins for entire dataset, i.e., 191.207 mins left.\n",
      "Going through comment chunk 75...\n",
      "The first 76000000 comments have been processed\n",
      "1.345 secs per chunk on average.Meaning 192.780 mins for entire dataset, i.e., 191.077 mins left.\n",
      "Going through comment chunk 76...\n",
      "The first 77000000 comments have been processed\n",
      "1.345 secs per chunk on average.Meaning 192.765 mins for entire dataset, i.e., 191.039 mins left.\n",
      "Going through comment chunk 77...\n",
      "The first 78000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.019 mins for entire dataset, i.e., 191.268 mins left.\n",
      "Going through comment chunk 78...\n",
      "The first 79000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.019 mins for entire dataset, i.e., 191.246 mins left.\n",
      "Going through comment chunk 79...\n",
      "The first 80000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.028 mins for entire dataset, i.e., 191.232 mins left.\n",
      "Going through comment chunk 80...\n",
      "The first 81000000 comments have been processed\n",
      "1.350 secs per chunk on average.Meaning 193.470 mins for entire dataset, i.e., 191.648 mins left.\n",
      "Going through comment chunk 81...\n",
      "The first 82000000 comments have been processed\n",
      "1.350 secs per chunk on average.Meaning 193.527 mins for entire dataset, i.e., 191.682 mins left.\n",
      "Going through comment chunk 82...\n",
      "The first 83000000 comments have been processed\n",
      "1.350 secs per chunk on average.Meaning 193.434 mins for entire dataset, i.e., 191.567 mins left.\n",
      "Going through comment chunk 83...\n",
      "The first 84000000 comments have been processed\n",
      "1.348 secs per chunk on average.Meaning 193.149 mins for entire dataset, i.e., 191.263 mins left.\n",
      "Going through comment chunk 84...\n",
      "The first 85000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.045 mins for entire dataset, i.e., 191.137 mins left.\n",
      "Going through comment chunk 85...\n",
      "The first 86000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.993 mins for entire dataset, i.e., 191.063 mins left.\n",
      "Going through comment chunk 86...\n",
      "The first 87000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.044 mins for entire dataset, i.e., 191.091 mins left.\n",
      "Going through comment chunk 87...\n",
      "The first 88000000 comments have been processed\n",
      "1.348 secs per chunk on average.Meaning 193.147 mins for entire dataset, i.e., 191.170 mins left.\n",
      "Going through comment chunk 88...\n",
      "The first 89000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.131 mins for entire dataset, i.e., 191.132 mins left.\n",
      "Going through comment chunk 89...\n",
      "The first 90000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.990 mins for entire dataset, i.e., 190.970 mins left.\n",
      "Going through comment chunk 90...\n",
      "The first 91000000 comments have been processed\n",
      "1.345 secs per chunk on average.Meaning 192.837 mins for entire dataset, i.e., 190.797 mins left.\n",
      "Going through comment chunk 91...\n",
      "The first 92000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.918 mins for entire dataset, i.e., 190.855 mins left.\n",
      "Going through comment chunk 92...\n",
      "The first 93000000 comments have been processed\n",
      "1.348 secs per chunk on average.Meaning 193.143 mins for entire dataset, i.e., 191.054 mins left.\n",
      "Going through comment chunk 93...\n",
      "The first 94000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.057 mins for entire dataset, i.e., 190.946 mins left.\n",
      "Going through comment chunk 94...\n",
      "The first 95000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.093 mins for entire dataset, i.e., 190.960 mins left.\n",
      "Going through comment chunk 95...\n",
      "The first 96000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.004 mins for entire dataset, i.e., 190.849 mins left.\n",
      "Going through comment chunk 96...\n",
      "The first 97000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.029 mins for entire dataset, i.e., 190.852 mins left.\n",
      "Going through comment chunk 97...\n",
      "The first 98000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.026 mins for entire dataset, i.e., 190.826 mins left.\n",
      "Going through comment chunk 98...\n",
      "The first 99000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.992 mins for entire dataset, i.e., 190.770 mins left.\n",
      "Going through comment chunk 99...\n",
      "The first 100000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.914 mins for entire dataset, i.e., 190.671 mins left.\n",
      "Going through comment chunk 100...\n",
      "The first 101000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.978 mins for entire dataset, i.e., 190.712 mins left.\n",
      "Going through comment chunk 101...\n",
      "The first 102000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.919 mins for entire dataset, i.e., 190.631 mins left.\n",
      "Going through comment chunk 102...\n",
      "The first 103000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.935 mins for entire dataset, i.e., 190.624 mins left.\n",
      "Going through comment chunk 103...\n",
      "The first 104000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.097 mins for entire dataset, i.e., 190.762 mins left.\n",
      "Going through comment chunk 104...\n",
      "The first 105000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.062 mins for entire dataset, i.e., 190.704 mins left.\n",
      "Going through comment chunk 105...\n",
      "The first 106000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.058 mins for entire dataset, i.e., 190.679 mins left.\n",
      "Going through comment chunk 106...\n",
      "The first 107000000 comments have been processed\n",
      "1.348 secs per chunk on average.Meaning 193.152 mins for entire dataset, i.e., 190.749 mins left.\n",
      "Going through comment chunk 107...\n",
      "The first 108000000 comments have been processed\n",
      "1.348 secs per chunk on average.Meaning 193.237 mins for entire dataset, i.e., 190.810 mins left.\n",
      "Going through comment chunk 108...\n",
      "The first 109000000 comments have been processed\n",
      "1.348 secs per chunk on average.Meaning 193.147 mins for entire dataset, i.e., 190.699 mins left.\n",
      "Going through comment chunk 109...\n",
      "The first 110000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.075 mins for entire dataset, i.e., 190.605 mins left.\n",
      "Going through comment chunk 110...\n",
      "The first 111000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.014 mins for entire dataset, i.e., 190.523 mins left.\n",
      "Going through comment chunk 111...\n",
      "The first 112000000 comments have been processed\n",
      "1.346 secs per chunk on average.Meaning 192.998 mins for entire dataset, i.e., 190.485 mins left.\n",
      "Going through comment chunk 112...\n",
      "The first 113000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.107 mins for entire dataset, i.e., 190.569 mins left.\n",
      "Going through comment chunk 113...\n",
      "The first 114000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.037 mins for entire dataset, i.e., 190.478 mins left.\n",
      "Going through comment chunk 114...\n",
      "The first 115000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.061 mins for entire dataset, i.e., 190.479 mins left.\n",
      "Going through comment chunk 115...\n",
      "The first 116000000 comments have been processed\n",
      "1.347 secs per chunk on average.Meaning 193.111 mins for entire dataset, i.e., 190.506 mins left.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test the above functions by searching for comments from a certain channel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m filtered_comments_test \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_comment_by_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUCzWrhkg9eK5I8Bm3HfV-unA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[182], line 52\u001b[0m, in \u001b[0;36mfilter_comment_by_channel\u001b[0;34m(channel_id, video_chunksize, comment_chunksize)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m comments_in_chunks(comment_chunksize) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m     51\u001b[0m     time_start_global \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGoing through comment chunk \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomments_filtered\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomments_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcomment_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomment_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_videos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/gzip.py:507\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    505\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 507\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test the above functions by searching for comments from a certain channel\n",
    "filtered_comments_test = filter_comment_by_channel(\"UCzWrhkg9eK5I8Bm3HfV-unA\", video_chunksize=100000, comment_chunksize=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_comments_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[262], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfiltered_comments_test\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_comments_test' is not defined"
     ]
    }
   ],
   "source": [
    "# filtered_comments_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: next step\n",
    "\n",
    "make a function which takes dataframes such as the above \"filtered_comments_test\" (df with comment data from videos only from one specific channel),\n",
    "and returns the number of comments by each user id\n",
    "( filtered_comments_test.groupby(\"user_id\").agg(\"sum\") )\n",
    "\n",
    "sort so that only users with x number of comments remain\n",
    "\n",
    "(flexible function where you give the threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo after that\n",
    "\n",
    "create dataset of all comments which are under a video in the news and politics category\n",
    "\n",
    "use this dataset to get the list of videos under which each of the users we found (above) have made a comment\n",
    "\n",
    "for each pair of users, calculate \"number of videos in common (under which both have commented) / min number of videos both users have commented on\"\n",
    "(example: mila commented on 10 videos, andreas on 100 videos, they have 8 videos they both commented on, so the value we calculate is 8/10 = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "1.133 secs per chunk on average. Meaning  162.410 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "1.161 secs per chunk on average. Meaning  166.386 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "1.188 secs per chunk on average. Meaning  170.172 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "1.179 secs per chunk on average. Meaning  168.851 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "1.175 secs per chunk on average. Meaning  168.353 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "1.180 secs per chunk on average. Meaning  169.047 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "1.180 secs per chunk on average. Meaning  168.946 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "1.187 secs per chunk on average. Meaning  170.013 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "1.192 secs per chunk on average. Meaning  170.659 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "1.198 secs per chunk on average. Meaning  171.486 minutes left.\n",
      "Going through chunk 10...\n",
      "The first 11000000 entries have been processed. 8589000000 left.\n",
      "1.204 secs per chunk on average. Meaning  172.411 minutes left.\n",
      "Going through chunk 11...\n",
      "The first 12000000 entries have been processed. 8588000000 left.\n",
      "1.197 secs per chunk on average. Meaning  171.369 minutes left.\n",
      "Going through chunk 12...\n",
      "The first 13000000 entries have been processed. 8587000000 left.\n",
      "1.199 secs per chunk on average. Meaning  171.614 minutes left.\n",
      "Going through chunk 13...\n",
      "The first 14000000 entries have been processed. 8586000000 left.\n",
      "1.195 secs per chunk on average. Meaning  170.961 minutes left.\n",
      "Going through chunk 14...\n",
      "The first 15000000 entries have been processed. 8585000000 left.\n",
      "1.194 secs per chunk on average. Meaning  170.883 minutes left.\n",
      "Going through chunk 15...\n",
      "The first 16000000 entries have been processed. 8584000000 left.\n",
      "1.188 secs per chunk on average. Meaning  169.946 minutes left.\n",
      "Going through chunk 16...\n",
      "The first 17000000 entries have been processed. 8583000000 left.\n",
      "1.187 secs per chunk on average. Meaning  169.854 minutes left.\n",
      "Going through chunk 17...\n",
      "The first 18000000 entries have been processed. 8582000000 left.\n",
      "1.189 secs per chunk on average. Meaning  170.076 minutes left.\n",
      "Going through chunk 18...\n",
      "The first 19000000 entries have been processed. 8581000000 left.\n",
      "1.189 secs per chunk on average. Meaning  170.049 minutes left.\n",
      "Going through chunk 19...\n",
      "The first 20000000 entries have been processed. 8580000000 left.\n",
      "1.189 secs per chunk on average. Meaning  169.992 minutes left.\n",
      "Going through chunk 20...\n",
      "The first 21000000 entries have been processed. 8579000000 left.\n",
      "1.192 secs per chunk on average. Meaning  170.418 minutes left.\n",
      "Going through chunk 21...\n",
      "The first 22000000 entries have been processed. 8578000000 left.\n",
      "1.193 secs per chunk on average. Meaning  170.620 minutes left.\n",
      "Going through chunk 22...\n",
      "The first 23000000 entries have been processed. 8577000000 left.\n",
      "1.196 secs per chunk on average. Meaning  170.978 minutes left.\n",
      "Going through chunk 23...\n",
      "The first 24000000 entries have been processed. 8576000000 left.\n",
      "1.198 secs per chunk on average. Meaning  171.181 minutes left.\n",
      "Going through chunk 24...\n",
      "The first 25000000 entries have been processed. 8575000000 left.\n",
      "1.203 secs per chunk on average. Meaning  171.992 minutes left.\n",
      "Going through chunk 25...\n",
      "The first 26000000 entries have been processed. 8574000000 left.\n",
      "1.205 secs per chunk on average. Meaning  172.225 minutes left.\n",
      "Going through chunk 26...\n",
      "The first 27000000 entries have been processed. 8573000000 left.\n",
      "1.206 secs per chunk on average. Meaning  172.355 minutes left.\n",
      "Going through chunk 27...\n",
      "The first 28000000 entries have been processed. 8572000000 left.\n",
      "1.204 secs per chunk on average. Meaning  172.062 minutes left.\n",
      "Going through chunk 28...\n",
      "The first 29000000 entries have been processed. 8571000000 left.\n",
      "1.205 secs per chunk on average. Meaning  172.151 minutes left.\n",
      "Going through chunk 29...\n",
      "The first 30000000 entries have been processed. 8570000000 left.\n",
      "1.205 secs per chunk on average. Meaning  172.109 minutes left.\n",
      "Going through chunk 30...\n",
      "The first 31000000 entries have been processed. 8569000000 left.\n",
      "1.206 secs per chunk on average. Meaning  172.246 minutes left.\n",
      "Going through chunk 31...\n",
      "The first 32000000 entries have been processed. 8568000000 left.\n",
      "1.206 secs per chunk on average. Meaning  172.227 minutes left.\n",
      "Going through chunk 32...\n",
      "The first 33000000 entries have been processed. 8567000000 left.\n",
      "1.208 secs per chunk on average. Meaning  172.508 minutes left.\n",
      "Going through chunk 33...\n",
      "The first 34000000 entries have been processed. 8566000000 left.\n",
      "1.205 secs per chunk on average. Meaning  172.057 minutes left.\n",
      "Going through chunk 34...\n",
      "The first 35000000 entries have been processed. 8565000000 left.\n",
      "1.209 secs per chunk on average. Meaning  172.525 minutes left.\n",
      "Going through chunk 35...\n",
      "The first 36000000 entries have been processed. 8564000000 left.\n",
      "1.209 secs per chunk on average. Meaning  172.626 minutes left.\n",
      "Going through chunk 36...\n",
      "The first 37000000 entries have been processed. 8563000000 left.\n",
      "1.210 secs per chunk on average. Meaning  172.677 minutes left.\n",
      "Going through chunk 37...\n",
      "The first 38000000 entries have been processed. 8562000000 left.\n",
      "1.219 secs per chunk on average. Meaning  173.941 minutes left.\n",
      "Going through chunk 38...\n",
      "The first 39000000 entries have been processed. 8561000000 left.\n",
      "1.220 secs per chunk on average. Meaning  174.022 minutes left.\n",
      "Going through chunk 39...\n",
      "The first 40000000 entries have been processed. 8560000000 left.\n",
      "1.220 secs per chunk on average. Meaning  174.074 minutes left.\n",
      "Going through chunk 40...\n",
      "The first 41000000 entries have been processed. 8559000000 left.\n",
      "1.222 secs per chunk on average. Meaning  174.380 minutes left.\n",
      "Going through chunk 41...\n",
      "The first 42000000 entries have been processed. 8558000000 left.\n",
      "1.224 secs per chunk on average. Meaning  174.569 minutes left.\n",
      "Going through chunk 42...\n",
      "The first 43000000 entries have been processed. 8557000000 left.\n",
      "1.224 secs per chunk on average. Meaning  174.580 minutes left.\n",
      "Going through chunk 43...\n",
      "The first 44000000 entries have been processed. 8556000000 left.\n",
      "1.224 secs per chunk on average. Meaning  174.483 minutes left.\n",
      "Going through chunk 44...\n",
      "The first 45000000 entries have been processed. 8555000000 left.\n",
      "1.224 secs per chunk on average. Meaning  174.479 minutes left.\n",
      "Going through chunk 45...\n",
      "The first 46000000 entries have been processed. 8554000000 left.\n",
      "1.222 secs per chunk on average. Meaning  174.219 minutes left.\n",
      "Going through chunk 46...\n",
      "The first 47000000 entries have been processed. 8553000000 left.\n",
      "1.222 secs per chunk on average. Meaning  174.257 minutes left.\n",
      "Going through chunk 47...\n",
      "The first 48000000 entries have been processed. 8552000000 left.\n",
      "1.223 secs per chunk on average. Meaning  174.254 minutes left.\n",
      "Going through chunk 48...\n",
      "The first 49000000 entries have been processed. 8551000000 left.\n",
      "1.222 secs per chunk on average. Meaning  174.175 minutes left.\n",
      "Going through chunk 49...\n",
      "The first 50000000 entries have been processed. 8550000000 left.\n",
      "1.223 secs per chunk on average. Meaning  174.312 minutes left.\n",
      "Going through chunk 50...\n",
      "The first 51000000 entries have been processed. 8549000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.885 minutes left.\n",
      "Going through chunk 51...\n",
      "The first 52000000 entries have been processed. 8548000000 left.\n",
      "1.228 secs per chunk on average. Meaning  174.967 minutes left.\n",
      "Going through chunk 52...\n",
      "The first 53000000 entries have been processed. 8547000000 left.\n",
      "1.228 secs per chunk on average. Meaning  174.922 minutes left.\n",
      "Going through chunk 53...\n",
      "The first 54000000 entries have been processed. 8546000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.789 minutes left.\n",
      "Going through chunk 54...\n",
      "The first 55000000 entries have been processed. 8545000000 left.\n",
      "1.228 secs per chunk on average. Meaning  174.839 minutes left.\n",
      "Going through chunk 55...\n",
      "The first 56000000 entries have been processed. 8544000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.743 minutes left.\n",
      "Going through chunk 56...\n",
      "The first 57000000 entries have been processed. 8543000000 left.\n",
      "1.228 secs per chunk on average. Meaning  174.828 minutes left.\n",
      "Going through chunk 57...\n",
      "The first 58000000 entries have been processed. 8542000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.692 minutes left.\n",
      "Going through chunk 58...\n",
      "The first 59000000 entries have been processed. 8541000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.615 minutes left.\n",
      "Going through chunk 59...\n",
      "The first 60000000 entries have been processed. 8540000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.640 minutes left.\n",
      "Going through chunk 60...\n",
      "The first 61000000 entries have been processed. 8539000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.644 minutes left.\n",
      "Going through chunk 61...\n",
      "The first 62000000 entries have been processed. 8538000000 left.\n",
      "1.229 secs per chunk on average. Meaning  174.928 minutes left.\n",
      "Going through chunk 62...\n",
      "The first 63000000 entries have been processed. 8537000000 left.\n",
      "1.229 secs per chunk on average. Meaning  174.899 minutes left.\n",
      "Going through chunk 63...\n",
      "The first 64000000 entries have been processed. 8536000000 left.\n",
      "1.228 secs per chunk on average. Meaning  174.699 minutes left.\n",
      "Going through chunk 64...\n",
      "The first 65000000 entries have been processed. 8535000000 left.\n",
      "1.228 secs per chunk on average. Meaning  174.630 minutes left.\n",
      "Going through chunk 65...\n",
      "The first 66000000 entries have been processed. 8534000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.479 minutes left.\n",
      "Going through chunk 66...\n",
      "The first 67000000 entries have been processed. 8533000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.427 minutes left.\n",
      "Going through chunk 67...\n",
      "The first 68000000 entries have been processed. 8532000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.291 minutes left.\n",
      "Going through chunk 68...\n",
      "The first 69000000 entries have been processed. 8531000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.250 minutes left.\n",
      "Going through chunk 69...\n",
      "The first 70000000 entries have been processed. 8530000000 left.\n",
      "1.225 secs per chunk on average. Meaning  174.199 minutes left.\n",
      "Going through chunk 70...\n",
      "The first 71000000 entries have been processed. 8529000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.216 minutes left.\n",
      "Going through chunk 71...\n",
      "The first 72000000 entries have been processed. 8528000000 left.\n",
      "1.225 secs per chunk on average. Meaning  174.163 minutes left.\n",
      "Going through chunk 72...\n",
      "The first 73000000 entries have been processed. 8527000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.252 minutes left.\n",
      "Going through chunk 73...\n",
      "The first 74000000 entries have been processed. 8526000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.247 minutes left.\n",
      "Going through chunk 74...\n",
      "The first 75000000 entries have been processed. 8525000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.140 minutes left.\n",
      "Going through chunk 75...\n",
      "The first 76000000 entries have been processed. 8524000000 left.\n",
      "1.225 secs per chunk on average. Meaning  174.018 minutes left.\n",
      "Going through chunk 76...\n",
      "The first 77000000 entries have been processed. 8523000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.108 minutes left.\n",
      "Going through chunk 77...\n",
      "The first 78000000 entries have been processed. 8522000000 left.\n",
      "1.225 secs per chunk on average. Meaning  174.018 minutes left.\n",
      "Going through chunk 78...\n",
      "The first 79000000 entries have been processed. 8521000000 left.\n",
      "1.225 secs per chunk on average. Meaning  173.993 minutes left.\n",
      "Going through chunk 79...\n",
      "The first 80000000 entries have been processed. 8520000000 left.\n",
      "1.225 secs per chunk on average. Meaning  173.972 minutes left.\n",
      "Going through chunk 80...\n",
      "The first 81000000 entries have been processed. 8519000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.109 minutes left.\n",
      "Going through chunk 81...\n",
      "The first 82000000 entries have been processed. 8518000000 left.\n",
      "1.226 secs per chunk on average. Meaning  174.019 minutes left.\n",
      "Going through chunk 82...\n",
      "The first 83000000 entries have been processed. 8517000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.117 minutes left.\n",
      "Going through chunk 83...\n",
      "The first 84000000 entries have been processed. 8516000000 left.\n",
      "1.228 secs per chunk on average. Meaning  174.245 minutes left.\n",
      "Going through chunk 84...\n",
      "The first 85000000 entries have been processed. 8515000000 left.\n",
      "1.227 secs per chunk on average. Meaning  174.137 minutes left.\n",
      "Going through chunk 85...\n",
      "The first 86000000 entries have been processed. 8514000000 left.\n",
      "1.226 secs per chunk on average. Meaning  173.911 minutes left.\n",
      "Going through chunk 86...\n",
      "The first 87000000 entries have been processed. 8513000000 left.\n",
      "1.225 secs per chunk on average. Meaning  173.783 minutes left.\n",
      "Going through chunk 87...\n",
      "The first 88000000 entries have been processed. 8512000000 left.\n",
      "1.226 secs per chunk on average. Meaning  173.875 minutes left.\n",
      "Going through chunk 88...\n",
      "The first 89000000 entries have been processed. 8511000000 left.\n",
      "1.225 secs per chunk on average. Meaning  173.793 minutes left.\n",
      "Going through chunk 89...\n",
      "The first 90000000 entries have been processed. 8510000000 left.\n",
      "1.224 secs per chunk on average. Meaning  173.641 minutes left.\n",
      "Going through chunk 90...\n",
      "The first 91000000 entries have been processed. 8509000000 left.\n",
      "1.224 secs per chunk on average. Meaning  173.596 minutes left.\n",
      "Going through chunk 91...\n",
      "The first 92000000 entries have been processed. 8508000000 left.\n",
      "1.224 secs per chunk on average. Meaning  173.534 minutes left.\n",
      "Going through chunk 92...\n",
      "The first 93000000 entries have been processed. 8507000000 left.\n",
      "1.224 secs per chunk on average. Meaning  173.521 minutes left.\n",
      "Going through chunk 93...\n",
      "The first 94000000 entries have been processed. 8506000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.410 minutes left.\n",
      "Going through chunk 94...\n",
      "The first 95000000 entries have been processed. 8505000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.384 minutes left.\n",
      "Going through chunk 95...\n",
      "The first 96000000 entries have been processed. 8504000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.401 minutes left.\n",
      "Going through chunk 96...\n",
      "The first 97000000 entries have been processed. 8503000000 left.\n",
      "1.224 secs per chunk on average. Meaning  173.466 minutes left.\n",
      "Going through chunk 97...\n",
      "The first 98000000 entries have been processed. 8502000000 left.\n",
      "1.224 secs per chunk on average. Meaning  173.386 minutes left.\n",
      "Going through chunk 98...\n",
      "The first 99000000 entries have been processed. 8501000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.336 minutes left.\n",
      "Going through chunk 99...\n",
      "The first 100000000 entries have been processed. 8500000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.263 minutes left.\n",
      "Going through chunk 100...\n",
      "The first 101000000 entries have been processed. 8499000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.283 minutes left.\n",
      "Going through chunk 101...\n",
      "The first 102000000 entries have been processed. 8498000000 left.\n",
      "1.222 secs per chunk on average. Meaning  173.143 minutes left.\n",
      "Going through chunk 102...\n",
      "The first 103000000 entries have been processed. 8497000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.164 minutes left.\n",
      "Going through chunk 103...\n",
      "The first 104000000 entries have been processed. 8496000000 left.\n",
      "1.223 secs per chunk on average. Meaning  173.134 minutes left.\n",
      "Going through chunk 104...\n",
      "The first 105000000 entries have been processed. 8495000000 left.\n",
      "1.222 secs per chunk on average. Meaning  173.084 minutes left.\n",
      "Going through chunk 105...\n",
      "The first 106000000 entries have been processed. 8494000000 left.\n",
      "1.222 secs per chunk on average. Meaning  173.006 minutes left.\n",
      "Going through chunk 106...\n",
      "The first 107000000 entries have been processed. 8493000000 left.\n",
      "1.222 secs per chunk on average. Meaning  172.952 minutes left.\n",
      "Going through chunk 107...\n",
      "The first 108000000 entries have been processed. 8492000000 left.\n",
      "1.222 secs per chunk on average. Meaning  172.891 minutes left.\n",
      "Going through chunk 108...\n",
      "The first 109000000 entries have been processed. 8491000000 left.\n",
      "1.221 secs per chunk on average. Meaning  172.860 minutes left.\n",
      "Going through chunk 109...\n",
      "The first 110000000 entries have been processed. 8490000000 left.\n",
      "1.221 secs per chunk on average. Meaning  172.806 minutes left.\n",
      "Going through chunk 110...\n",
      "The first 111000000 entries have been processed. 8489000000 left.\n",
      "1.221 secs per chunk on average. Meaning  172.821 minutes left.\n",
      "Going through chunk 111...\n",
      "The first 112000000 entries have been processed. 8488000000 left.\n",
      "1.221 secs per chunk on average. Meaning  172.714 minutes left.\n",
      "Going through chunk 112...\n",
      "The first 113000000 entries have been processed. 8487000000 left.\n",
      "1.220 secs per chunk on average. Meaning  172.615 minutes left.\n",
      "Going through chunk 113...\n",
      "The first 114000000 entries have been processed. 8486000000 left.\n",
      "1.220 secs per chunk on average. Meaning  172.524 minutes left.\n",
      "Going through chunk 114...\n",
      "The first 115000000 entries have been processed. 8485000000 left.\n",
      "1.220 secs per chunk on average. Meaning  172.482 minutes left.\n",
      "Going through chunk 115...\n",
      "The first 116000000 entries have been processed. 8484000000 left.\n",
      "1.219 secs per chunk on average. Meaning  172.429 minutes left.\n",
      "Going through chunk 116...\n",
      "The first 117000000 entries have been processed. 8483000000 left.\n",
      "1.220 secs per chunk on average. Meaning  172.493 minutes left.\n",
      "Going through chunk 117...\n",
      "The first 118000000 entries have been processed. 8482000000 left.\n",
      "1.219 secs per chunk on average. Meaning  172.391 minutes left.\n",
      "Going through chunk 118...\n",
      "The first 119000000 entries have been processed. 8481000000 left.\n",
      "1.219 secs per chunk on average. Meaning  172.354 minutes left.\n",
      "Going through chunk 119...\n",
      "The first 120000000 entries have been processed. 8480000000 left.\n",
      "1.219 secs per chunk on average. Meaning  172.268 minutes left.\n",
      "Going through chunk 120...\n",
      "The first 121000000 entries have been processed. 8479000000 left.\n",
      "1.219 secs per chunk on average. Meaning  172.227 minutes left.\n",
      "Going through chunk 121...\n",
      "The first 122000000 entries have been processed. 8478000000 left.\n",
      "1.218 secs per chunk on average. Meaning  172.090 minutes left.\n",
      "Going through chunk 122...\n",
      "The first 123000000 entries have been processed. 8477000000 left.\n",
      "1.218 secs per chunk on average. Meaning  172.015 minutes left.\n",
      "Going through chunk 123...\n",
      "The first 124000000 entries have been processed. 8476000000 left.\n",
      "1.217 secs per chunk on average. Meaning  171.986 minutes left.\n",
      "Going through chunk 124...\n",
      "The first 125000000 entries have been processed. 8475000000 left.\n",
      "1.217 secs per chunk on average. Meaning  171.908 minutes left.\n",
      "Going through chunk 125...\n",
      "The first 126000000 entries have been processed. 8474000000 left.\n",
      "1.217 secs per chunk on average. Meaning  171.860 minutes left.\n",
      "Going through chunk 126...\n",
      "The first 127000000 entries have been processed. 8473000000 left.\n",
      "1.217 secs per chunk on average. Meaning  171.821 minutes left.\n",
      "Going through chunk 127...\n",
      "The first 128000000 entries have been processed. 8472000000 left.\n",
      "1.216 secs per chunk on average. Meaning  171.758 minutes left.\n",
      "Going through chunk 128...\n",
      "The first 129000000 entries have been processed. 8471000000 left.\n",
      "1.217 secs per chunk on average. Meaning  171.751 minutes left.\n",
      "Going through chunk 129...\n",
      "The first 130000000 entries have been processed. 8470000000 left.\n",
      "1.216 secs per chunk on average. Meaning  171.687 minutes left.\n",
      "Going through chunk 130...\n",
      "The first 131000000 entries have been processed. 8469000000 left.\n",
      "1.216 secs per chunk on average. Meaning  171.637 minutes left.\n",
      "Going through chunk 131...\n",
      "The first 132000000 entries have been processed. 8468000000 left.\n",
      "1.216 secs per chunk on average. Meaning  171.665 minutes left.\n",
      "Going through chunk 132...\n",
      "The first 133000000 entries have been processed. 8467000000 left.\n",
      "1.216 secs per chunk on average. Meaning  171.606 minutes left.\n",
      "Going through chunk 133...\n",
      "The first 134000000 entries have been processed. 8466000000 left.\n",
      "1.216 secs per chunk on average. Meaning  171.526 minutes left.\n",
      "Going through chunk 134...\n",
      "The first 135000000 entries have been processed. 8465000000 left.\n",
      "1.215 secs per chunk on average. Meaning  171.471 minutes left.\n",
      "Going through chunk 135...\n",
      "The first 136000000 entries have been processed. 8464000000 left.\n",
      "1.215 secs per chunk on average. Meaning  171.379 minutes left.\n",
      "Going through chunk 136...\n",
      "The first 137000000 entries have been processed. 8463000000 left.\n",
      "1.214 secs per chunk on average. Meaning  171.298 minutes left.\n",
      "Going through chunk 137...\n",
      "The first 138000000 entries have been processed. 8462000000 left.\n",
      "1.214 secs per chunk on average. Meaning  171.216 minutes left.\n",
      "Going through chunk 138...\n",
      "The first 139000000 entries have been processed. 8461000000 left.\n",
      "1.214 secs per chunk on average. Meaning  171.207 minutes left.\n",
      "Going through chunk 139...\n",
      "The first 140000000 entries have been processed. 8460000000 left.\n",
      "1.214 secs per chunk on average. Meaning  171.119 minutes left.\n",
      "Going through chunk 140...\n",
      "The first 141000000 entries have been processed. 8459000000 left.\n",
      "1.214 secs per chunk on average. Meaning  171.109 minutes left.\n",
      "Going through chunk 141...\n",
      "The first 142000000 entries have been processed. 8458000000 left.\n",
      "1.213 secs per chunk on average. Meaning  171.059 minutes left.\n",
      "Going through chunk 142...\n",
      "The first 143000000 entries have been processed. 8457000000 left.\n",
      "1.213 secs per chunk on average. Meaning  171.010 minutes left.\n",
      "Going through chunk 143...\n",
      "The first 144000000 entries have been processed. 8456000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.985 minutes left.\n",
      "Going through chunk 144...\n",
      "The first 145000000 entries have been processed. 8455000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.952 minutes left.\n",
      "Going through chunk 145...\n",
      "The first 146000000 entries have been processed. 8454000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.922 minutes left.\n",
      "Going through chunk 146...\n",
      "The first 147000000 entries have been processed. 8453000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.881 minutes left.\n",
      "Going through chunk 147...\n",
      "The first 148000000 entries have been processed. 8452000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.831 minutes left.\n",
      "Going through chunk 148...\n",
      "The first 149000000 entries have been processed. 8451000000 left.\n",
      "1.212 secs per chunk on average. Meaning  170.771 minutes left.\n",
      "Going through chunk 149...\n",
      "The first 150000000 entries have been processed. 8450000000 left.\n",
      "1.212 secs per chunk on average. Meaning  170.721 minutes left.\n",
      "Going through chunk 150...\n",
      "The first 151000000 entries have been processed. 8449000000 left.\n",
      "1.212 secs per chunk on average. Meaning  170.711 minutes left.\n",
      "Going through chunk 151...\n",
      "The first 152000000 entries have been processed. 8448000000 left.\n",
      "1.212 secs per chunk on average. Meaning  170.652 minutes left.\n",
      "Going through chunk 152...\n",
      "The first 153000000 entries have been processed. 8447000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.739 minutes left.\n",
      "Going through chunk 153...\n",
      "The first 154000000 entries have been processed. 8446000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.702 minutes left.\n",
      "Going through chunk 154...\n",
      "The first 155000000 entries have been processed. 8445000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.744 minutes left.\n",
      "Going through chunk 155...\n",
      "The first 156000000 entries have been processed. 8444000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.705 minutes left.\n",
      "Going through chunk 156...\n",
      "The first 157000000 entries have been processed. 8443000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.685 minutes left.\n",
      "Going through chunk 157...\n",
      "The first 158000000 entries have been processed. 8442000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.672 minutes left.\n",
      "Going through chunk 158...\n",
      "The first 159000000 entries have been processed. 8441000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.655 minutes left.\n",
      "Going through chunk 159...\n",
      "The first 160000000 entries have been processed. 8440000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.646 minutes left.\n",
      "Going through chunk 160...\n",
      "The first 161000000 entries have been processed. 8439000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.638 minutes left.\n",
      "Going through chunk 161...\n",
      "The first 162000000 entries have been processed. 8438000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.639 minutes left.\n",
      "Going through chunk 162...\n",
      "The first 163000000 entries have been processed. 8437000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.719 minutes left.\n",
      "Going through chunk 163...\n",
      "The first 164000000 entries have been processed. 8436000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.670 minutes left.\n",
      "Going through chunk 164...\n",
      "The first 165000000 entries have been processed. 8435000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.630 minutes left.\n",
      "Going through chunk 165...\n",
      "The first 166000000 entries have been processed. 8434000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.589 minutes left.\n",
      "Going through chunk 166...\n",
      "The first 167000000 entries have been processed. 8433000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.539 minutes left.\n",
      "Going through chunk 167...\n",
      "The first 168000000 entries have been processed. 8432000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.551 minutes left.\n",
      "Going through chunk 168...\n",
      "The first 169000000 entries have been processed. 8431000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.493 minutes left.\n",
      "Going through chunk 169...\n",
      "The first 170000000 entries have been processed. 8430000000 left.\n",
      "1.213 secs per chunk on average. Meaning  170.444 minutes left.\n",
      "Going through chunk 170...\n",
      "The first 171000000 entries have been processed. 8429000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.492 minutes left.\n",
      "Going through chunk 171...\n",
      "The first 172000000 entries have been processed. 8428000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.469 minutes left.\n",
      "Going through chunk 172...\n",
      "The first 173000000 entries have been processed. 8427000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.488 minutes left.\n",
      "Going through chunk 173...\n",
      "The first 174000000 entries have been processed. 8426000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.463 minutes left.\n",
      "Going through chunk 174...\n",
      "The first 175000000 entries have been processed. 8425000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.453 minutes left.\n",
      "Going through chunk 175...\n",
      "The first 176000000 entries have been processed. 8424000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.406 minutes left.\n",
      "Going through chunk 176...\n",
      "The first 177000000 entries have been processed. 8423000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.415 minutes left.\n",
      "Going through chunk 177...\n",
      "The first 178000000 entries have been processed. 8422000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.372 minutes left.\n",
      "Going through chunk 178...\n",
      "The first 179000000 entries have been processed. 8421000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.353 minutes left.\n",
      "Going through chunk 179...\n",
      "The first 180000000 entries have been processed. 8420000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.341 minutes left.\n",
      "Going through chunk 180...\n",
      "The first 181000000 entries have been processed. 8419000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.336 minutes left.\n",
      "Going through chunk 181...\n",
      "The first 182000000 entries have been processed. 8418000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.331 minutes left.\n",
      "Going through chunk 182...\n",
      "The first 183000000 entries have been processed. 8417000000 left.\n",
      "1.214 secs per chunk on average. Meaning  170.341 minutes left.\n",
      "Going through chunk 183...\n",
      "The first 184000000 entries have been processed. 8416000000 left.\n",
      "1.215 secs per chunk on average. Meaning  170.405 minutes left.\n",
      "Going through chunk 184...\n",
      "The first 185000000 entries have been processed. 8415000000 left.\n",
      "1.215 secs per chunk on average. Meaning  170.367 minutes left.\n",
      "Going through chunk 185...\n",
      "The first 186000000 entries have been processed. 8414000000 left.\n",
      "1.216 secs per chunk on average. Meaning  170.459 minutes left.\n",
      "Going through chunk 186...\n",
      "The first 187000000 entries have been processed. 8413000000 left.\n",
      "1.216 secs per chunk on average. Meaning  170.483 minutes left.\n",
      "Going through chunk 187...\n",
      "The first 188000000 entries have been processed. 8412000000 left.\n",
      "1.216 secs per chunk on average. Meaning  170.474 minutes left.\n",
      "Going through chunk 188...\n",
      "The first 189000000 entries have been processed. 8411000000 left.\n",
      "1.217 secs per chunk on average. Meaning  170.541 minutes left.\n",
      "Going through chunk 189...\n",
      "The first 190000000 entries have been processed. 8410000000 left.\n",
      "1.217 secs per chunk on average. Meaning  170.590 minutes left.\n",
      "Going through chunk 190...\n",
      "The first 191000000 entries have been processed. 8409000000 left.\n",
      "1.217 secs per chunk on average. Meaning  170.607 minutes left.\n",
      "Going through chunk 191...\n",
      "The first 192000000 entries have been processed. 8408000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.620 minutes left.\n",
      "Going through chunk 192...\n",
      "The first 193000000 entries have been processed. 8407000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.647 minutes left.\n",
      "Going through chunk 193...\n",
      "The first 194000000 entries have been processed. 8406000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.621 minutes left.\n",
      "Going through chunk 194...\n",
      "The first 195000000 entries have been processed. 8405000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.626 minutes left.\n",
      "Going through chunk 195...\n",
      "The first 196000000 entries have been processed. 8404000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.599 minutes left.\n",
      "Going through chunk 196...\n",
      "The first 197000000 entries have been processed. 8403000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.566 minutes left.\n",
      "Going through chunk 197...\n",
      "The first 198000000 entries have been processed. 8402000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.626 minutes left.\n",
      "Going through chunk 198...\n",
      "The first 199000000 entries have been processed. 8401000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.616 minutes left.\n",
      "Going through chunk 199...\n",
      "The first 200000000 entries have been processed. 8400000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.563 minutes left.\n",
      "Going through chunk 200...\n",
      "The first 201000000 entries have been processed. 8399000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.532 minutes left.\n",
      "Going through chunk 201...\n",
      "The first 202000000 entries have been processed. 8398000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.481 minutes left.\n",
      "Going through chunk 202...\n",
      "The first 203000000 entries have been processed. 8397000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.475 minutes left.\n",
      "Going through chunk 203...\n",
      "The first 204000000 entries have been processed. 8396000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.430 minutes left.\n",
      "Going through chunk 204...\n",
      "The first 205000000 entries have been processed. 8395000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.439 minutes left.\n",
      "Going through chunk 205...\n",
      "The first 206000000 entries have been processed. 8394000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.450 minutes left.\n",
      "Going through chunk 206...\n",
      "The first 207000000 entries have been processed. 8393000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.416 minutes left.\n",
      "Going through chunk 207...\n",
      "The first 208000000 entries have been processed. 8392000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.390 minutes left.\n",
      "Going through chunk 208...\n",
      "The first 209000000 entries have been processed. 8391000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.443 minutes left.\n",
      "Going through chunk 209...\n",
      "The first 210000000 entries have been processed. 8390000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.438 minutes left.\n",
      "Going through chunk 210...\n",
      "The first 211000000 entries have been processed. 8389000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.413 minutes left.\n",
      "Going through chunk 211...\n",
      "The first 212000000 entries have been processed. 8388000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.382 minutes left.\n",
      "Going through chunk 212...\n",
      "The first 213000000 entries have been processed. 8387000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.375 minutes left.\n",
      "Going through chunk 213...\n",
      "The first 214000000 entries have been processed. 8386000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.329 minutes left.\n",
      "Going through chunk 214...\n",
      "The first 215000000 entries have been processed. 8385000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.291 minutes left.\n",
      "Going through chunk 215...\n",
      "The first 216000000 entries have been processed. 8384000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.256 minutes left.\n",
      "Going through chunk 216...\n",
      "The first 217000000 entries have been processed. 8383000000 left.\n",
      "1.218 secs per chunk on average. Meaning  170.235 minutes left.\n",
      "Going through chunk 217...\n",
      "The first 218000000 entries have been processed. 8382000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.251 minutes left.\n",
      "Going through chunk 218...\n",
      "The first 219000000 entries have been processed. 8381000000 left.\n",
      "1.219 secs per chunk on average. Meaning  170.334 minutes left.\n",
      "Going through chunk 219...\n",
      "The first 220000000 entries have been processed. 8380000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.368 minutes left.\n",
      "Going through chunk 220...\n",
      "The first 221000000 entries have been processed. 8379000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.414 minutes left.\n",
      "Going through chunk 221...\n",
      "The first 222000000 entries have been processed. 8378000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.460 minutes left.\n",
      "Going through chunk 222...\n",
      "The first 223000000 entries have been processed. 8377000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.399 minutes left.\n",
      "Going through chunk 223...\n",
      "The first 224000000 entries have been processed. 8376000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.331 minutes left.\n",
      "Going through chunk 224...\n",
      "The first 225000000 entries have been processed. 8375000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.313 minutes left.\n",
      "Going through chunk 225...\n",
      "The first 226000000 entries have been processed. 8374000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.257 minutes left.\n",
      "Going through chunk 226...\n",
      "The first 227000000 entries have been processed. 8373000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.271 minutes left.\n",
      "Going through chunk 227...\n",
      "The first 228000000 entries have been processed. 8372000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.252 minutes left.\n",
      "Going through chunk 228...\n",
      "The first 229000000 entries have been processed. 8371000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.240 minutes left.\n",
      "Going through chunk 229...\n",
      "The first 230000000 entries have been processed. 8370000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.214 minutes left.\n",
      "Going through chunk 230...\n",
      "The first 231000000 entries have been processed. 8369000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.190 minutes left.\n",
      "Going through chunk 231...\n",
      "The first 232000000 entries have been processed. 8368000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.154 minutes left.\n",
      "Going through chunk 232...\n",
      "The first 233000000 entries have been processed. 8367000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.155 minutes left.\n",
      "Going through chunk 233...\n",
      "The first 234000000 entries have been processed. 8366000000 left.\n",
      "1.220 secs per chunk on average. Meaning  170.166 minutes left.\n",
      "Going through chunk 234...\n",
      "The first 235000000 entries have been processed. 8365000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.223 minutes left.\n",
      "Going through chunk 235...\n",
      "The first 236000000 entries have been processed. 8364000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.272 minutes left.\n",
      "Going through chunk 236...\n",
      "The first 237000000 entries have been processed. 8363000000 left.\n",
      "1.222 secs per chunk on average. Meaning  170.283 minutes left.\n",
      "Going through chunk 237...\n",
      "The first 238000000 entries have been processed. 8362000000 left.\n",
      "1.222 secs per chunk on average. Meaning  170.285 minutes left.\n",
      "Going through chunk 238...\n",
      "The first 239000000 entries have been processed. 8361000000 left.\n",
      "1.222 secs per chunk on average. Meaning  170.262 minutes left.\n",
      "Going through chunk 239...\n",
      "The first 240000000 entries have been processed. 8360000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.190 minutes left.\n",
      "Going through chunk 240...\n",
      "The first 241000000 entries have been processed. 8359000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.118 minutes left.\n",
      "Going through chunk 241...\n",
      "The first 242000000 entries have been processed. 8358000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.046 minutes left.\n",
      "Going through chunk 242...\n",
      "The first 243000000 entries have been processed. 8357000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.996 minutes left.\n",
      "Going through chunk 243...\n",
      "The first 244000000 entries have been processed. 8356000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.985 minutes left.\n",
      "Going through chunk 244...\n",
      "The first 245000000 entries have been processed. 8355000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.021 minutes left.\n",
      "Going through chunk 245...\n",
      "The first 246000000 entries have been processed. 8354000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.037 minutes left.\n",
      "Going through chunk 246...\n",
      "The first 247000000 entries have been processed. 8353000000 left.\n",
      "1.221 secs per chunk on average. Meaning  170.023 minutes left.\n",
      "Going through chunk 247...\n",
      "The first 248000000 entries have been processed. 8352000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.950 minutes left.\n",
      "Going through chunk 248...\n",
      "The first 249000000 entries have been processed. 8351000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.894 minutes left.\n",
      "Going through chunk 249...\n",
      "The first 250000000 entries have been processed. 8350000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.853 minutes left.\n",
      "Going through chunk 250...\n",
      "The first 251000000 entries have been processed. 8349000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.863 minutes left.\n",
      "Going through chunk 251...\n",
      "The first 252000000 entries have been processed. 8348000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.833 minutes left.\n",
      "Going through chunk 252...\n",
      "The first 253000000 entries have been processed. 8347000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.823 minutes left.\n",
      "Going through chunk 253...\n",
      "The first 254000000 entries have been processed. 8346000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.763 minutes left.\n",
      "Going through chunk 254...\n",
      "The first 255000000 entries have been processed. 8345000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.689 minutes left.\n",
      "Going through chunk 255...\n",
      "The first 256000000 entries have been processed. 8344000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.613 minutes left.\n",
      "Going through chunk 256...\n",
      "The first 257000000 entries have been processed. 8343000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.576 minutes left.\n",
      "Going through chunk 257...\n",
      "The first 258000000 entries have been processed. 8342000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.513 minutes left.\n",
      "Going through chunk 258...\n",
      "The first 259000000 entries have been processed. 8341000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.468 minutes left.\n",
      "Going through chunk 259...\n",
      "The first 260000000 entries have been processed. 8340000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.476 minutes left.\n",
      "Going through chunk 260...\n",
      "The first 261000000 entries have been processed. 8339000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.430 minutes left.\n",
      "Going through chunk 261...\n",
      "The first 262000000 entries have been processed. 8338000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.360 minutes left.\n",
      "Going through chunk 262...\n",
      "The first 263000000 entries have been processed. 8337000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.315 minutes left.\n",
      "Going through chunk 263...\n",
      "The first 264000000 entries have been processed. 8336000000 left.\n",
      "1.218 secs per chunk on average. Meaning  169.242 minutes left.\n",
      "Going through chunk 264...\n",
      "The first 265000000 entries have been processed. 8335000000 left.\n",
      "1.218 secs per chunk on average. Meaning  169.179 minutes left.\n",
      "Going through chunk 265...\n",
      "The first 266000000 entries have been processed. 8334000000 left.\n",
      "1.218 secs per chunk on average. Meaning  169.141 minutes left.\n",
      "Going through chunk 266...\n",
      "The first 267000000 entries have been processed. 8333000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.328 minutes left.\n",
      "Going through chunk 267...\n",
      "The first 268000000 entries have been processed. 8332000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.307 minutes left.\n",
      "Going through chunk 268...\n",
      "The first 269000000 entries have been processed. 8331000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.309 minutes left.\n",
      "Going through chunk 269...\n",
      "The first 270000000 entries have been processed. 8330000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.283 minutes left.\n",
      "Going through chunk 270...\n",
      "The first 271000000 entries have been processed. 8329000000 left.\n",
      "1.219 secs per chunk on average. Meaning  169.246 minutes left.\n",
      "Going through chunk 271...\n",
      "The first 272000000 entries have been processed. 8328000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.373 minutes left.\n",
      "Going through chunk 272...\n",
      "The first 273000000 entries have been processed. 8327000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.360 minutes left.\n",
      "Going through chunk 273...\n",
      "The first 274000000 entries have been processed. 8326000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.359 minutes left.\n",
      "Going through chunk 274...\n",
      "The first 275000000 entries have been processed. 8325000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.356 minutes left.\n",
      "Going through chunk 275...\n",
      "The first 276000000 entries have been processed. 8324000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.318 minutes left.\n",
      "Going through chunk 276...\n",
      "The first 277000000 entries have been processed. 8323000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.300 minutes left.\n",
      "Going through chunk 277...\n",
      "The first 278000000 entries have been processed. 8322000000 left.\n",
      "1.220 secs per chunk on average. Meaning  169.280 minutes left.\n",
      "Going through chunk 278...\n",
      "The first 279000000 entries have been processed. 8321000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.269 minutes left.\n",
      "Going through chunk 279...\n",
      "The first 280000000 entries have been processed. 8320000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.275 minutes left.\n",
      "Going through chunk 280...\n",
      "The first 281000000 entries have been processed. 8319000000 left.\n",
      "1.221 secs per chunk on average. Meaning  169.330 minutes left.\n",
      "Going through chunk 281...\n",
      "The first 282000000 entries have been processed. 8318000000 left.\n",
      "1.223 secs per chunk on average. Meaning  169.530 minutes left.\n",
      "Going through chunk 282...\n",
      "The first 283000000 entries have been processed. 8317000000 left.\n",
      "1.223 secs per chunk on average. Meaning  169.594 minutes left.\n",
      "Going through chunk 283...\n",
      "The first 284000000 entries have been processed. 8316000000 left.\n",
      "1.223 secs per chunk on average. Meaning  169.565 minutes left.\n",
      "Going through chunk 284...\n",
      "The first 285000000 entries have been processed. 8315000000 left.\n",
      "1.223 secs per chunk on average. Meaning  169.555 minutes left.\n",
      "Going through chunk 285...\n",
      "The first 286000000 entries have been processed. 8314000000 left.\n",
      "1.224 secs per chunk on average. Meaning  169.540 minutes left.\n",
      "Going through chunk 286...\n",
      "The first 287000000 entries have been processed. 8313000000 left.\n",
      "1.224 secs per chunk on average. Meaning  169.554 minutes left.\n",
      "Going through chunk 287...\n",
      "The first 288000000 entries have been processed. 8312000000 left.\n",
      "1.224 secs per chunk on average. Meaning  169.520 minutes left.\n",
      "Going through chunk 288...\n",
      "The first 289000000 entries have been processed. 8311000000 left.\n",
      "1.224 secs per chunk on average. Meaning  169.516 minutes left.\n",
      "Going through chunk 289...\n",
      "The first 290000000 entries have been processed. 8310000000 left.\n",
      "1.224 secs per chunk on average. Meaning  169.489 minutes left.\n",
      "Going through chunk 290...\n",
      "The first 291000000 entries have been processed. 8309000000 left.\n",
      "1.224 secs per chunk on average. Meaning  169.489 minutes left.\n",
      "Going through chunk 291...\n",
      "The first 292000000 entries have been processed. 8308000000 left.\n",
      "1.225 secs per chunk on average. Meaning  169.560 minutes left.\n",
      "Going through chunk 292...\n",
      "The first 293000000 entries have been processed. 8307000000 left.\n",
      "1.225 secs per chunk on average. Meaning  169.568 minutes left.\n",
      "Going through chunk 293...\n",
      "The first 294000000 entries have been processed. 8306000000 left.\n",
      "1.225 secs per chunk on average. Meaning  169.564 minutes left.\n",
      "Going through chunk 294...\n",
      "The first 295000000 entries have been processed. 8305000000 left.\n",
      "1.225 secs per chunk on average. Meaning  169.572 minutes left.\n",
      "Going through chunk 295...\n",
      "The first 296000000 entries have been processed. 8304000000 left.\n",
      "1.225 secs per chunk on average. Meaning  169.571 minutes left.\n",
      "Going through chunk 296...\n",
      "The first 297000000 entries have been processed. 8303000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.598 minutes left.\n",
      "Going through chunk 297...\n",
      "The first 298000000 entries have been processed. 8302000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.617 minutes left.\n",
      "Going through chunk 298...\n",
      "The first 299000000 entries have been processed. 8301000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.634 minutes left.\n",
      "Going through chunk 299...\n",
      "The first 300000000 entries have been processed. 8300000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.641 minutes left.\n",
      "Going through chunk 300...\n",
      "The first 301000000 entries have been processed. 8299000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.643 minutes left.\n",
      "Going through chunk 301...\n",
      "The first 302000000 entries have been processed. 8298000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.610 minutes left.\n",
      "Going through chunk 302...\n",
      "The first 303000000 entries have been processed. 8297000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.647 minutes left.\n",
      "Going through chunk 303...\n",
      "The first 304000000 entries have been processed. 8296000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.609 minutes left.\n",
      "Going through chunk 304...\n",
      "The first 305000000 entries have been processed. 8295000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.583 minutes left.\n",
      "Going through chunk 305...\n",
      "The first 306000000 entries have been processed. 8294000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.567 minutes left.\n",
      "Going through chunk 306...\n",
      "The first 307000000 entries have been processed. 8293000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.562 minutes left.\n",
      "Going through chunk 307...\n",
      "The first 308000000 entries have been processed. 8292000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.538 minutes left.\n",
      "Going through chunk 308...\n",
      "The first 309000000 entries have been processed. 8291000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.513 minutes left.\n",
      "Going through chunk 309...\n",
      "The first 310000000 entries have been processed. 8290000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.460 minutes left.\n",
      "Going through chunk 310...\n",
      "The first 311000000 entries have been processed. 8289000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.458 minutes left.\n",
      "Going through chunk 311...\n",
      "The first 312000000 entries have been processed. 8288000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.416 minutes left.\n",
      "Going through chunk 312...\n",
      "The first 313000000 entries have been processed. 8287000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.376 minutes left.\n",
      "Going through chunk 313...\n",
      "The first 314000000 entries have been processed. 8286000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.381 minutes left.\n",
      "Going through chunk 314...\n",
      "The first 315000000 entries have been processed. 8285000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.348 minutes left.\n",
      "Going through chunk 315...\n",
      "The first 316000000 entries have been processed. 8284000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.315 minutes left.\n",
      "Going through chunk 316...\n",
      "The first 317000000 entries have been processed. 8283000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.285 minutes left.\n",
      "Going through chunk 317...\n",
      "The first 318000000 entries have been processed. 8282000000 left.\n",
      "1.226 secs per chunk on average. Meaning  169.297 minutes left.\n",
      "Going through chunk 318...\n",
      "The first 319000000 entries have been processed. 8281000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.332 minutes left.\n",
      "Going through chunk 319...\n",
      "The first 320000000 entries have been processed. 8280000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.307 minutes left.\n",
      "Going through chunk 320...\n",
      "The first 321000000 entries have been processed. 8279000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.292 minutes left.\n",
      "Going through chunk 321...\n",
      "The first 322000000 entries have been processed. 8278000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.257 minutes left.\n",
      "Going through chunk 322...\n",
      "The first 323000000 entries have been processed. 8277000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.230 minutes left.\n",
      "Going through chunk 323...\n",
      "The first 324000000 entries have been processed. 8276000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.213 minutes left.\n",
      "Going through chunk 324...\n",
      "The first 325000000 entries have been processed. 8275000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.254 minutes left.\n",
      "Going through chunk 325...\n",
      "The first 326000000 entries have been processed. 8274000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.227 minutes left.\n",
      "Going through chunk 326...\n",
      "The first 327000000 entries have been processed. 8273000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.198 minutes left.\n",
      "Going through chunk 327...\n",
      "The first 328000000 entries have been processed. 8272000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.169 minutes left.\n",
      "Going through chunk 328...\n",
      "The first 329000000 entries have been processed. 8271000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.176 minutes left.\n",
      "Going through chunk 329...\n",
      "The first 330000000 entries have been processed. 8270000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.142 minutes left.\n",
      "Going through chunk 330...\n",
      "The first 331000000 entries have been processed. 8269000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.129 minutes left.\n",
      "Going through chunk 331...\n",
      "The first 332000000 entries have been processed. 8268000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.097 minutes left.\n",
      "Going through chunk 332...\n",
      "The first 333000000 entries have been processed. 8267000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.074 minutes left.\n",
      "Going through chunk 333...\n",
      "The first 334000000 entries have been processed. 8266000000 left.\n",
      "1.227 secs per chunk on average. Meaning  169.080 minutes left.\n",
      "Going through chunk 334...\n",
      "The first 335000000 entries have been processed. 8265000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.145 minutes left.\n",
      "Going through chunk 335...\n",
      "The first 336000000 entries have been processed. 8264000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.128 minutes left.\n",
      "Going through chunk 336...\n",
      "The first 337000000 entries have been processed. 8263000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.104 minutes left.\n",
      "Going through chunk 337...\n",
      "The first 338000000 entries have been processed. 8262000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.096 minutes left.\n",
      "Going through chunk 338...\n",
      "The first 339000000 entries have been processed. 8261000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.093 minutes left.\n",
      "Going through chunk 339...\n",
      "The first 340000000 entries have been processed. 8260000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.066 minutes left.\n",
      "Going through chunk 340...\n",
      "The first 341000000 entries have been processed. 8259000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.086 minutes left.\n",
      "Going through chunk 341...\n",
      "The first 342000000 entries have been processed. 8258000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.049 minutes left.\n",
      "Going through chunk 342...\n",
      "The first 343000000 entries have been processed. 8257000000 left.\n",
      "1.228 secs per chunk on average. Meaning  169.033 minutes left.\n",
      "Going through chunk 343...\n",
      "The first 344000000 entries have been processed. 8256000000 left.\n",
      "1.228 secs per chunk on average. Meaning  168.990 minutes left.\n",
      "Going through chunk 344...\n",
      "The first 345000000 entries have been processed. 8255000000 left.\n",
      "1.228 secs per chunk on average. Meaning  168.974 minutes left.\n",
      "Going through chunk 345...\n",
      "The first 346000000 entries have been processed. 8254000000 left.\n",
      "1.228 secs per chunk on average. Meaning  168.941 minutes left.\n",
      "Going through chunk 346...\n",
      "The first 347000000 entries have been processed. 8253000000 left.\n",
      "1.228 secs per chunk on average. Meaning  168.923 minutes left.\n",
      "Going through chunk 347...\n",
      "The first 348000000 entries have been processed. 8252000000 left.\n",
      "1.228 secs per chunk on average. Meaning  168.873 minutes left.\n",
      "Going through chunk 348...\n",
      "The first 349000000 entries have been processed. 8251000000 left.\n",
      "1.228 secs per chunk on average. Meaning  168.923 minutes left.\n",
      "Going through chunk 349...\n",
      "The first 350000000 entries have been processed. 8250000000 left.\n",
      "1.229 secs per chunk on average. Meaning  169.008 minutes left.\n",
      "Going through chunk 350...\n",
      "The first 351000000 entries have been processed. 8249000000 left.\n",
      "1.229 secs per chunk on average. Meaning  168.993 minutes left.\n",
      "Going through chunk 351...\n",
      "The first 352000000 entries have been processed. 8248000000 left.\n",
      "1.230 secs per chunk on average. Meaning  169.032 minutes left.\n",
      "Going through chunk 352...\n",
      "The first 353000000 entries have been processed. 8247000000 left.\n",
      "1.230 secs per chunk on average. Meaning  169.050 minutes left.\n",
      "Going through chunk 353...\n",
      "The first 354000000 entries have been processed. 8246000000 left.\n",
      "1.231 secs per chunk on average. Meaning  169.113 minutes left.\n",
      "Going through chunk 354...\n",
      "The first 355000000 entries have been processed. 8245000000 left.\n",
      "1.231 secs per chunk on average. Meaning  169.119 minutes left.\n",
      "Going through chunk 355...\n",
      "The first 356000000 entries have been processed. 8244000000 left.\n",
      "1.231 secs per chunk on average. Meaning  169.099 minutes left.\n",
      "Going through chunk 356...\n",
      "The first 357000000 entries have been processed. 8243000000 left.\n",
      "1.231 secs per chunk on average. Meaning  169.057 minutes left.\n",
      "Going through chunk 357...\n",
      "The first 358000000 entries have been processed. 8242000000 left.\n",
      "1.231 secs per chunk on average. Meaning  169.041 minutes left.\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get the entries of the comment dataframe which have a na value in any column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nans \u001b[38;5;241m=\u001b[39m \u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_simple_function_on_chunks_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_na_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43many\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mprint_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8600000000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Nextcloud/Dokumente/Uni/Module/3sem-EPFL/ada/Project/ada-2024-project-thedataminions/preprocessing_tests/data_processing.py:37\u001b[0m, in \u001b[0;36mrun_simple_function_on_chunks_concat\u001b[0;34m(reader, fct, print_time)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     time_start_global \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGoing through chunk \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# get the entries of the comment dataframe which have a na value in any column\n",
    "nans = dp.run_simple_function_on_chunks_concat(comments_in_chunks(), \n",
    "                                        lambda x: dp.get_na_entries(x, \"any\", False),\n",
    "                                        print_time=(1000000, 8600000000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>video_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Gkb1QMHrGvA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CNtp0xqoods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>249EEzQmVmQ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>_U443T2K_Bs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rJbjhm0weYc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>664459</td>\n",
       "      <td>GC3gqIbrK7c</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>664459</td>\n",
       "      <td>GC3gqIbrK7c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>664459</td>\n",
       "      <td>i9VRGaoFw8k</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>664459</td>\n",
       "      <td>-JLWZ1jz3FY</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>664459</td>\n",
       "      <td>-JLWZ1jz3FY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author     video_id  likes  replies\n",
       "0             1  Gkb1QMHrGvA      2        0\n",
       "1             1  CNtp0xqoods      0        0\n",
       "2             1  249EEzQmVmQ      1        0\n",
       "3             1  _U443T2K_Bs      0        0\n",
       "4             1  rJbjhm0weYc      0        0\n",
       "...         ...          ...    ...      ...\n",
       "9999995  664459  GC3gqIbrK7c      9        1\n",
       "9999996  664459  GC3gqIbrK7c      1        0\n",
       "9999997  664459  i9VRGaoFw8k      1        1\n",
       "9999998  664459  -JLWZ1jz3FY      0        2\n",
       "9999999  664459  -JLWZ1jz3FY      0        0\n",
       "\n",
       "[10000000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 1000000 entries have been processed. 8599000000 left.\n",
      "0.630 secs per chunk on average. Meaning  90.327 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 2000000 entries have been processed. 8598000000 left.\n",
      "0.647 secs per chunk on average. Meaning  92.675 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 3000000 entries have been processed. 8597000000 left.\n",
      "0.652 secs per chunk on average. Meaning  93.408 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 4000000 entries have been processed. 8596000000 left.\n",
      "0.655 secs per chunk on average. Meaning  93.778 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 5000000 entries have been processed. 8595000000 left.\n",
      "0.653 secs per chunk on average. Meaning  93.605 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 6000000 entries have been processed. 8594000000 left.\n",
      "0.652 secs per chunk on average. Meaning  93.418 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 7000000 entries have been processed. 8593000000 left.\n",
      "0.651 secs per chunk on average. Meaning  93.228 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 8000000 entries have been processed. 8592000000 left.\n",
      "0.658 secs per chunk on average. Meaning  94.252 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 9000000 entries have been processed. 8591000000 left.\n",
      "0.656 secs per chunk on average. Meaning  93.867 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 10000000 entries have been processed. 8590000000 left.\n",
      "0.658 secs per chunk on average. Meaning  94.268 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# count the entries of the comment dataframe which have a na value in any column\n",
    "counted_nans = dp.run_simple_function_on_chunks_concat(comments_in_chunks(), \n",
    "                                                lambda x: dp.count_na_entries(x, \"any\", False),\n",
    "                                                print_time=(1000000, 8600000000)).sum(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "na rows              0\n",
       "total rows    10000000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(counted_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through chunk 0...\n",
      "The first 100000 entries have been processed. 72824794 left.\n",
      "5.211 secs per chunk on average. Meaning  63.244 minutes left.\n",
      "Going through chunk 1...\n",
      "The first 200000 entries have been processed. 72724794 left.\n",
      "3.801 secs per chunk on average. Meaning  46.075 minutes left.\n",
      "Going through chunk 2...\n",
      "The first 300000 entries have been processed. 72624794 left.\n",
      "3.651 secs per chunk on average. Meaning  44.187 minutes left.\n",
      "Going through chunk 3...\n",
      "The first 400000 entries have been processed. 72524794 left.\n",
      "3.300 secs per chunk on average. Meaning  39.889 minutes left.\n",
      "Going through chunk 4...\n",
      "The first 500000 entries have been processed. 72424794 left.\n",
      "3.056 secs per chunk on average. Meaning  36.894 minutes left.\n",
      "Going through chunk 5...\n",
      "The first 600000 entries have been processed. 72324794 left.\n",
      "2.854 secs per chunk on average. Meaning  34.408 minutes left.\n",
      "Going through chunk 6...\n",
      "The first 700000 entries have been processed. 72224794 left.\n",
      "2.727 secs per chunk on average. Meaning  32.828 minutes left.\n",
      "Going through chunk 7...\n",
      "The first 800000 entries have been processed. 72124794 left.\n",
      "2.611 secs per chunk on average. Meaning  31.384 minutes left.\n",
      "Going through chunk 8...\n",
      "The first 900000 entries have been processed. 72024794 left.\n",
      "2.592 secs per chunk on average. Meaning  31.112 minutes left.\n",
      "Going through chunk 9...\n",
      "The first 1000000 entries have been processed. 71924794 left.\n",
      "2.573 secs per chunk on average. Meaning  30.839 minutes left.\n"
     ]
    }
   ],
   "source": [
    "# filter the video dataframe to only include videos from news and politics category\n",
    "\n",
    "df_videos_news_pol = dp.run_simple_function_on_chunks_concat(videos_in_chunks(chunksize=100000),\n",
    "                                                             lambda x: x[x.categories == \"News & Politics\"], \n",
    "                                                             print_time=(100000, 72924794))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels_news_pol = df_channels[df_channels.category_cc == \"News & Politics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video 1   title blwe id 34547 comment_user 1234\n",
    "video 1   title blwe id 34547 comment_user 1234\n",
    "video 1   title blwe id 34547 comment_user 1235\n",
    "vide\n",
    "video 2\n",
    "\n",
    "\n",
    "\n",
    "comment 1 video id 253465\n",
    "comment 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWm1-4XF7AHxVUTkHCM1uw</td>\n",
       "      <td>2019-11-17 06:28:42.593675</td>\n",
       "      <td>retrogamer3.com</td>\n",
       "      <td>16.0</td>\n",
       "      <td>dfa8RRkKoa4</td>\n",
       "      <td>9251</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RetroGamer3,Live Stream,politics,Trump</td>\n",
       "      <td>Retrogamer3 Political Stream</td>\n",
       "      <td>2018-08-23 00:00:00</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWLsxDD373D4tY8kN-0LGQ</td>\n",
       "      <td>2019-11-05 00:42:33.012228</td>\n",
       "      <td>What are the forces at work that have created ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>_dIIEMvH86k</td>\n",
       "      <td>309</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NWO,Ebola,Ukraine,Mainstream,Media,Pyschology</td>\n",
       "      <td>Adam Curtis describes the Surkow Strategy of M...</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18005</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzVBu6oqlrAix0oq9T2rBFg</td>\n",
       "      <td>2019-11-19 20:40:22.403775</td>\n",
       "      <td>Social Media:\\n\\nFacebook.com/thebookoflaura\\n...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>eWXefhNB2po</td>\n",
       "      <td>707</td>\n",
       "      <td>625.0</td>\n",
       "      <td>michael jackson,lyrics,music video,court,child...</td>\n",
       "      <td>my thoughts on the michael jackson documentary.</td>\n",
       "      <td>2019-04-24 00:00:00</td>\n",
       "      <td>12780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24361</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:47:10.520209</td>\n",
       "      <td>👕 Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>MBgzne7djFU</td>\n",
       "      <td>378</td>\n",
       "      <td>47027.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>Elizabeth Warren Gets a Big Surprise at the Ai...</td>\n",
       "      <td>2019-10-03 00:00:00</td>\n",
       "      <td>374711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24362</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:46:16.481889</td>\n",
       "      <td>👕 Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>AbH3pJnFgY8</td>\n",
       "      <td>278</td>\n",
       "      <td>36384.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>No More Twitter? 😂</td>\n",
       "      <td>2019-10-02 00:00:00</td>\n",
       "      <td>245617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999870</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:26.687460</td>\n",
       "      <td>The Young Turks recently posted a video entitl...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Rmq0JmUbt8k</td>\n",
       "      <td>857</td>\n",
       "      <td>25.0</td>\n",
       "      <td>American Joe,American Joe Show,The Young Turks...</td>\n",
       "      <td>Young Turks Caught Lying and Race Baiting.... ...</td>\n",
       "      <td>2018-11-17 00:00:00</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999871</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:27.273595</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ts__Orp310M</td>\n",
       "      <td>49</td>\n",
       "      <td>34.0</td>\n",
       "      <td>American Joe,American Joe Show</td>\n",
       "      <td>President says he will send migrant Children B...</td>\n",
       "      <td>2018-11-15 00:00:00</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999872</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:27.847348</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bQ3_ZMVpiio</td>\n",
       "      <td>298</td>\n",
       "      <td>6.0</td>\n",
       "      <td>American Joe,American Joe Show,Michael Avenatt...</td>\n",
       "      <td>Creepy Porn Lawyer, and Woman Beater Michael A...</td>\n",
       "      <td>2018-11-14 00:00:00</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999873</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:28.400609</td>\n",
       "      <td>Patriots I need your help growing the American...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>q92A939Nyj8</td>\n",
       "      <td>388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>American Joe,American Joe Show,Midterm Electio...</td>\n",
       "      <td>Midterm Fallout - How Bad is it For Trump?</td>\n",
       "      <td>2018-11-14 00:00:00</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999874</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrUkx0UAxgybbbMvWphd62Q</td>\n",
       "      <td>2019-11-10 14:27:32.321224</td>\n",
       "      <td>Link to video by Conservative Youtuber ABL, An...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CPYdzsvg-Ns</td>\n",
       "      <td>573</td>\n",
       "      <td>18.0</td>\n",
       "      <td>American Joe,American Joe Show,Jim Acosta Dona...</td>\n",
       "      <td>Trump Vs. Jim Acosta - The Fake News is at it ...</td>\n",
       "      <td>2018-11-11 00:00:00</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145768 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             categories                channel_id                  crawl_date  \\\n",
       "1827    News & Politics  UCzWm1-4XF7AHxVUTkHCM1uw  2019-11-17 06:28:42.593675   \n",
       "7605    News & Politics  UCzWLsxDD373D4tY8kN-0LGQ  2019-11-05 00:42:33.012228   \n",
       "18005   News & Politics  UCzVBu6oqlrAix0oq9T2rBFg  2019-11-19 20:40:22.403775   \n",
       "24361   News & Politics  UCzUV5283-l5c0oKRtyenj6Q  2019-11-22 08:47:10.520209   \n",
       "24362   News & Politics  UCzUV5283-l5c0oKRtyenj6Q  2019-11-22 08:46:16.481889   \n",
       "...                 ...                       ...                         ...   \n",
       "999870  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:26.687460   \n",
       "999871  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:27.273595   \n",
       "999872  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:27.847348   \n",
       "999873  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:28.400609   \n",
       "999874  News & Politics  UCrUkx0UAxgybbbMvWphd62Q  2019-11-10 14:27:32.321224   \n",
       "\n",
       "                                              description  dislike_count  \\\n",
       "1827                                      retrogamer3.com           16.0   \n",
       "7605    What are the forces at work that have created ...            0.0   \n",
       "18005   Social Media:\\n\\nFacebook.com/thebookoflaura\\n...           89.0   \n",
       "24361   👕 Order your shirts here: https://Teespring.co...          195.0   \n",
       "24362   👕 Order your shirts here: https://Teespring.co...          114.0   \n",
       "...                                                   ...            ...   \n",
       "999870  The Young Turks recently posted a video entitl...            2.0   \n",
       "999871  Patriots I need your help growing the American...            0.0   \n",
       "999872  Patriots I need your help growing the American...            1.0   \n",
       "999873  Patriots I need your help growing the American...            2.0   \n",
       "999874  Link to video by Conservative Youtuber ABL, An...            2.0   \n",
       "\n",
       "         display_id  duration  like_count  \\\n",
       "1827    dfa8RRkKoa4      9251        25.0   \n",
       "7605    _dIIEMvH86k       309         9.0   \n",
       "18005   eWXefhNB2po       707       625.0   \n",
       "24361   MBgzne7djFU       378     47027.0   \n",
       "24362   AbH3pJnFgY8       278     36384.0   \n",
       "...             ...       ...         ...   \n",
       "999870  Rmq0JmUbt8k       857        25.0   \n",
       "999871  ts__Orp310M        49        34.0   \n",
       "999872  bQ3_ZMVpiio       298         6.0   \n",
       "999873  q92A939Nyj8       388         2.0   \n",
       "999874  CPYdzsvg-Ns       573        18.0   \n",
       "\n",
       "                                                     tags  \\\n",
       "1827               RetroGamer3,Live Stream,politics,Trump   \n",
       "7605        NWO,Ebola,Ukraine,Mainstream,Media,Pyschology   \n",
       "18005   michael jackson,lyrics,music video,court,child...   \n",
       "24361   Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "24362   Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "...                                                   ...   \n",
       "999870  American Joe,American Joe Show,The Young Turks...   \n",
       "999871                     American Joe,American Joe Show   \n",
       "999872  American Joe,American Joe Show,Michael Avenatt...   \n",
       "999873  American Joe,American Joe Show,Midterm Electio...   \n",
       "999874  American Joe,American Joe Show,Jim Acosta Dona...   \n",
       "\n",
       "                                                    title  \\\n",
       "1827                         Retrogamer3 Political Stream   \n",
       "7605    Adam Curtis describes the Surkow Strategy of M...   \n",
       "18005     my thoughts on the michael jackson documentary.   \n",
       "24361   Elizabeth Warren Gets a Big Surprise at the Ai...   \n",
       "24362                                  No More Twitter? 😂   \n",
       "...                                                   ...   \n",
       "999870  Young Turks Caught Lying and Race Baiting.... ...   \n",
       "999871  President says he will send migrant Children B...   \n",
       "999872  Creepy Porn Lawyer, and Woman Beater Michael A...   \n",
       "999873         Midterm Fallout - How Bad is it For Trump?   \n",
       "999874  Trump Vs. Jim Acosta - The Fake News is at it ...   \n",
       "\n",
       "                upload_date  view_count  \n",
       "1827    2018-08-23 00:00:00       478.0  \n",
       "7605    2015-01-04 00:00:00       865.0  \n",
       "18005   2019-04-24 00:00:00     12780.0  \n",
       "24361   2019-10-03 00:00:00    374711.0  \n",
       "24362   2019-10-02 00:00:00    245617.0  \n",
       "...                     ...         ...  \n",
       "999870  2018-11-17 00:00:00       273.0  \n",
       "999871  2018-11-15 00:00:00       353.0  \n",
       "999872  2018-11-14 00:00:00        76.0  \n",
       "999873  2018-11-14 00:00:00        38.0  \n",
       "999874  2018-11-11 00:00:00       146.0  \n",
       "\n",
       "[145768 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_videos_news_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2006-08-26</td>\n",
       "      <td>UCttspZesZIDEwwpVIgoZtWQ</td>\n",
       "      <td>IndiaTV</td>\n",
       "      <td>15177282</td>\n",
       "      <td>139814</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>UCRWFSbif-RFENbBrSiez1DA</td>\n",
       "      <td>ABP NEWS</td>\n",
       "      <td>16274836</td>\n",
       "      <td>129027</td>\n",
       "      <td>207.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>UCmphdqZNmqL72WJ2uyiNw5w</td>\n",
       "      <td>ABP NEWS HINDI</td>\n",
       "      <td>10800000</td>\n",
       "      <td>51298</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2015-03-23</td>\n",
       "      <td>UCx8Z14PpntdaxCt2hakbQLQ</td>\n",
       "      <td>The Lallantop</td>\n",
       "      <td>9120000</td>\n",
       "      <td>9423</td>\n",
       "      <td>438.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2007-06-19</td>\n",
       "      <td>UCIvaYmXn910QMdemBG3v1pQ</td>\n",
       "      <td>Zee News</td>\n",
       "      <td>9280000</td>\n",
       "      <td>102648</td>\n",
       "      <td>549.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135820</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2010-08-07</td>\n",
       "      <td>UC5rxiCGcNunIi5zI1hMYLMg</td>\n",
       "      <td>Salman Akhtar</td>\n",
       "      <td>10400</td>\n",
       "      <td>40</td>\n",
       "      <td>962468.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135825</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>UCLSEJQ8TWtlEkaytaa4Y7lw</td>\n",
       "      <td>WingsOfChrist</td>\n",
       "      <td>10420</td>\n",
       "      <td>61</td>\n",
       "      <td>962547.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135901</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>UCnkG_c5cyemVVsgCDoHiXew</td>\n",
       "      <td>The American Mirror</td>\n",
       "      <td>10500</td>\n",
       "      <td>329</td>\n",
       "      <td>963417.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136231</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>UC69lWS7UMbBQc-9yqp4nGjA</td>\n",
       "      <td>Patriotism Show</td>\n",
       "      <td>10320</td>\n",
       "      <td>46</td>\n",
       "      <td>975448.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136301</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>UCpbE1CJWNHpu8knuok8YBZQ</td>\n",
       "      <td>Jenny Constantine</td>\n",
       "      <td>10200</td>\n",
       "      <td>30</td>\n",
       "      <td>978433.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            category_cc   join_date                   channel  \\\n",
       "129     News & Politics  2006-08-26  UCttspZesZIDEwwpVIgoZtWQ   \n",
       "133     News & Politics  2012-06-01  UCRWFSbif-RFENbBrSiez1DA   \n",
       "212     News & Politics  2017-03-03  UCmphdqZNmqL72WJ2uyiNw5w   \n",
       "268     News & Politics  2015-03-23  UCx8Z14PpntdaxCt2hakbQLQ   \n",
       "337     News & Politics  2007-06-19  UCIvaYmXn910QMdemBG3v1pQ   \n",
       "...                 ...         ...                       ...   \n",
       "135820  News & Politics  2010-08-07  UC5rxiCGcNunIi5zI1hMYLMg   \n",
       "135825  News & Politics  2013-02-01  UCLSEJQ8TWtlEkaytaa4Y7lw   \n",
       "135901  News & Politics  2012-10-19  UCnkG_c5cyemVVsgCDoHiXew   \n",
       "136231  News & Politics  2017-11-25  UC69lWS7UMbBQc-9yqp4nGjA   \n",
       "136301  News & Politics  2017-03-26  UCpbE1CJWNHpu8knuok8YBZQ   \n",
       "\n",
       "                    name_cc  subscribers_cc  videos_cc  subscriber_rank_sb  \\\n",
       "129                 IndiaTV        15177282     139814               199.0   \n",
       "133                ABP NEWS        16274836     129027               207.0   \n",
       "212          ABP NEWS HINDI        10800000      51298               340.0   \n",
       "268           The Lallantop         9120000       9423               438.0   \n",
       "337                Zee News         9280000     102648               549.0   \n",
       "...                     ...             ...        ...                 ...   \n",
       "135820        Salman Akhtar           10400         40            962468.0   \n",
       "135825        WingsOfChrist           10420         61            962547.0   \n",
       "135901  The American Mirror           10500        329            963417.0   \n",
       "136231      Patriotism Show           10320         46            975448.0   \n",
       "136301    Jenny Constantine           10200         30            978433.0   \n",
       "\n",
       "        weights  \n",
       "129      2.0870  \n",
       "133      2.0870  \n",
       "212      2.0870  \n",
       "268      2.0870  \n",
       "337      2.0870  \n",
       "...         ...  \n",
       "135820  53.1435  \n",
       "135825  53.1435  \n",
       "135901  53.1435  \n",
       "136231  53.1435  \n",
       "136301  53.1435  \n",
       "\n",
       "[2263 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_channels_news_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWm1-4XF7AHxVUTkHCM1uw</td>\n",
       "      <td>2019-11-17 06:28:42.593675</td>\n",
       "      <td>retrogamer3.com</td>\n",
       "      <td>16.0</td>\n",
       "      <td>dfa8RRkKoa4</td>\n",
       "      <td>9251</td>\n",
       "      <td>25.0</td>\n",
       "      <td>RetroGamer3,Live Stream,politics,Trump</td>\n",
       "      <td>Retrogamer3 Political Stream</td>\n",
       "      <td>2018-08-23 00:00:00</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzWLsxDD373D4tY8kN-0LGQ</td>\n",
       "      <td>2019-11-05 00:42:33.012228</td>\n",
       "      <td>What are the forces at work that have created ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>_dIIEMvH86k</td>\n",
       "      <td>309</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NWO,Ebola,Ukraine,Mainstream,Media,Pyschology</td>\n",
       "      <td>Adam Curtis describes the Surkow Strategy of M...</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18005</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzVBu6oqlrAix0oq9T2rBFg</td>\n",
       "      <td>2019-11-19 20:40:22.403775</td>\n",
       "      <td>Social Media:\\n\\nFacebook.com/thebookoflaura\\n...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>eWXefhNB2po</td>\n",
       "      <td>707</td>\n",
       "      <td>625.0</td>\n",
       "      <td>michael jackson,lyrics,music video,court,child...</td>\n",
       "      <td>my thoughts on the michael jackson documentary.</td>\n",
       "      <td>2019-04-24 00:00:00</td>\n",
       "      <td>12780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28840</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzTmNzBxLEHbpZNOCpUTWbA</td>\n",
       "      <td>2019-11-03 04:38:01.617657</td>\n",
       "      <td>A young man is living a normal life with no ca...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>ck6Yl8TNoWs</td>\n",
       "      <td>1257</td>\n",
       "      <td>452.0</td>\n",
       "      <td>JoiRida,Cheatham,JoiRidaCheatham,Accepted,Detr...</td>\n",
       "      <td>Accepted - Award Winning Short Film</td>\n",
       "      <td>2013-10-13 00:00:00</td>\n",
       "      <td>27366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28860</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzTmNzBxLEHbpZNOCpUTWbA</td>\n",
       "      <td>2019-11-03 04:38:06.565138</td>\n",
       "      <td>Short Film</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tjUajxZAIZ8</td>\n",
       "      <td>422</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Joi.Rida,Cheatham,joiridacheatham,dread,loc,up...</td>\n",
       "      <td>JoiRida Twin Visit (Introducing Jive Viper)</td>\n",
       "      <td>2010-03-04 00:00:00</td>\n",
       "      <td>987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970869</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrXcatz6wlNHjuqgf-tglOA</td>\n",
       "      <td>2019-11-07 00:55:48.241832</td>\n",
       "      <td>As promised, our Wet Head Challenge using the ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>lIuK9DGtOx8</td>\n",
       "      <td>321</td>\n",
       "      <td>141.0</td>\n",
       "      <td>challenge,wet,head,gross,wet head challenge,we...</td>\n",
       "      <td>Gross Smoothie Wet Head Challenge 😕</td>\n",
       "      <td>2016-08-23 00:00:00</td>\n",
       "      <td>8941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991815</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrVnMcE3GIyg2rM4gH34YWg</td>\n",
       "      <td>2019-11-10 10:02:03.075065</td>\n",
       "      <td>More Travel News...\\nhttp://www.petergreenberg...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aSWbywb7SBE</td>\n",
       "      <td>423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2008,Travel Inspiration,clinton,Presidential,P...</td>\n",
       "      <td>2008 Presidential Candidates Travel Scorecard</td>\n",
       "      <td>2008-01-25 00:00:00</td>\n",
       "      <td>588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998347</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:30.119011</td>\n",
       "      <td>© 2012 WMG  Webisode by Mutemath from The Blue...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H8-Al6B_J1g</td>\n",
       "      <td>106</td>\n",
       "      <td>17.0</td>\n",
       "      <td>mutemath,wbr,INDMUSIC,warner bros records</td>\n",
       "      <td>Mutemath - What Happens Before The Show [Webis...</td>\n",
       "      <td>2006-11-04 00:00:00</td>\n",
       "      <td>3136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998349</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:31.498209</td>\n",
       "      <td>© 2012 WMG  Webisode by Mutemath from Park Wes...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>wYI6dWaEHjk</td>\n",
       "      <td>56</td>\n",
       "      <td>8.0</td>\n",
       "      <td>INDMUSIC,wbr,mutemath,warner bros records</td>\n",
       "      <td>Mutemath - Built for Destruction [Webisode]</td>\n",
       "      <td>2006-11-04 00:00:00</td>\n",
       "      <td>1881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998365</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrV-WEtbXrRIkgWgbXLAvcQ</td>\n",
       "      <td>2019-10-31 15:06:41.975562</td>\n",
       "      <td>© 2012 WMG  Webisode by Mutemath from The Goth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gs5jw7rtrn8</td>\n",
       "      <td>216</td>\n",
       "      <td>27.0</td>\n",
       "      <td>wbr,warner bros records,INDMUSIC</td>\n",
       "      <td>Mutemath - The Gothic Theatre in Denver [Webis...</td>\n",
       "      <td>2005-12-31 00:00:00</td>\n",
       "      <td>7916.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8564 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             categories                channel_id                  crawl_date  \\\n",
       "1827    News & Politics  UCzWm1-4XF7AHxVUTkHCM1uw  2019-11-17 06:28:42.593675   \n",
       "7605    News & Politics  UCzWLsxDD373D4tY8kN-0LGQ  2019-11-05 00:42:33.012228   \n",
       "18005   News & Politics  UCzVBu6oqlrAix0oq9T2rBFg  2019-11-19 20:40:22.403775   \n",
       "28840   News & Politics  UCzTmNzBxLEHbpZNOCpUTWbA  2019-11-03 04:38:01.617657   \n",
       "28860   News & Politics  UCzTmNzBxLEHbpZNOCpUTWbA  2019-11-03 04:38:06.565138   \n",
       "...                 ...                       ...                         ...   \n",
       "970869  News & Politics  UCrXcatz6wlNHjuqgf-tglOA  2019-11-07 00:55:48.241832   \n",
       "991815  News & Politics  UCrVnMcE3GIyg2rM4gH34YWg  2019-11-10 10:02:03.075065   \n",
       "998347  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:30.119011   \n",
       "998349  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:31.498209   \n",
       "998365  News & Politics  UCrV-WEtbXrRIkgWgbXLAvcQ  2019-10-31 15:06:41.975562   \n",
       "\n",
       "                                              description  dislike_count  \\\n",
       "1827                                      retrogamer3.com           16.0   \n",
       "7605    What are the forces at work that have created ...            0.0   \n",
       "18005   Social Media:\\n\\nFacebook.com/thebookoflaura\\n...           89.0   \n",
       "28840   A young man is living a normal life with no ca...           16.0   \n",
       "28860                                          Short Film            1.0   \n",
       "...                                                   ...            ...   \n",
       "970869  As promised, our Wet Head Challenge using the ...            3.0   \n",
       "991815  More Travel News...\\nhttp://www.petergreenberg...            0.0   \n",
       "998347  © 2012 WMG  Webisode by Mutemath from The Blue...            3.0   \n",
       "998349  © 2012 WMG  Webisode by Mutemath from Park Wes...            2.0   \n",
       "998365  © 2012 WMG  Webisode by Mutemath from The Goth...            0.0   \n",
       "\n",
       "         display_id  duration  like_count  \\\n",
       "1827    dfa8RRkKoa4      9251        25.0   \n",
       "7605    _dIIEMvH86k       309         9.0   \n",
       "18005   eWXefhNB2po       707       625.0   \n",
       "28840   ck6Yl8TNoWs      1257       452.0   \n",
       "28860   tjUajxZAIZ8       422        15.0   \n",
       "...             ...       ...         ...   \n",
       "970869  lIuK9DGtOx8       321       141.0   \n",
       "991815  aSWbywb7SBE       423         1.0   \n",
       "998347  H8-Al6B_J1g       106        17.0   \n",
       "998349  wYI6dWaEHjk        56         8.0   \n",
       "998365  Gs5jw7rtrn8       216        27.0   \n",
       "\n",
       "                                                     tags  \\\n",
       "1827               RetroGamer3,Live Stream,politics,Trump   \n",
       "7605        NWO,Ebola,Ukraine,Mainstream,Media,Pyschology   \n",
       "18005   michael jackson,lyrics,music video,court,child...   \n",
       "28840   JoiRida,Cheatham,JoiRidaCheatham,Accepted,Detr...   \n",
       "28860   Joi.Rida,Cheatham,joiridacheatham,dread,loc,up...   \n",
       "...                                                   ...   \n",
       "970869  challenge,wet,head,gross,wet head challenge,we...   \n",
       "991815  2008,Travel Inspiration,clinton,Presidential,P...   \n",
       "998347          mutemath,wbr,INDMUSIC,warner bros records   \n",
       "998349          INDMUSIC,wbr,mutemath,warner bros records   \n",
       "998365                   wbr,warner bros records,INDMUSIC   \n",
       "\n",
       "                                                    title  \\\n",
       "1827                         Retrogamer3 Political Stream   \n",
       "7605    Adam Curtis describes the Surkow Strategy of M...   \n",
       "18005     my thoughts on the michael jackson documentary.   \n",
       "28840                 Accepted - Award Winning Short Film   \n",
       "28860         JoiRida Twin Visit (Introducing Jive Viper)   \n",
       "...                                                   ...   \n",
       "970869                Gross Smoothie Wet Head Challenge 😕   \n",
       "991815      2008 Presidential Candidates Travel Scorecard   \n",
       "998347  Mutemath - What Happens Before The Show [Webis...   \n",
       "998349        Mutemath - Built for Destruction [Webisode]   \n",
       "998365  Mutemath - The Gothic Theatre in Denver [Webis...   \n",
       "\n",
       "                upload_date  view_count  \n",
       "1827    2018-08-23 00:00:00       478.0  \n",
       "7605    2015-01-04 00:00:00       865.0  \n",
       "18005   2019-04-24 00:00:00     12780.0  \n",
       "28840   2013-10-13 00:00:00     27366.0  \n",
       "28860   2010-03-04 00:00:00       987.0  \n",
       "...                     ...         ...  \n",
       "970869  2016-08-23 00:00:00      8941.0  \n",
       "991815  2008-01-25 00:00:00       588.0  \n",
       "998347  2006-11-04 00:00:00      3136.0  \n",
       "998349  2006-11-04 00:00:00      1881.0  \n",
       "998365  2005-12-31 00:00:00      7916.0  \n",
       "\n",
       "[8564 rows x 12 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all videos we found in news&pol are also pulished by a channel in category news&pol\n",
    "\n",
    "df_videos_news_pol[np.logical_not(df_videos_news_pol.channel_id.isin(df_channels_news_pol.channel))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed, not all videos in the news and politics category belong to a channel in this category!\n",
    "\n",
    "A google search shows that apparently, you don't have to have the same category for all videos, but you set a \"default\" channel category which will be used for videos if you don't change it manually. Also, you can probably change the default category after a while if you want.\n",
    "\n",
    "This is the reason why most of the news&pol videos are uploaded by a news&pol channe, but not all.\n",
    "\n",
    "In the paper about the dataset, the authors say that the channel category is actually the \"most frequent category\", so I guess the video categories are the most relevant, as they are the true categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to verify this, if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter news&pol videos to only include the channels we are interested in\n",
    "\n",
    "df_videos_news_pol_manually_selected = df_videos_news_pol[df_videos_news_pol.channel_id.isin([\"UCupvZG-5ko_eiXAupbDfxWw\",  # CNN\n",
    "                                                                                              \"UCXIJgqnII2ZOINSWNOGFThA\",  # Fox News\n",
    "                                                                                              \"UC16niRr50-MSBwiO3YDb3RA\",  # BBC News\n",
    "                                                                                              \"UCaXkIU1QidjPwiAYu6GcHjg\",  # MSNBC\n",
    "                                                                                            ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>UCaXkIU1QidjPwiAYu6GcHjg</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>1910000</td>\n",
       "      <td>23393</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>2.444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          category_cc   join_date                   channel name_cc  \\\n",
       "2604  News & Politics  2011-12-01  UCaXkIU1QidjPwiAYu6GcHjg   MSNBC   \n",
       "\n",
       "      subscribers_cc  videos_cc  subscriber_rank_sb  weights  \n",
       "2604         1910000      23393              5600.0    2.444  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_channels[df_channels.name_cc==\"MSNBC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [categories, channel_id, crawl_date, description, dislike_count, display_id, duration, like_count, tags, title, upload_date, view_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos_news_pol_manually_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort news&pol channels by subscriber count according to channel crawler\n",
    "df_channels_news_pol_sort_subscribers = df_channels_news_pol.sort_values(by=\"subscribers_cc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>UCRWFSbif-RFENbBrSiez1DA</td>\n",
       "      <td>ABP NEWS</td>\n",
       "      <td>16274836</td>\n",
       "      <td>129027</td>\n",
       "      <td>207.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2006-08-26</td>\n",
       "      <td>UCttspZesZIDEwwpVIgoZtWQ</td>\n",
       "      <td>IndiaTV</td>\n",
       "      <td>15177282</td>\n",
       "      <td>139814</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>UCmphdqZNmqL72WJ2uyiNw5w</td>\n",
       "      <td>ABP NEWS HINDI</td>\n",
       "      <td>10800000</td>\n",
       "      <td>51298</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2007-06-19</td>\n",
       "      <td>UCIvaYmXn910QMdemBG3v1pQ</td>\n",
       "      <td>Zee News</td>\n",
       "      <td>9280000</td>\n",
       "      <td>102648</td>\n",
       "      <td>549.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2015-03-23</td>\n",
       "      <td>UCx8Z14PpntdaxCt2hakbQLQ</td>\n",
       "      <td>The Lallantop</td>\n",
       "      <td>9120000</td>\n",
       "      <td>9423</td>\n",
       "      <td>438.0</td>\n",
       "      <td>2.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130581</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2008-03-26</td>\n",
       "      <td>UC65jmwvHLoCMBJXnzHrGwiA</td>\n",
       "      <td>The Virginian-Pilot</td>\n",
       "      <td>10200</td>\n",
       "      <td>3037</td>\n",
       "      <td>868834.0</td>\n",
       "      <td>14.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134955</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2009-05-17</td>\n",
       "      <td>UCuZTp4-0xPoGUxdKPLiQE6w</td>\n",
       "      <td>Harold Jackson</td>\n",
       "      <td>10133</td>\n",
       "      <td>1330</td>\n",
       "      <td>932456.0</td>\n",
       "      <td>23.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133413</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2016-03-25</td>\n",
       "      <td>UCWtAa1fyxYxR1gAH-IZtaXg</td>\n",
       "      <td>Trumpennials</td>\n",
       "      <td>10000</td>\n",
       "      <td>64</td>\n",
       "      <td>905989.0</td>\n",
       "      <td>14.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133426</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>UCaUCzPRX1bKUwz7a2aJgIDQ</td>\n",
       "      <td>Missing Persons &amp; My...</td>\n",
       "      <td>10000</td>\n",
       "      <td>31</td>\n",
       "      <td>906002.0</td>\n",
       "      <td>14.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133831</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2008-04-21</td>\n",
       "      <td>UCt0rmyaJboPs29J_YSQucqg</td>\n",
       "      <td>Christian World News...</td>\n",
       "      <td>10000</td>\n",
       "      <td>1184</td>\n",
       "      <td>915543.0</td>\n",
       "      <td>15.959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            category_cc   join_date                   channel  \\\n",
       "133     News & Politics  2012-06-01  UCRWFSbif-RFENbBrSiez1DA   \n",
       "129     News & Politics  2006-08-26  UCttspZesZIDEwwpVIgoZtWQ   \n",
       "212     News & Politics  2017-03-03  UCmphdqZNmqL72WJ2uyiNw5w   \n",
       "337     News & Politics  2007-06-19  UCIvaYmXn910QMdemBG3v1pQ   \n",
       "268     News & Politics  2015-03-23  UCx8Z14PpntdaxCt2hakbQLQ   \n",
       "...                 ...         ...                       ...   \n",
       "130581  News & Politics  2008-03-26  UC65jmwvHLoCMBJXnzHrGwiA   \n",
       "134955  News & Politics  2009-05-17  UCuZTp4-0xPoGUxdKPLiQE6w   \n",
       "133413  News & Politics  2016-03-25  UCWtAa1fyxYxR1gAH-IZtaXg   \n",
       "133426  News & Politics  2018-08-31  UCaUCzPRX1bKUwz7a2aJgIDQ   \n",
       "133831  News & Politics  2008-04-21  UCt0rmyaJboPs29J_YSQucqg   \n",
       "\n",
       "                        name_cc  subscribers_cc  videos_cc  \\\n",
       "133                    ABP NEWS        16274836     129027   \n",
       "129                     IndiaTV        15177282     139814   \n",
       "212              ABP NEWS HINDI        10800000      51298   \n",
       "337                    Zee News         9280000     102648   \n",
       "268               The Lallantop         9120000       9423   \n",
       "...                         ...             ...        ...   \n",
       "130581      The Virginian-Pilot           10200       3037   \n",
       "134955           Harold Jackson           10133       1330   \n",
       "133413             Trumpennials           10000         64   \n",
       "133426  Missing Persons & My...           10000         31   \n",
       "133831  Christian World News...           10000       1184   \n",
       "\n",
       "        subscriber_rank_sb  weights  \n",
       "133                  207.0    2.087  \n",
       "129                  199.0    2.087  \n",
       "212                  340.0    2.087  \n",
       "337                  549.0    2.087  \n",
       "268                  438.0    2.087  \n",
       "...                    ...      ...  \n",
       "130581            868834.0   14.148  \n",
       "134955            932456.0   23.492  \n",
       "133413            905989.0   14.780  \n",
       "133426            906002.0   14.699  \n",
       "133831            915543.0   15.959  \n",
       "\n",
       "[2263 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_channels_news_pol_sort_subscribers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
