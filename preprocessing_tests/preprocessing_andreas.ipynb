{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import data_processing as dp  # own functions and logic will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the folder where the YouNiverse dataset is stored here\n",
    "\n",
    "# when adding your own path, don't remove the existing path, just comment it\n",
    "# in this way, everyone can quickly uncomment their own path\n",
    "dataset_root_path = \"/media/andreas/Backup Plus/youniverse_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if we want to test something on the dataset without using chunks. Otherwise, keep commented\n",
    "\n",
    "\n",
    "# # load channel data\n",
    "# df_channels = pd.read_csv(dataset_root_path + \"df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n",
    "\n",
    "# # load (first 100000 rows of) video data\n",
    "# df_videos = pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", compression=\"infer\", lines=True, nrows=100000)\n",
    "\n",
    "# # load (first 1000000 rows of) comment data\n",
    "# df_comments = pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", compression=\"infer\", sep=\"\\t\", nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of \"readers\", i.e., objects that we can iterate through \n",
    "# and always get a chunk of the dataframe in each iteration\n",
    "\n",
    "def videos_in_chunks(chunksize: int = 100000):\n",
    "    return pd.read_json(dataset_root_path + \"yt_metadata_en.jsonl.gz\", \n",
    "                        compression=\"infer\", lines=True, chunksize=chunksize, )  \n",
    "                        # nrows=1000000, )   # uncomment this to only use the first million videos, for testing\n",
    "                                             # (remove the paranthesis above as well)\n",
    "def comments_in_chunks(chunksize: int = 1000000):\n",
    "    return pd.read_csv(dataset_root_path + \"youtube_comments.tsv.gz\", \n",
    "                       compression=\"infer\", sep=\"\\t\", chunksize=chunksize, )\n",
    "                       # nrows = 10000000)  # uncomment this to only use the first 10 million comments, for testing\n",
    "                                            # (remove the paranthesis above as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vid_by_channel(channel_id: str, video_chunksize: int = 100000) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Returns a dataframe which is the video metadata dataframe, filtered so that only videos from the \n",
    "    given channel remain.\n",
    "\n",
    "    The function does this by going through the dataset in chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        channel_id :  id of the channel which the videos will be sorted by\n",
    "        video_chunksize :  number of entries in each chunk. Default is 100 000.\n",
    "\n",
    "    Returns:\n",
    "        The filtered dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    videos_filtered = pd.DataFrame(dict())\n",
    "    \n",
    "    with videos_in_chunks(video_chunksize) as reader:\n",
    "        time_start_global = time.time()\n",
    "        i=0\n",
    "        for video_chunk in reader:\n",
    "            time_start_chunk = time.time()\n",
    "            print(f\"Going through video chunk {i}...\")\n",
    "            videos_filtered = pd.concat([videos_filtered, video_chunk.loc[video_chunk.channel_id == channel_id]])\n",
    "            print(f\"The first {(i+1) * video_chunksize} videos have been processed.\")\n",
    "            time_end = time.time()\n",
    "            print(f\"{(time_end-time_start_global)/(i+1):.3f} secs per chunk on average. Meaning {72924794 / video_chunksize * (time_end-time_start_global)/((i+1)*60):.3f} mins for entire dataset, i.e., {72924794 / video_chunksize * (time_end-time_start_global)/((i+1)*60) - (time_end-time_start_global)/60:.3f} mins left.\")\n",
    "            i=i+1\n",
    "    return videos_filtered\n",
    "\n",
    "def filter_comment_by_channel(channel_id: str,video_chunksize: int = 100000, comment_chunksize: int = 1000000) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Returns a dataframe which is the comment data, but filtered so that only comments\n",
    "    made on videos which were published by a certain channel are left.\n",
    "\n",
    "    The function does this by going through the dataset in chunks of a defined size.\n",
    "\n",
    "    Args:\n",
    "        channel_id: id of the channel to be filtered by\n",
    "        video_chunksize: number of entries per chunk when going through the videos (to find the videos uploaded by a certain channel)\n",
    "        comment_chunksize: number of entries per chunk when going through the comments\n",
    "\n",
    "    Returns:\n",
    "        The filtered dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    comments_filtered = pd.DataFrame(dict())\n",
    "    filtered_videos = filter_vid_by_channel(channel_id, video_chunksize=video_chunksize)\n",
    "    print(f\"Videos have been filtered by channel, {len(filtered_videos)} videos found. \\nNow going through comments....\")\n",
    "    with comments_in_chunks(comment_chunksize) as reader:\n",
    "        time_start_global = time.time()\n",
    "        for i, comment_chunk in enumerate(reader):\n",
    "            print(f\"Going through comment chunk {i}...\")\n",
    "            comments_filtered = pd.concat([comments_filtered, \n",
    "                                comment_chunk.loc[comment_chunk.video_id.isin(filtered_videos.display_id)]])\n",
    "            print(f\"The first {(i+1) * comment_chunksize} comments have been processed\")\n",
    "            time_end = time.time()\n",
    "            print(f\"{(time_end-time_start_global)/(i+1):.3f} secs per chunk on average.Meaning {8600000000 / comment_chunksize * (time_end-time_start_global)/((i+1)*60):.3f} mins for entire dataset, i.e., {8600000000 / comment_chunksize * (time_end-time_start_global)/((i+1)*60) - (time_end-time_start_global)/60:.3f} mins left.\")\n",
    "    return comments_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through video chunk 0...\n",
      "The first 100000 videos have been processed.\n",
      "2.307 secs per chunk on average. Meaning 28.041 mins for entire dataset, i.e., 28.003 mins left.\n",
      "Going through video chunk 1...\n",
      "The first 200000 videos have been processed.\n",
      "3.337 secs per chunk on average. Meaning 40.560 mins for entire dataset, i.e., 40.449 mins left.\n",
      "Going through video chunk 2...\n",
      "The first 300000 videos have been processed.\n",
      "2.968 secs per chunk on average. Meaning 36.068 mins for entire dataset, i.e., 35.919 mins left.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test the above functions by searching for comments from a certain channel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m filtered_comments_test \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_comment_by_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUCzWrhkg9eK5I8Bm3HfV-unA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[87], line 48\u001b[0m, in \u001b[0;36mfilter_comment_by_channel\u001b[0;34m(channel_id, video_chunksize, comment_chunksize)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mReturns a dataframe which is the comment data, but filtered so that only comments\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mmade on videos which were published by a certain channel are left.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    The filtered dataframe\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m comments_filtered \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m())\n\u001b[0;32m---> 48\u001b[0m filtered_videos \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_vid_by_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_chunksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideos have been filtered by channel, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_videos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m videos found. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNow going through comments....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m comments_in_chunks(comment_chunksize) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "Cell \u001b[0;32mIn[87], line 21\u001b[0m, in \u001b[0;36mfilter_vid_by_channel\u001b[0;34m(channel_id, video_chunksize)\u001b[0m\n\u001b[1;32m     19\u001b[0m time_start_global \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     20\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvideo_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_start_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGoing through video chunk \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/site-packages/pandas/io/json/_json.py:1090\u001b[0m, in \u001b[0;36mJsonReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m-> 1090\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(islice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize))\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/gzip.py:314\u001b[0m, in \u001b[0;36mGzipFile.read1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     size \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread1(size)\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/ada/lib/python3.11/gzip.py:507\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    505\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 507\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test the above functions by searching for comments from a certain channel\n",
    "filtered_comments_test = filter_comment_by_channel(\"UCzWrhkg9eK5I8Bm3HfV-unA\", video_chunksize=100000, comment_chunksize=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>video_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132678</th>\n",
       "      <td>9352</td>\n",
       "      <td>qr9Hm1pTZKA</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895322</th>\n",
       "      <td>58129</td>\n",
       "      <td>zj5TOsMZ-a4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868268</th>\n",
       "      <td>453667</td>\n",
       "      <td>3vQK78eUg2A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094579</th>\n",
       "      <td>468696</td>\n",
       "      <td>SWZG-ba1qDk</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912192</th>\n",
       "      <td>594074</td>\n",
       "      <td>hn2zYwqSINY</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8593459329</th>\n",
       "      <td>575741522</td>\n",
       "      <td>kYkokQgnu20</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8594971150</th>\n",
       "      <td>575837890</td>\n",
       "      <td>qr9Hm1pTZKA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8595593308</th>\n",
       "      <td>575877790</td>\n",
       "      <td>ObYU8s2psvQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8604498138</th>\n",
       "      <td>576474588</td>\n",
       "      <td>ikPgqOPXiAw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8605286187</th>\n",
       "      <td>576528514</td>\n",
       "      <td>SWZG-ba1qDk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author     video_id likes replies\n",
       "132678           9352  qr9Hm1pTZKA     6       0\n",
       "895322          58129  zj5TOsMZ-a4     0       0\n",
       "6868268        453667  3vQK78eUg2A     2       1\n",
       "7094579        468696  SWZG-ba1qDk    15      18\n",
       "8912192        594074  hn2zYwqSINY     0       1\n",
       "...               ...          ...   ...     ...\n",
       "8593459329  575741522  kYkokQgnu20     6       8\n",
       "8594971150  575837890  qr9Hm1pTZKA     1       0\n",
       "8595593308  575877790  ObYU8s2psvQ     0       0\n",
       "8604498138  576474588  ikPgqOPXiAw     0       0\n",
       "8605286187  576528514  SWZG-ba1qDk     0       0\n",
       "\n",
       "[5250 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_comments_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
